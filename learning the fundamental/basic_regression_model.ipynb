{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3WbG_6wW809r"
      },
      "outputs": [],
      "source": [
        "  #imports\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight = 0.2\n",
        "bias = 0.3\n",
        "# create\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start , end , step).unsqueeze(dim=1)\n",
        "y = weight * X + bias"
      ],
      "metadata": {
        "id": "l8egsTom-gkg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:10] , y[:10],len(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvgmpDnIFCs7",
        "outputId": "2e44879a-ca6e-4e84-a23d-a76048386681"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3040],\n",
              "         [0.3080],\n",
              "         [0.3120],\n",
              "         [0.3160],\n",
              "         [0.3200],\n",
              "         [0.3240],\n",
              "         [0.3280],\n",
              "         [0.3320],\n",
              "         [0.3360]]),\n",
              " 50)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train/test split\n",
        "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWxeI_2otNPq",
        "outputId": "f6709f03-e687-46a8-a7ff-2e5515722586"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data,\n",
        "                     train_labels,\n",
        "                     test_data,\n",
        "                     test_labels,\n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(7, 5))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions in red (predictions were made on the test data)\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14});"
      ],
      "metadata": {
        "id": "209R11VOtIv8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(X_train , y_train ,X_test ,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "HggZNa02tcAq",
        "outputId": "f93138b5-0e12-42fd-d729-09eabd7ef08d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First model"
      ],
      "metadata": {
        "id": "dumfPOeIfj--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bulding a very simple regression model"
      ],
      "metadata": {
        "id": "5zE8urjpaBHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LienearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1 ,\n",
        "                                                requires_grad=True ,\n",
        "                                                 dtype= torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1 ,\n",
        "                                        requires_grad=True ,\n",
        "                                        dtype=torch.float))\n",
        "  def forward(self , x : torch.Tensor) -> torch.Tensor:\n",
        "    return  self.weights * x + self.bias\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mcd4jFpdaDnQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(43)\n",
        "model0 = LienearRegressionModel()\n",
        "model0"
      ],
      "metadata": {
        "id": "5J02Ac4Ta0wT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf42d25-a868-40f7-a27a-dbfbbcb2866b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LienearRegressionModel()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction with out trainin\n",
        "with torch.inference_mode():\n",
        "  y_preds = model0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M86lLesTpZYn",
        "outputId": "8a813014-5dc1-4cf0-f9b6-3014023678cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.2246],\n",
              "        [-1.2375],\n",
              "        [-1.2505],\n",
              "        [-1.2635],\n",
              "        [-1.2764],\n",
              "        [-1.2894],\n",
              "        [-1.3024],\n",
              "        [-1.3153],\n",
              "        [-1.3283],\n",
              "        [-1.3413]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(X_train , y_train ,X_test ,y_test ,predictions=y_preds )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "aAP_k2Phtwoh",
        "outputId": "99a9b48d-49d6-4d1a-9fa6-7e4c616128f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0.state_dict()"
      ],
      "metadata": {
        "id": "ZmjOqReSuR0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b59d373-0e3a-4e60-d888-c9e2fedff9a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([-0.6484])), ('bias', tensor([-0.7058]))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NpE1tCgmZF7",
        "outputId": "538dfe6c-c4a1-4beb-ae3a-7d5d9e490ce0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x79d1deca69d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyLEStFeoZu2",
        "outputId": "f6b30d83-5871-4fb2-a94e-22b7c795b31b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([-0.6484], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.7058], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "assigning the loss and optimizer"
      ],
      "metadata": {
        "id": "QTCKV9IvetR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(params=model0.parameters()  , lr=0.001)"
      ],
      "metadata": {
        "id": "l7scAi-ClNLn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "making the trian loop"
      ],
      "metadata": {
        "id": "iSHN1BcGe3iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trian_loss_value = []\n",
        "test_loss_value =[]\n",
        "epoch_count = []\n",
        "def trianing():\n",
        "  epochs = 5000\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    #set the model to traning mode\n",
        "\n",
        "    model0.train() # set all the prameters that require graidient to tracking\n",
        "\n",
        "    # forward pass\n",
        "    y_pred = model0(X_train)\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = loss_fn(y_pred  , y_train)\n",
        "\n",
        "    #optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #perform bachprop on the loss with respect ot prameters\n",
        "    loss.backward()\n",
        "\n",
        "    #step the o0p-timizser (perform the grtadient desecent)\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    model0.eval() # turns off the gradient tracking\n",
        "\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      pred = model0(X_test)\n",
        "    trian_loss_value.append(loss)\n",
        "    test_loss_value.append(loss_fn(pred , y_test))\n",
        "    epoch_count.append(epoch)\n",
        "    print(f\" epoch = {epoch}\")\n",
        "    print(f\"    the train loss= {loss}\")\n",
        "    print(f\"    the test loss = {loss_fn(pred , y_test)}\")\n",
        "    print(\"---------------------------------------------\")\n",
        "  print(model0.state_dict())"
      ],
      "metadata": {
        "id": "c7QXc3e97Voo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trianing()\n",
        "#feel free to increment the epochs number"
      ],
      "metadata": {
        "id": "mIVgkv0J9WTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b381432-512d-42a5-bae7-c800ceab5c43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3751\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3752\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3753\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3754\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3755\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3756\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3757\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3758\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3759\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3760\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3761\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3762\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3763\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3764\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3765\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3766\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3767\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3768\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3769\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3770\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3771\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3772\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3773\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3774\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3775\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3776\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3777\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3778\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3779\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3780\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3781\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3782\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3783\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3784\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3785\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3786\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3787\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3788\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3789\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3790\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3791\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3792\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3793\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3794\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3795\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3796\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3797\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3798\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3799\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3800\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3801\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3802\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3803\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3804\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3805\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3806\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3807\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3808\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3809\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3810\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3811\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3812\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3813\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3814\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3815\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3816\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3817\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3818\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3819\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3820\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3821\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3822\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3823\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3824\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3825\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3826\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3827\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3828\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3829\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3830\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3831\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3832\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3833\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3834\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3835\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3836\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3837\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3838\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3839\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3840\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3841\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3842\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3843\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3844\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3845\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3846\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3847\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3848\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3849\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3850\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3851\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3852\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3853\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3854\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3855\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3856\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3857\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3858\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3859\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3860\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3861\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3862\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3863\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3864\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3865\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3866\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3867\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3868\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3869\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3870\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3871\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3872\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3873\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3874\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3875\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3876\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3877\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3878\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3879\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3880\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3881\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3882\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3883\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3884\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3885\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3886\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3887\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3888\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3889\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3890\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3891\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3892\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3893\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3894\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3895\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3896\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3897\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3898\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3899\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3900\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3901\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3902\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3903\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3904\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3905\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3906\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3907\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3908\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3909\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3910\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3911\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3912\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3913\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3914\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3915\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3916\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3917\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3918\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3919\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3920\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3921\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3922\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3923\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3924\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3925\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3926\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3927\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3928\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3929\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3930\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3931\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3932\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3933\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3934\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3935\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3936\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3937\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3938\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3939\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3940\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3941\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3942\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3943\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3944\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3945\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3946\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3947\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3948\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3949\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3950\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3951\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3952\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3953\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3954\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3955\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3956\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3957\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3958\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3959\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3960\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3961\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3962\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3963\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3964\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3965\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3966\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3967\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3968\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3969\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3970\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3971\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3972\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3973\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3974\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3975\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3976\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3977\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3978\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3979\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3980\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3981\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3982\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3983\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3984\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3985\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3986\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3987\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3988\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3989\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3990\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3991\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3992\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3993\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3994\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3995\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3996\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3997\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 3998\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 3999\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4000\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4001\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4002\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4003\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4004\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4005\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4006\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4007\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4008\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4009\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4010\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4011\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4012\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4013\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4014\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4015\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4016\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4017\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4018\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4019\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4020\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4021\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4022\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4023\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4024\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4025\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4026\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4027\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4028\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4029\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4030\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4031\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4032\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4033\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4034\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4035\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4036\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4037\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4038\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4039\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4040\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4041\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4042\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4043\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4044\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4045\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4046\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4047\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4048\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4049\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4050\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4051\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4052\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4053\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4054\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4055\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4056\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4057\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4058\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4059\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4060\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4061\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4062\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4063\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4064\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4065\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4066\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4067\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4068\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4069\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4070\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4071\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4072\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4073\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4074\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4075\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4076\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4077\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4078\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4079\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4080\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4081\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4082\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4083\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4084\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4085\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4086\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4087\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4088\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4089\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4090\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4091\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4092\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4093\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4094\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4095\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4096\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4097\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4098\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4099\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4100\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4101\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4102\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4103\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4104\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4105\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4106\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4107\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4108\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4109\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4110\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4111\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4112\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4113\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4114\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4115\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4116\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4117\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4118\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4119\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4120\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4121\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4122\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4123\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4124\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4125\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4126\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4127\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4128\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4129\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4130\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4131\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4132\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4133\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4134\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4135\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4136\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4137\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4138\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4139\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4140\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4141\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4142\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4143\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4144\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4145\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4146\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4147\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4148\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4149\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4150\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4151\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4152\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4153\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4154\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4155\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4156\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4157\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4158\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4159\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4160\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4161\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4162\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4163\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4164\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4165\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4166\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4167\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4168\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4169\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4170\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4171\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4172\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4173\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4174\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4175\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4176\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4177\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4178\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4179\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4180\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4181\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4182\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4183\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4184\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4185\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4186\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4187\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4188\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4189\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4190\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4191\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4192\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4193\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4194\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4195\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4196\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4197\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4198\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4199\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4200\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4201\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4202\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4203\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4204\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4205\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4206\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4207\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4208\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4209\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4210\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4211\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4212\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4213\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4214\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4215\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4216\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4217\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4218\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4219\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4220\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4221\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4222\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4223\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4224\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4225\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4226\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4227\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4228\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4229\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4230\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4231\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4232\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4233\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4234\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4235\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4236\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4237\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4238\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4239\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4240\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4241\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4242\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4243\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4244\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4245\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4246\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4247\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4248\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4249\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4250\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4251\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4252\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4253\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4254\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4255\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4256\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4257\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4258\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4259\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4260\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4261\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4262\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4263\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4264\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4265\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4266\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4267\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4268\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4269\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4270\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4271\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4272\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4273\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4274\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4275\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4276\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4277\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4278\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4279\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4280\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4281\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4282\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4283\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4284\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4285\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4286\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4287\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4288\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4289\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4290\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4291\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4292\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4293\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4294\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4295\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4296\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4297\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4298\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4299\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4300\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4301\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4302\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4303\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4304\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4305\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4306\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4307\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4308\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4309\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4310\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4311\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4312\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4313\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4314\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4315\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4316\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4317\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4318\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4319\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4320\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4321\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4322\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4323\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4324\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4325\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4326\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4327\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4328\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4329\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4330\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4331\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4332\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4333\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4334\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4335\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4336\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4337\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4338\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4339\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4340\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4341\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4342\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4343\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4344\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4345\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4346\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4347\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4348\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4349\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4350\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4351\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4352\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4353\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4354\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4355\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4356\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4357\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4358\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4359\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4360\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4361\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4362\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4363\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4364\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4365\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4366\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4367\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4368\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4369\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4370\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4371\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4372\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4373\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4374\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4375\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4376\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4377\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4378\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4379\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4380\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4381\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4382\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4383\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4384\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4385\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4386\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4387\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4388\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4389\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4390\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4391\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4392\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4393\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4394\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4395\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4396\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4397\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4398\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4399\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4400\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4401\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4402\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4403\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4404\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4405\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4406\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4407\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4408\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4409\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4410\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4411\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4412\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4413\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4414\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4415\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4416\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4417\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4418\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4419\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4420\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4421\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4422\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4423\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4424\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4425\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4426\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4427\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4428\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4429\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4430\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4431\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4432\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4433\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4434\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4435\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4436\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4437\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4438\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4439\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4440\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4441\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4442\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4443\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4444\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4445\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4446\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4447\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4448\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4449\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4450\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4451\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4452\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4453\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4454\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4455\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4456\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4457\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4458\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4459\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4460\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4461\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4462\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4463\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4464\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4465\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4466\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4467\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4468\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4469\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4470\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4471\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4472\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4473\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4474\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4475\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4476\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4477\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4478\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4479\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4480\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4481\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4482\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4483\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4484\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4485\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4486\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4487\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4488\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4489\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4490\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4491\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4492\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4493\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4494\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4495\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4496\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4497\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4498\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4499\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4500\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4501\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4502\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4503\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4504\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4505\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4506\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4507\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4508\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4509\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4510\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4511\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4512\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4513\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4514\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4515\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4516\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4517\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4518\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4519\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4520\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4521\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4522\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4523\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4524\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4525\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4526\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4527\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4528\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4529\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4530\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4531\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4532\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4533\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4534\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4535\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4536\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4537\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4538\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4539\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4540\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4541\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4542\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4543\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4544\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4545\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4546\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4547\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4548\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4549\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4550\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4551\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4552\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4553\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4554\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4555\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4556\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4557\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4558\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4559\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4560\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4561\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4562\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4563\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4564\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4565\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4566\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4567\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4568\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4569\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4570\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4571\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4572\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4573\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4574\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4575\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4576\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4577\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4578\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4579\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4580\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4581\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4582\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4583\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4584\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4585\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4586\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4587\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4588\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4589\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4590\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4591\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4592\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4593\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4594\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4595\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4596\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4597\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4598\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4599\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4600\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4601\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4602\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4603\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4604\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4605\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4606\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4607\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4608\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4609\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4610\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4611\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4612\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4613\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4614\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4615\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4616\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4617\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4618\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4619\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4620\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4621\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4622\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4623\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4624\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4625\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4626\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4627\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4628\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4629\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4630\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4631\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4632\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4633\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4634\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4635\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4636\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4637\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4638\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4639\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4640\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4641\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4642\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4643\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4644\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4645\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4646\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4647\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4648\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4649\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4650\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4651\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4652\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4653\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4654\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4655\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4656\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4657\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4658\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4659\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4660\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4661\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4662\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4663\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4664\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4665\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4666\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4667\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4668\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4669\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4670\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4671\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4672\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4673\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4674\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4675\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4676\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4677\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4678\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4679\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4680\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4681\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4682\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4683\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4684\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4685\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4686\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4687\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4688\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4689\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4690\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4691\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4692\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4693\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4694\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4695\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4696\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4697\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4698\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4699\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4700\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4701\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4702\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4703\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4704\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4705\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4706\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4707\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4708\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4709\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4710\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4711\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4712\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4713\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4714\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4715\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4716\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4717\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4718\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4719\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4720\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4721\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4722\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4723\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4724\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4725\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4726\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4727\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4728\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4729\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4730\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4731\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4732\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4733\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4734\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4735\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4736\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4737\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4738\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4739\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4740\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4741\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4742\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4743\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4744\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4745\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4746\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4747\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4748\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4749\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4750\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4751\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4752\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4753\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4754\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4755\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4756\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4757\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4758\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4759\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4760\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4761\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4762\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4763\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4764\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4765\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4766\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4767\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4768\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4769\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4770\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4771\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4772\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4773\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4774\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4775\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4776\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4777\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4778\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4779\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4780\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4781\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4782\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4783\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4784\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4785\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4786\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4787\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4788\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4789\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4790\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4791\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4792\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4793\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4794\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4795\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4796\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4797\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4798\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4799\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4800\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4801\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4802\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4803\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4804\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4805\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4806\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4807\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4808\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4809\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4810\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4811\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4812\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4813\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4814\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4815\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4816\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4817\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4818\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4819\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4820\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4821\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4822\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4823\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4824\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4825\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4826\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4827\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4828\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4829\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4830\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4831\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4832\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4833\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4834\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4835\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4836\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4837\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4838\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4839\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4840\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4841\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4842\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4843\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4844\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4845\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4846\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4847\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4848\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4849\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4850\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4851\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4852\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4853\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4854\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4855\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4856\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4857\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4858\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4859\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4860\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4861\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4862\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4863\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4864\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4865\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4866\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4867\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4868\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4869\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4870\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4871\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4872\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4873\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4874\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4875\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4876\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4877\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4878\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4879\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4880\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4881\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4882\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4883\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4884\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4885\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4886\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4887\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4888\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4889\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4890\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4891\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4892\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4893\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4894\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4895\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4896\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4897\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4898\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4899\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4900\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4901\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4902\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4903\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4904\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4905\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4906\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4907\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4908\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4909\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4910\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4911\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4912\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4913\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4914\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4915\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4916\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4917\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4918\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4919\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4920\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4921\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4922\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4923\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4924\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4925\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4926\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4927\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4928\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4929\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4930\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4931\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4932\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4933\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4934\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4935\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4936\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4937\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4938\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4939\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4940\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4941\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4942\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4943\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4944\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4945\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4946\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4947\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4948\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4949\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4950\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4951\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4952\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4953\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4954\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4955\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4956\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4957\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4958\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4959\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4960\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4961\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4962\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4963\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4964\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4965\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4966\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4967\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4968\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4969\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4970\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4971\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4972\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4973\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4974\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4975\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4976\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4977\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4978\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4979\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4980\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4981\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4982\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4983\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4984\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4985\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4986\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4987\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4988\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4989\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4990\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4991\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4992\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4993\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4994\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4995\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4996\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4997\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            " epoch = 4998\n",
            "    the train loss= 0.0008967459434643388\n",
            "    the test loss = 0.0005214422708377242\n",
            "---------------------------------------------\n",
            " epoch = 4999\n",
            "    the train loss= 0.00025533660664223135\n",
            "    the test loss = 0.0008256376022472978\n",
            "---------------------------------------------\n",
            "OrderedDict([('weights', tensor([0.1999])), ('bias', tensor([0.3010]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prediction after training"
      ],
      "metadata": {
        "id": "2AaHF-Yde83R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_preds = model0(X_test)\n",
        "\n",
        "y_preds\n",
        "print(\"loss = \", loss_fn(y_preds  , y_test))\n",
        "plot_predictions(X_train , y_train ,X_test ,y_test ,predictions=y_preds )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "4QU6LnUeMGg0",
        "outputId": "3868db11-c3f7-43c2-d9c5-89d3db189054"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss =  tensor(0.0008)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAGsCAYAAACRnqCBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWm0lEQVR4nO3de1yUZd4/8M/M4AxqHALiKAIe8rCpJMhEnkCncPURLXs8ZIqs4ZanFF3TLPGwhZUZK5r2czXNXHUrUtIeShE0Ew9hlHlg84CKCorKjJJymLl+f7BMjgw6g8CcPu/Xa14s133d133dczPrp/vwHYkQQoCIiIiILEJq6QkQEREROTKGMSIiIiILYhgjIiIisiCGMSIiIiILYhgjIiIisiCGMSIiIiILYhgjIiIisiAnS0+gqeh0Oly6dAkuLi6QSCSWng4RERHZOSEEbt68CX9/f0ildZ//cpgwdunSJQQGBlp6GkRERORgLly4gFatWtW53GHCmIuLC4DqN8TV1dXCsyEiIiJ7p9FoEBgYqM8gdXGYMFZzadLV1ZVhjIiIiJrMg26P4g38RERERBbEMEZERERkQQxjRERERBbEMEZERERkQQxjRERERBZUrzC2YsUKBAcHw9nZGUqlEocOHaqz77p16yCRSAxezs7OBn2EEJg3bx78/PzQvHlzqFQq/PbbbwZ9rl+/jtGjR8PV1RXu7u4YP348bt26VZ/pExEREVkNs0tbbNmyBYmJiVi1ahWUSiVSUlIQExOD/Px8eHt7G13H1dUV+fn5+t/vfcTzvffew7Jly7B+/XqEhITgrbfeQkxMDI4fP64PbqNHj8bly5exc+dOVFZWIj4+HhMmTMC//vUvc3fBLJWVldBqtY26DSJrJJPJ0KxZM0tPg4jI7kmEEMKcFZRKJXr06IHly5cDqP6aocDAQEyZMgWzZ8+u1X/dunWYNm0aSktLjY4nhIC/vz9mzJiBmTNnAgDUajV8fHywbt06jBw5EidOnEDnzp1x+PBhhIeHAwAyMjIwcOBAFBYWwt/f/4Hz1mg0cHNzg1qtNqnOmEajQUlJCcrLyx/Yl8heKRQKeHl5sTYfEVE9mJo9zDozVlFRgdzcXMyZM0ffJpVKoVKpkJOTU+d6t27dQlBQEHQ6Hbp374533nkHf/rTnwAAZ8+eRVFREVQqlb6/m5sblEolcnJyMHLkSOTk5MDd3V0fxABApVJBKpXi4MGDeO6552pts7y83CBIaTQak/dTo9Hg4sWLeOSRR+Dl5YVmzZrx+yzJoQghUFlZCbVajYsXLwIAAxkRUSMxK4yVlJRAq9XCx8fHoN3HxwcnT540uk6HDh2wdu1adO3aFWq1GkuWLMHTTz+NY8eOoVWrVigqKtKPce+YNcuKiopqXQJ1cnKCh4eHvs+9kpOTsWDBAnN2z2A/H3nkEbRq1YohjBxW8+bN4eLigsLCQpSUlDCMERE1kkZ/mjIyMhJjx45FaGgo+vbti7S0NDz22GP4+OOPG3W7c+bMgVqt1r8uXLhg0nqVlZUoLy+Hm5sbgxg5PIlEAjc3N5SXl6OystLS0yEisktmhTEvLy/IZDIUFxcbtBcXF8PX19ekMZo1a4Ynn3wSp06dAgD9evcb09fXF1euXDFYXlVVhevXr9e5XYVCof8eSnO+j7LmZn3euExUreazwAdZiIgah1lhTC6XIywsDJmZmfo2nU6HzMxMREZGmjSGVqvF0aNH4efnBwAICQmBr6+vwZgajQYHDx7UjxkZGYnS0lLk5ubq++zevRs6nQ5KpdKcXTAZz4oRVeNngYiocZld2iIxMRFxcXEIDw9HREQEUlJSUFZWhvj4eADA2LFjERAQgOTkZADAwoUL8dRTT6Fdu3YoLS3F+++/j3PnzuHll18GUP1/9NOmTcPf//53tG/fXl/awt/fH0OHDgUAdOrUCQMGDEBCQgJWrVqFyspKTJ48GSNHjjTpSUoiIiIiA+npQFYWEB0NxMZadCpmh7ERI0bg6tWrmDdvHoqKihAaGoqMjAz9Dfjnz5+HVPrHCbcbN24gISEBRUVFePTRRxEWFob9+/ejc+fO+j6zZs1CWVkZJkyYgNLSUvTq1QsZGRkGxWE3btyIyZMno3///pBKpRg2bBiWLVv2MPtOREREjig9HRgyBFqpBLKUFGDbNosGMrPrjNkqU2t93LlzB2fPnkVISEitbwogckT8TBCRvTk9LhZBn34NJwFUSYBzY2PRdt22Bt+OqdmD301JVkEikSAqKuqhxsjOzoZEIsH8+fMbZE6NLTg4GMHBwZaeBhGRw8kKhj6IOQkgO9iy82EYI717v0P0QS+yvKioKB4LIiIzeY96GbEjgeVPSRA7Enhs1HiLzsfse8bIfiUlJdVqS0lJgVqtNrqsIZ04cQItWrR4qDEiIiJw4sQJeHl5NdCsiIjIHsV2iAXmb0N2QTZeDo6q/t2CeM/YPXh/jKHg4GCcO3cODvJn0qRqLlEWFBTUe4yoqCjs2bOnUY8PPxNERPXDe8ao0RQUFEAikWDcuHE4ceIEnnvuOXh6ekIikeiDxVdffYVRo0ahXbt2aNGiBdzc3NC7d298+eWXRsc0ds/YuHHjIJFIcPbsWSxbtgwdO3aEQqFAUFAQFixYAJ1OZ9C/rnvGau7NunXrFl577TX4+/tDoVCga9eu+OKLL+rcxxEjRsDDwwOPPPII+vbti71792L+/PmQSCTIzs42+f3atm0bevTogebNm8PHxwcJCQm4ceOG0b7/+c9/MGvWLHTv3h2enp5wdnbG448/jtmzZ+PWrVu13rM9e/bo/3fNa9y4cfo+a9euxZAhQxAcHAxnZ2d4eHggJiYGWVlZJs+fiMimpKcD06dX/7QRvExJ9Xbq1Ck89dRT6NKlC8aNG4dr165BLpcDqP46Krlcjl69esHPzw9Xr15Feno6XnjhBSxbtgxTpkwxeTt/+9vfsGfPHvzP//wPYmJisHXrVsyfPx8VFRV4++23TRqjsrISzz77LG7cuIFhw4bh999/x+bNmzF8+HBkZGTg2Wef1fe9ePEinn76aVy+fBkDBgzAk08+ifz8fDzzzDPo16+fWe/Rp59+iri4OLi6umLMmDFwd3fH9u3boVKpUFFRoX+/aqSlpWHNmjWIjo5GVFQUdDodDhw4gHfffRd79uzB3r179RXxk5KSsG7dOpw7d87gMnJoaKj+f0+aNAndunWDSqXCY489hosXL2Lr1q1QqVRIS0vDkCFDzNofIiKrZmUlK0wmHIRarRYAhFqtvm+/27dvi+PHj4vbt2830cysW1BQkLj3z+Ts2bMCgAAg5s2bZ3S906dP12q7efOm6NKli3BzcxNlZWUGywCIvn37GrTFxcUJACIkJERcunRJ33716lXh7u4uXFxcRHl5ub49KytLABBJSUlG92HIkCEG/Xft2iUAiJiYGIP+L730kgAg3n77bYP2NWvW6Pc7KyvL6H7fTa1WC1dXV9GyZUuRn5+vb6+oqBB9+vQRAERQUJDBOoWFhQZzrLFgwQIBQHz22WcG7X379q11fO525syZWm2XLl0S/v7+on379g/cByH4mSAi23EqbrColEAIQFRKIE7FxVp0PqZmD16mtCAbPJNqwNfXF3PnzjW6rE2bNrXaHnnkEYwbNw5qtRqHDx82eTtvvfWW/uuzgOrvSB0yZAhu3ryJ/Px8k8f58MMPDc5E9e/fH0FBQQZzKS8vx+effw5vb2/MmDHDYP34+Hh06NDB5O1t3boVGo0Gf/nLX/D444/r25s1a1bnGb2AgIBaZ8sAYPLkyQCAXbt2mbx9oPrrxu7l5+eHYcOG4bfffsO5c+fMGo+IyJpZW8kKUzGMWch/z6QiNbX6py0Gsm7duhkNDgBw5coVJCYmolOnTmjRooX+fqaagHPp0iWTtxMWFlarrVWrVgCA0tJSk8Zwd3c3GkxatWplMEZ+fj7Ky8sRHh4OhUJh0FcikeDpp582ed4///wzAKB37961lkVGRsLJqfZdAkIIrF27Fn369IGHhwdkMhkkEgk8PT0BmPe+AcCZM2eQkJCAtm3bwtnZWX8cUlNT6zUeEZE1s7aSFabiPWMWkpUFyGSAVlv9MzvbNi5r363mK7Dudf36dfTo0QPnz59Hz549oVKp4O7uDplMhry8PGzbtg3l5eUmb8fYEyg1QUar1Zo0hpubm9F2JycngwcBNBoNAMDb29to/7r22Ri1Wl3nWDKZTB+w7jZ16lQsX74cgYGBiI2NhZ+fnz4ULliwwKz37dSpU4iIiIBGo0F0dDQGDx4MV1dXSKVSZGdnY8+ePWaNR0Rk7aytZIWpGMYsJDoaSEn5I5A9ZPF5i6ir2OiaNWtw/vx5LFq0CG+++abBssWLF2Pbtob/yomGUhP8rly5YnR5cXGxyWPVBEBjY2m1Wly7dg0BAQH6titXrmDFihXo2rUrcnJyDOquFRUVYcGCBSZvG6i+LHvjxg1s2LABL730ksGyV155Rf8kJhGRPYntEGszIawGL1NaSGxs9UMeU6fazsMepjp9+jQAGH1S7/vvv2/q6ZilQ4cOUCgUyM3NrXXWSAiBnJwck8fq1q0bAOP7nJOTg6qqKoO2M2fOQAgBlUpVqwBuXe+bTCYDYPwMYV3HQQiBH374wcS9ICKyErZ+o/V9MIxZUGwssHSpfQUxAAgKCgIA7Nu3z6D9X//6F7755htLTMlkCoUCL7zwAoqLi5GSkmKw7NNPP8XJkydNHmvIkCFwdXXF2rVr8Z///EffXllZWeuMIfDH+7Z//36DS6eFhYWYM2eO0W14eHgAAC5cuFDnePceh8WLF+PXX381eT+IiCyupmTFsn/Y7o3W98HLlNTgxowZg3fffRdTpkxBVlYWgoKC8PPPPyMzMxPPP/880tLSLD3F+0pOTsauXbswe/Zs7NmzR19nbPv27RgwYAAyMjIglT74v2Pc3NywbNkyjBs3Dj169MDIkSPh5uaG7du3o3nz5gZPiAJ/POX45ZdfIjw8HP3790dxcTG2b9+O/v3768903a1fv3744osvMGzYMPz5z3+Gs7MzunXrhsGDB+OVV17BJ598gmHDhmH48OHw9PTEgQMHcOTIEQwaNAg7duxosPeMiKgxnU77J4IkgJNOoEoCnEtbg7Z2dCaDZ8aowbVq1Qp79uxB//79sWvXLnz88ceoqKjAd999h8GDB1t6eg8UGBiInJwc/O///i/279+PlJQUXLlyBd999x3atWsHwPhDBcbExcXhq6++Qvv27bF+/XqsX78ePXv2xK5du4w+ibpu3TrMmDEDN27cQGpqKg4cOIDExET861//Mjp+QkICZs2ahZKSErz77rt466239N9y8OSTT+K7775D9+7dkZaWhrVr18Ld3R0//PADwsPD6/nuEBE1PVstWWEqfjflPfg9fHQ/vXr1Qk5ODtRqNR555BFLT6dJ8DNBRJaWnp+Of84fgn7nJNgdJPDy/G02cZO+qdmDlymJjLh8+XKty4ifffYZfvjhBzz77LMOE8SIiKyBrZasMBXDGJERTzzxBJ588kl07txZXx8tOzsbLi4uWLJkiaWnR0TkcGyxZIWpGMaIjHjllVfw9ddf48cff0RZWRkee+wxvPjii3jrrbfQsWNHS0+PiMh+pKdXV0KPjra/8gIm4j1j9+D9MUSG+JkgokZT892ANRXQ7azwpqnZg09TEhERkWVkZUEnkwJabfXP7GxLz8giGMaIiIjIIg4+3gJSrQ5VEkCq1eFg++aWnpJF8J4xIiIisojNIb8jeZQUfc7qsDdEijZtbkNp6UlZAMMYERERWUR0SDRSOqRge0cZtEKLbcFRlp6SRTCMERERkUXEdojFtpHV9cOi7LB+mKkYxoiIiKjhmViywp7rh5mKN/ATERFRw6opWZGaWv0zPd3SM7JqDGNERETUsFiywiwMY2QzoqKiIJFILD0Nk6xbtw4SiQTr1q2z9FSIiJocS1aYh2GM9CQSiVmvhjZ//nxIJBJk87+gAADZ2dmQSCSYP3++padCRGSWzSG/Y+goKZYpgaGjpNjS5ralp2TVeAM/6SUlJdVqS0lJgVqtNrqsqX366af4/fffLT0NIiJ6AJasMA/DGOkZOwOzbt06qNVqqzg707p1a0tPgYiITMCSFebhZUqql4qKCixduhTdu3dHy5Yt4eLigt69eyPdyBMzarUa8+bNQ+fOnfHII4/A1dUV7dq1Q1xcHM6dOweg+n6wBQsWAACio6P1l0KDg4P14xi7Z+zue7O+++47PP3002jRogU8PT0RFxeHa9euGZ3/xx9/jD/96U9wdnZGYGAgZs2ahTt37kAikSAqKsrk9+H69et45ZVX4OPjgxYtWqBHjx746quv6uy/du1aDBkyBMHBwXB2doaHhwdiYmKQlZVl0G/+/PmIjo4GACxYsMDg8nBBQQEA4D//+Q9mzZqF7t27w9PTE87Oznj88ccxe/Zs3Lp1y+R9ICIyS3o6MH36A5+QjO0Qi6UxSxnETMAzY2S28vJyDBgwANnZ2QgNDcX48eNRWVmJHTt2YMiQIUhNTcXkyZMBAEIIxMTE4ODBg+jZsycGDBgAqVSKc+fOIT09HWPGjEFQUBDGjRsHANizZw/i4uL0Iczd3d2kOaWnp2PHjh0YPHgwnn76aezduxeffvopTp8+jX379hn0nTdvHhYtWgQfHx8kJCSgWbNm+Pe//42TJ0+a9T78/vvviIqKwtGjRxEZGYm+ffviwoULGDFiBJ599lmj60yaNAndunWDSqXCY489hosXL2Lr1q1QqVRIS0vDkCFDAFQHz4KCAqxfvx59+/Y1CIg170laWhrWrFmD6OhoREVFQafT4cCBA3j33XexZ88e7N27F82aNTNrn4iI7qumZIVMBqSkANu23beGGJlI1MPy5ctFUFCQUCgUIiIiQhw8eNCk9TZt2iQAiCFDhhi0AzD6eu+99/R9goKCai1PTk42ec5qtVoAEGq1+r79bt++LY4fPy5u375t8tj2rOZ9v9sbb7whAIi33npL6HQ6fbtGoxHh4eFCLpeLixcvCiGE+OWXXwQAMXTo0Fpj37lzR9y8eVP/e1JSkgAgsrKyjM6lb9++tebyySefCADCyclJ7Nu3T99eVVUloqKiBACRk5Ojb8/PzxcymUwEBASI4uJig7l37txZABB9+/Z98Btz13wTEhIM2jMyMvR/o5988onBsjNnztQa59KlS8Lf31+0b9/eoD0rK0sAEElJSUa3X1hYKMrLy2u1L1iwQAAQn332mUn78SD8TBCR3rRpQiuTCgFU/5w+3dIzsmqmZg+zL1Nu2bIFiYmJSEpKwpEjR9CtWzfExMTgypUr912voKAAM2fORO/evWstu3z5ssFr7dq1kEgkGDZsmEG/hQsXGvSbMmWKudO3Kun56ZieMR3p+bZTDE+n02HlypVo27at/vJZDRcXF8ybNw8VFRVIS0szWK9589qPNSsUCjzyyCMNMq8XX3wRPXv21P8uk8kQFxcHADh8+LC+fdOmTdBqtZgxYwa8vb0N5v7mm2+atc1PP/0UcrkcCxcuNGiPiYlB//79ja4TEhJSq83Pzw/Dhg3Db7/9pr9sa4qAgADI5fJa7TVnJXft2mXyWEREpmDJisZh9mXKpUuXIiEhAfHx8QCAVatWYceOHVi7di1mz55tdB2tVovRo0djwYIF+P7771FaWmqw3NfX1+D3bdu2ITo6Gm3atDFod3FxqdXXVqXnp2PI5iGQSWRIOZiCbSO32cR19fz8fNy4cQP+/v76e7zudvXqVQDQX/Lr1KkTunbtik2bNqGwsBBDhw5FVFQUQkNDIZU23C2LYWFhtdpatWoFAAZ/bz///DMAoFevXrX63x3mHkSj0eDs2bPo3Lmz0b/J3r17IzMzs1b7mTNnkJycjN27d+PixYsoLy83WH7p0iUEBQWZNAchBD755BOsW7cOv/76K9RqNXQ6ncFYREQNaXPI70geJUWfszrsDZGiTZvbUFp6UnbArDBWUVGB3NxczJkzR98mlUqhUqmQk5NT53oLFy6Et7c3xo8fj++///6+2yguLsaOHTuwfv36WssWL16MRYsWoXXr1njxxRcxffp0ODkZ34Xy8nKDf+g0Gs2Ddq9JZZ3NgkxS/civTCJDdkG2TYSx69evAwCOHTuGY8eO1dmvrKwMAODk5ITdu3dj/vz5+PLLLzFjxgwAwGOPPYbJkydj7ty5kMlkDz0vV1fXWm01fxtarVbfVvN3cPdZsRo+Pj4mb+9+49Q11qlTpxAREQGNRoPo6GgMHjwYrq6ukEqlyM7Oxp49e2qFs/uZOnUqli9fjsDAQMTGxsLPzw8KhQJA9U3/5oxFRGQKlqxoHGaFsZKSEmi12lr/0Pj4+NR58/O+ffuwZs0a5OXlmbSN9evXw8XFBc8//7xB+9SpU9G9e3d4eHhg//79mDNnDi5fvoylS5caHSc5OdnomRtrER0SjZSDKfpAFmUjf9A1oWfYsGH44osvTFrH09MTqampWLZsGU6ePIndu3cjNTUVSUlJaNasmUG4b2w1879y5UqtM1DFxcX1GscYY2N9+OGHuHHjBjZs2ICXXnrJYNkrr7yCPXv2mLz9K1euYMWKFejatStycnLQokUL/bKioiKr/tsnItvFkhWNo1FLW9y8eRNjxozB6tWr4eXlZdI6a9euxejRo+Hs7GzQnpiYiKioKHTt2hWvvPIKPvjgA6Smptb5X/9z5syBWq3Wvy5cuPDQ+9OQav6gpyqn2swlSqD6sqOrqyt+/PFHVFZWmrWuRCJBp06dMGnSJOzcuRMADEph1Jwhu/tMVkPr1q0bAOCHH36otWz//v0mj+Pq6oqQkBCcOnUKRUVFtZYbOwN8+vRpANA/MVlDCGF0Pvd7P86cOQMhBFQqlUEQq2vbREQPxJIVFmNWGPPy8oJMJqv1X/3FxcVG75s5ffo0CgoKMHjwYDg5OcHJyQmffvop0tPT4eTkpP/Hqcb333+P/Px8vPzyyw+ci1KpRFVVlb7m0r0UCgVcXV0NXtbGFv+gnZyc8Oqrr+LcuXOYOXOm0UD266+/6s8YFRQUGD1GNX9Dd4duDw8PAGjU4Dxy5EhIpVJ88MEHKCkp0beXlZXh7bffNmusMWPGoKKiAvPmzTNo/+6774zeL1ZzJu7eUhuLFy/Gr7/+Wqv//d6PmrH2799vcJ9YYWFhk55pJCI7UVOyIjW1+ucDAhk1LLMuU8rlcoSFhSEzMxNDhw4FUP10XWZmpv4Jrrt17NgRR48eNWh78803cfPmTfzjH/9AYGCgwbI1a9YgLCxMf/bifvLy8iCVSuu8Z4caz4IFC3DkyBEsW7YMO3bsQJ8+feDt7Y2LFy/i6NGj+Pnnn5GTkwNvb2/k5eXh+eefR0REhP5m95raWlKpFNOnT9ePW1Ps9Y033sCxY8fg5uYGd3d3o39b9dWhQwfMnj0b77zzDrp06YLhw4fDyckJaWlp6NKlC3799VeTHyyYNWsW0tLSsHr1ahw7dgx9+vTBhQsX8O9//xuDBg3Cjh07DPq/8sor+OSTTzBs2DAMHz4cnp6eOHDgAI4cOWK0f8eOHeHv74/NmzdDoVCgVatWkEgkmDJliv4JzC+//BLh4eHo378/iouLsX37dvTv37/Wf+gQEd1XVhZ0MimkWm31z+xs1g9rSubWzNi8ebNQKBRi3bp14vjx42LChAnC3d1dFBUVCSGEGDNmjJg9e3ad68fFxdWqM1ZTi6NFixZi5cqVtZbt379ffPjhhyIvL0+cPn1afPbZZ+Kxxx4TY8eONXnerDNWP8bqjAlRXcfr448/Fj179hSurq5CoVCI1q1biwEDBoiVK1eKW7duCSGEuHDhgpg9e7Z46qmnhLe3t5DL5aJ169bi+eefN6j/VWPdunWiS5cuQqFQCAAiKChIv+x+dcbureclxP3rdH300UeiU6dOQi6Xi1atWomZM2eKCxcuGK2Ddz/Xrl0TEyZMEI899phwdnYWYWFhIi0trc55ZWVliZ49ewoXFxfh7u4uBg4cKHJzc+ussXbgwAHRt29f4eLioq9ddvbsWSGEEDdv3hQzZswQwcHBQqFQiPbt24tFixaJiooKs+qlPQg/E0T278BHbwgBiEoJhACqf6eHZmr2qFfR19TUVNG6dWshl8tFRESEOHDggH5Z3759RVxcXJ3r1hXGPv74Y9G8eXNRWlpaa1lubq5QKpXCzc1NODs7i06dOol33nlH3Llzx+Q5M4zRg+zcuVMAELNmzbL0VKwKPxNE9m/a/00TQ0ZJxQdPQQwZJRXTM1jMtSGYmj0kQgjR9Ofjmp5Go4GbmxvUavV97x+7c+cOzp49i5CQkFoPEZB9uHr1Kjw8PAxKapSWluKZZ57Bjz/+iP379yMyMtKCM7Qu/EwQ2b+7a19qhdamHiyzZqZmD343JTmcjRs3YsmSJejXrx/8/f1x+fJlZGRk4MqVKxg3bhyDGBE5HJassCyGMXI4Tz/9NMLCwrBr1y5cv34dMpkMnTp1wltvvYWJEydaenpERBYR2yGWIcxCGMbI4URERGDbtm2WngYRUeNLTweysoDoaD4dacUategrERERWch/a4dpl/2DtcOsHMMYERGRHTqd9k9USQCZTqBKApxOW2PpKVEdGMaIiIjsUFYw4CSAKkn1z+xgS8+I6sJ7xoiIiOyQ96iXEZv/Nfqdk2B3kMDLo8ZbekpUB4YxIiIiOxTbIRaYX12u4mWWq7BqDGNERER2iuUqbAPvGSMiIrI16enA9Ol8QtJOMIwRERHZEpassDsMY0RERDaEJSvsD8MY2YSCggJIJBKMGzfOoD0qKgoSiaTRthscHIzg4OBGG5+IyFwsWWF/GMaolprgc/dLLpcjMDAQL774In755RdLT7HBjBs3DhKJBAUFBZaeChGRSbxHvYzYkcDypySIHQk8xpIVNo9PU1Kd2rZti5deegkAcOvWLRw4cACbNm1CWloaMjMz0bNnTwvPEPj000/x+++/N9r4mZmZjTY2EVF9sGSF/WEYozq1a9cO8+fPN2h788038fbbb2Pu3LnIzs62yLzu1rp160Ydv23bto06PhFRfbBkhX3hZUoyy5QpUwAAhw8fBgBIJBJERUXh4sWLGDt2LHx9fSGVSg2C2t69ezF48GB4eXlBoVCgffv2ePPNN42e0dJqtXj33XfRrl07ODs7o127dkhOToZOpzM6n/vdM7Zt2zY8++yz8PT0hLOzM4KDgzFmzBj8+uuvAKrvB1u/fj0AICQkRH9JNioqSj9GXfeMlZWVISkpCR07doSzszM8PDwwaNAg/PDDD7X6zp8/HxKJBNnZ2fjXv/6F0NBQNG/eHH5+fnjttddw+/btWut8+eWX6Nu3L7y9veHs7Ax/f3+oVCp8+eWXRveViOwES1Y4JJ4Zo3q5OwBdu3YNkZGR8PDwwMiRI3Hnzh24uroCAFauXIlJkybB3d0dgwcPhre3N3788Ue8/fbbyMrKQlZWFuRyuX6sCRMmYO3atQgJCcGkSZNw584dLF26FPv37zdrfjNmzMDSpUvh4eGBoUOHwtvbGxcuXMCuXbsQFhaGJ554AtOmTcO6devw888/47XXXoO7uzsAPPCG/Tt37qBfv344dOgQunfvjmnTpqG4uBhbtmzBt99+i02bNuF///d/a623fPlyZGRkYMiQIejXrx8yMjKwbNkylJSUYOPGjfp+K1euxMSJE+Hn54fnnnsOnp6eKCoqwqFDh/DVV19h2LBhZr0XRGQjakpWSCWQpaQA27YBsTz75RCEg1Cr1QKAUKvV9+13+/Ztcfz4cXH79u0mmpn1OXv2rAAgYmJiai2bN2+eACCio6OFEEIAEABEfHy8qKqqMuh77Ngx4eTkJLp16yZKSkoMliUnJwsAYsmSJfq2rKwsAUB069ZN3Lp1S99eWFgovLy8BAARFxdnME7fvn3FvX/GX3/9tQAgunTpUmu7lZWVoqioSP97XFycACDOnj1r9L0ICgoSQUFBBm0LFiwQAMTo0aOFTqfTtx85ckTI5XLh7u4uNBqNvj0pKUkAEG5ubuLkyZP69t9//108/vjjQiqViosXL+rbu3fvLuRyuSguLq41n3v3pynwM0HUNE7FDRaVEggBiEoJxKm4WEtPiR6SqdmDlyktycpPR586dQrz58/H/Pnz8be//Q19+vTBwoUL4ezsjLffflvfTy6X47333oNMJjNY/+OPP0ZVVRVSU1Ph6elpsGzWrFl47LHHsGnTJn3bp59+CgCYN28eWrZsqW8PCAjAa6+9ZvK8P/roIwDAP/7xj1rbdXJygo+Pj8ljGbN+/Xo0a9YMixcvNjhD+OSTTyIuLg6lpaXYunVrrfVee+01dOjQQf978+bNMWrUKOh0OuTm5hr0bdasGZo1a1ZrjHv3h4jsB0tWOC5eprSU/56OhkwGWOnp6NOnT2PBggUAqsOBj48PXnzxRcyePRtdunTR9wsJCYGXl1et9Q8cOAAA+Pbbb40+ldisWTOcPHlS//vPP/8MAOjdu3etvsba6nLo0CEoFAr07dvX5HVMpdFocObMGXTq1AmtWrWqtTw6OhqrV69GXl4exowZY7AsLCysVv+aMUpLS/VtI0eOxKxZs/DEE0/gxRdfRHR0NHr16qW/9EtE9sl71MuIzf8a/c5JsDtI4GWWrHAYDGOWkpVVHcS02uqf2dlWF8ZiYmKQkZHxwH51nWm6fv06ABicRbsftVoNqVRqNNiZczZLrVYjICAAUmnDn/jVaDT3nY+fn59Bv7sZC1NOTtUfQa1Wq2+bOXMmPD09sXLlSnzwwQdYsmQJnJycMGjQIHz44YcICQl56P0gIuvDkhWOi5cpLSU6+o8gptUCdz3BZ2vqepqxJnxoNBoIIep81XBzc4NOp0NJSUmtsYqLi02ej7u7O4qKiup8AvNh1OxTXfMpKioy6FcfEokEf/nLX3D48GFcvXoVX331FZ5//nls27YN//M//2MQ3IjIvsR2iMXSmKUMYg6GYcxSYmOrL01OnWqVlygbglKpBPDH5coH6datGwDg+++/r7XMWFtdIiIiUF5ejj179jywb819bqYGHFdXV7Rp0wanTp3CxYsXay2vKekRGhpq8nzvx9PTE0OHDsWWLVvQr18/HD9+HKdOnWqQsYmIyDowjFlSbCywdKldBjEAmDhxIpycnDBlyhScP3++1vLS0lL89NNP+t9r7rFauHAhysrK9O0XL17EP/7xD5O3O2nSJADVN8zXXCqtUVVVZXBWy8PDAwBw4cIFk8ePi4tDZWUl5syZY3Bm75dffsG6devg5uaGoUOHmjzevbKzsw3GBYDKykr9vjg7O9d7bCKyDCt/XossjPeMUaN54okn8NFHH+HVV19Fhw4dMHDgQLRt2xY3b97EmTNnsGfPHowbNw6rVq0CUH3ze3x8PD755BN06dIFzz33HMrLy7FlyxY89dRT2L59u0nbHThwIGbOnIklS5agffv2eO655+Dt7Y2LFy8iMzMTM2fOxLRp0wAA/fr1w5IlSzBhwgQMGzYMLVu2RFBQUK2b7+82a9Ys7NixAxs2bMCJEyfQv39/XLlyBVu2bEFVVRVWr14NFxeXer9vQ4cOhaurK5566ikEBQWhsrISO3fuxPHjx/HCCy8gKCio3mMTUdOzgee1yMIYxqhRJSQkIDQ0FEuXLsXevXvx9ddfw83NDa1bt8b06dMRFxdn0H/16tV4/PHHsXr1aixfvhytWrVCYmIihg8fbnIYA4D3338fkZGRWL58Ob744gvcuXMHfn5+6NevH5555hl9vz//+c947733sHr1anzwwQeorKxE37597xvGnJ2dsXv3brz77rvYsmULPvzwQ7Ro0QJ9+/bFG2+8gV69epn/Rt0lOTkZGRkZOHToEL7++mu0bNkSbdu2xcqVKzF+PJ+uIrI1NvC8FlmYRNx7PcROaTQauLm5Qa1W3/fm6jt37uDs2bMICQnh5SAi8DNB9LDuPjOm1fLMmCMxNXvwzBgREVEjqnleKzu7+sF5BjG6F8MYERFRI4uNZQijuvFpSiIiIiILYhgjIiKqJ5asoIbAMEZERFQPNTfmp6ZW/2Qgo/qqVxhbsWIFgoOD4ezsDKVSiUOHDpm03ubNmyGRSGoVxBw3bhwkEonBa8CAAQZ9rl+/jtGjR8PV1RXu7u4YP348bt26VZ/pExERPTRjJSuI6sPsMLZlyxYkJiYiKSkJR44cQbdu3RATE4MrV67cd72CggLMnDkTvXv3Nrp8wIABuHz5sv61adMmg+WjR4/GsWPHsHPnTmzfvh179+7FhAkTzJ2+yRyk4gfRA/GzQGScHX3FMFmY2WFs6dKlSEhIQHx8PDp37oxVq1ahRYsWWLt2bZ3raLVajB49GgsWLECbNm2M9lEoFPD19dW/Hn30Uf2yEydOICMjA//85z+hVCrRq1cvpKamYvPmzbh06ZK5u3BfNd9VWFlZ2aDjEtmqms9CzWeDiKo5wFcMUxMxK4xVVFQgNzcXKpXqjwGkUqhUKuTk5NS53sKFC+Ht7X3f6uHZ2dnw9vZGhw4d8Oqrr+LatWv6ZTk5OXB3d0d4eLi+TaVSQSqV4uDBg0bHKy8vh0ajMXiZolmzZlAoFFCr1TwjQA5PCAG1Wg2FQoFmzZpZejpEVsfOv2KYmohZdcZKSkqg1Wrh4+Nj0O7j44OTJ08aXWffvn1Ys2YN8vLy6hx3wIABeP755xESEoLTp0/jjTfewJ///Gfk5ORAJpOhqKgI3t7ehhN3coKHhweKioqMjpmcnIwFCxaYs3t6Xl5euHjxIgoLC+Hm5oZmzZpBIpHUaywiWySEQGVlJdRqNW7duoWAgABLT4mIyG41atHXmzdvYsyYMVi9ejW8vLzq7Ddy5Ej9/+7SpQu6du2Ktm3bIjs7G/3796/XtufMmYPExET97xqNBoGBgSatW/OVBSUlJbh48WK9tk9kDxQKBQICAu77NR5ERPRwzApjXl5ekMlkKC4uNmgvLi6Gr69vrf6nT59GQUEBBg8erG/T6XTVG3ZyQn5+Ptq2bVtrvTZt2sDLywunTp1C//794evrW+sBgaqqKly/ft3odoHqf0QUCoU5u2fA1dUVrq6uqKyshFarrfc4RLZKJpPx0iQ5rPT06qclo6N5CZIan1lhTC6XIywsDJmZmfryFDqdDpmZmZg8eXKt/h07dsTRo0cN2t58803cvHkT//jHP+o8U1VYWIhr167Bz88PABAZGYnS0lLk5uYiLCwMALB7927odDoolUpzdsFszZo14z9IREQO5O4v9k5J4c351PjMvkyZmJiIuLg4hIeHIyIiAikpKSgrK0N8fDwAYOzYsQgICEBycjKcnZ3xxBNPGKzv7u4OAPr2W7duYcGCBRg2bBh8fX1x+vRpzJo1C+3atUNMTAwAoFOnThgwYAASEhKwatUqVFZWYvLkyRg5ciT8/f0fZv+JiIgMGKsfxjBGjcnsMDZixAhcvXoV8+bNQ1FREUJDQ5GRkaG/qf/8+fOQSk1/SFMmk+GXX37B+vXrUVpaCn9/fzz77LNYtGiRwWXGjRs3YvLkyejfvz+kUimGDRuGZcuWmTt9IiKi+4qOrj4jxvph1FQkwkHqN2g0Gri5uUGtVvNmZCIiuq/09OozYlFRPCtG9Wdq9mjUpymJiIhsUWwsQxg1HX5ROBEREZEFMYwREZHDSE8Hpk+v/klkLRjGiIjIIdSUrEhNrf7JQEbWgmGMiIgcgrGSFUTWgGGMiIgcQnT0H0GMJSvImvBpSiIicgixsdXV9FmygqwNwxgRETkMlqwga8TLlEREREQWxDBGREQ2jyUryJYxjBERkU1jyQqydQxjRERk01iygmwdwxgREdk0lqwgW8enKYmIyKaxZAXZOoYxIiKyeSxZQbaMlymJiIiILIhhjIiIrBZLVpAjYBgjIiKrxJIV5CgYxoiIyCqxZAU5CoYxIiKySixZQY6CT1MSEZFVYskKchQMY0REZLVYsoIcAS9TEhEREVkQwxgRETU5lqwg+gPDGBERNSmWrCAyxDBGRERNiiUriAwxjBERUZNiyQoiQ3yakoiImhRLVhAZYhgjIqImx5IVRH/gZUoiIiIiC2IYIyKiBsOSFUTmYxgjIqIGwZIVRPXDMEZERA2CJSuI6odhjIiIGgRLVhDVT73C2IoVKxAcHAxnZ2colUocOnTIpPU2b94MiUSCoUOH6tsqKyvx+uuvo0uXLmjZsiX8/f0xduxYXLp0yWDd4OBgSCQSg9fixYvrM30iImoENSUrpk6t/smnJYlMIxFCCHNW2LJlC8aOHYtVq1ZBqVQiJSUFn3/+OfLz8+Ht7V3negUFBejVqxfatGkDDw8PbN26FQCgVqvxwgsvICEhAd26dcONGzfw2muvQavV4scff9SvHxwcjPHjxyMhIUHf5uLigpYtW5o0b41GAzc3N6jVari6upqzy0RERERmMzV7mB3GlEolevTogeXLlwMAdDodAgMDMWXKFMyePdvoOlqtFn369MFf/vIXfP/99ygtLdWHMWMOHz6MiIgInDt3Dq1btwZQHcamTZuGadOmmTTP8vJylJeX63/XaDQIDAxkGCMiIqImYWoYM+syZUVFBXJzc6FSqf4YQCqFSqVCTk5OnestXLgQ3t7eGD9+vEnbUavVkEgkcHd3N2hfvHgxPD098eSTT+L9999HVVVVnWMkJyfDzc1N/woMDDRp20REVBtLVhA1HrMq8JeUlECr1cLHx8eg3cfHBydPnjS6zr59+7BmzRrk5eWZtI07d+7g9ddfx6hRowxS5NSpU9G9e3d4eHhg//79mDNnDi5fvoylS5caHWfOnDlITEzU/15zZoyIiMxTU7JCJgNSUng/GFFDa9SvQ7p58ybGjBmD1atXw8vL64H9KysrMXz4cAghsHLlSoNldwerrl27Qi6X469//SuSk5OhUChqjaVQKIy2ExGReYyVrGAYI2o4ZoUxLy8vyGQyFBcXG7QXFxfD19e3Vv/Tp0+joKAAgwcP1rfpdLrqDTs5IT8/H23btgXwRxA7d+4cdu/e/cD7upRKJaqqqlBQUIAOHTqYsxtERGSG6OjqM2IsWUHUOMy6Z0wulyMsLAyZmZn6Np1Oh8zMTERGRtbq37FjRxw9ehR5eXn6V2xsLKKjo5GXl6e/bFgTxH777Tfs2rULnp6eD5xLXl4epFLpfZ/gJCKih8eSFUSNy+zLlImJiYiLi0N4eDgiIiKQkpKCsrIyxMfHAwDGjh2LgIAAJCcnw9nZGU888YTB+jU35de0V1ZW4oUXXsCRI0ewfft2aLVaFBUVAQA8PDwgl8uRk5ODgwcPIjo6Gi4uLsjJycH06dPx0ksv4dFHH32Y/SciIhPExjKEETUWs8PYiBEjcPXqVcybNw9FRUUIDQ1FRkaG/qb+8+fPQyo1/YTbxYsXkf7fx3NCQ0MNlmVlZSEqKgoKhQKbN2/G/PnzUV5ejpCQEEyfPt3gPjIiIiIiW2R2nTFbxaKvRES1padX36AfHc0zX0QNrVHqjBERkf2oKVmRmlr9kzXEiCyDYYyIyEEZK1lBRE2PYYyIyEFFR/8RxFiygshyGrXoKxERWa+akhXZ2dVBjPeMEVkGwxgRkQNjyQoiy+NlSiIiIiILYhgjIrJD6enA9Ol8QpLIFjCMERHZGZasILItDGNERHaGJSuIbAvDGBGRnWHJCiLbwqcpiYjsDEtWENkWhjEiIjvEkhVEtoOXKYmIiIgsiGGMiIiIyIIYxoiIbAjrhxHZH4YxIiIbwfphRPaJYYyIyEawfhiRfWIYIyKyEawfRmSfWNqCiMhGsH4YkX1iGCMisiGsH0Zkf3iZkoiIiMiCGMaIiKwAS1YQOS6GMSIiC2PJCiLHxjBGRGRhLFlB5NgYxoiILIwlK4gcG5+mJCKyMJasIHJsDGNERFaAJSuIHBcvUxIRERFZEMMYEVEjYbkKIjIFwxgRUSNguQoiMhXDGBFRI2C5CiIyFcMYEVEjYLkKIjIVn6YkImoELFdBRKaq15mxFStWIDg4GM7OzlAqlTh06JBJ623evBkSiQRDhw41aBdCYN68efDz80Pz5s2hUqnw22+/GfS5fv06Ro8eDVdXV7i7u2P8+PG4detWfaZPRNQkYmOBpUsZxIjo/swOY1u2bEFiYiKSkpJw5MgRdOvWDTExMbhy5cp91ysoKMDMmTPRu3fvWsvee+89LFu2DKtWrcLBgwfRsmVLxMTE4M6dO/o+o0ePxrFjx7Bz505s374de/fuxYQJE8ydPhEREZFVkQghhDkrKJVK9OjRA8uXLwcA6HQ6BAYGYsqUKZg9e7bRdbRaLfr06YO//OUv+P7771FaWoqtW7cCqD4r5u/vjxkzZmDmzJkAALVaDR8fH6xbtw4jR47EiRMn0LlzZxw+fBjh4eEAgIyMDAwcOBCFhYXw9/d/4Lw1Gg3c3NygVqvh6upqzi4TERlIT6++QT86mme9iKhupmYPs86MVVRUIDc3FyqV6o8BpFKoVCrk5OTUud7ChQvh7e2N8ePH11p29uxZFBUVGYzp5uYGpVKpHzMnJwfu7u76IAYAKpUKUqkUBw8eNLrN8vJyaDQagxcR0cNiyQoiamhmhbGSkhJotVr4+PgYtPv4+KCoqMjoOvv27cOaNWuwevVqo8tr1rvfmEVFRfD29jZY7uTkBA8Pjzq3m5ycDDc3N/0rMDDwwTtIRPQALFlBRA2tUUtb3Lx5E2PGjMHq1avh5eXVmJuqZc6cOVCr1frXhQsXmnT7RGSfWLKCiBqaWaUtvLy8IJPJUFxcbNBeXFwMX1/fWv1Pnz6NgoICDB48WN+m0+mqN+zkhPz8fP16xcXF8PPzMxgzNDQUAODr61vrAYGqqipcv37d6HYBQKFQQKFQmLN7REQPxJIVRNTQzDozJpfLERYWhszMTH2bTqdDZmYmIiMja/Xv2LEjjh49iry8PP0rNjYW0dHRyMvLQ2BgIEJCQuDr62swpkajwcGDB/VjRkZGorS0FLm5ufo+u3fvhk6ng1KpNHuniYgeBktWEFFDMrvoa2JiIuLi4hAeHo6IiAikpKSgrKwM8fHxAICxY8ciICAAycnJcHZ2xhNPPGGwvru7OwAYtE+bNg1///vf0b59e4SEhOCtt96Cv7+/vh5Zp06dMGDAACQkJGDVqlWorKzE5MmTMXLkSJOepCQiIiKyVmaHsREjRuDq1auYN28eioqKEBoaioyMDP0N+OfPn4dUat6taLNmzUJZWRkmTJiA0tJS9OrVCxkZGXB2dtb32bhxIyZPnoz+/ftDKpVi2LBhWLZsmbnTJyKqE0tWEJElmF1nzFaxzhgR3U9NyYqaG/O3bWMgI6KH0yh1xoiI7BVLVhCRpTCMERGBJSuIyHLMvmeMiMgesWQFEVkKwxgR0X/FxjKEEVHT42VKIiIiIgtiGCMiu5eeDkyfzi/1JiLrxDBGRHatpmRFamr1TwYyIrI2DGNEZNdYsoKIrB3DGBHZNZasICJrx6cpiciusWQFEVk7hjEisnssWUFE1oyXKYmIiIgsiGGMiGwWS1YQkT1gGCMim8SSFURkLxjGiMgmsWQFEdkLhjEiskksWUFE9oJPUxKRTWLJCiKyFwxjRGSzWLKCiOwBL1MSERERWRDDGBFZHZasICJHwjBGRFaFJSuIyNEwjBGRVWHJCiJyNAxjRGRVWLKCiBwNn6YkIqvCkhVE5GgYxojI6rBkBRE5El6mJCIiIrIghjEiIiIiC2IYI6Imw/phRES1MYwRUZNg/TAiIuMYxoioSbB+GBGRcQxjRNQkWD+MiMg4lrYgoibB+mFERMYxjBFRk2H9MCKi2niZkoiIiMiC6hXGVqxYgeDgYDg7O0OpVOLQoUN19k1LS0N4eDjc3d3RsmVLhIaGYsOGDQZ9JBKJ0df777+v7xMcHFxr+eLFi+szfSJqYCxZQURUf2ZfptyyZQsSExOxatUqKJVKpKSkICYmBvn5+fD29q7V38PDA3PnzkXHjh0hl8uxfft2xMfHw9vbGzExMQCAy5cvG6zzf//3fxg/fjyGDRtm0L5w4UIkJCTof3dxcTF3+kTUwGpKVshkQEpK9X1hvBRJRGQ6s8PY0qVLkZCQgPj4eADAqlWrsGPHDqxduxazZ8+u1T/qnkemXnvtNaxfvx779u3ThzFfX1+DPtu2bUN0dDTatGlj0O7i4lKrb13Ky8tRXl6u/12j0Zi0HhGZx1jJCoYxIiLTmXWZsqKiArm5uVCpVH8MIJVCpVIhJyfngesLIZCZmYn8/Hz06dPHaJ/i4mLs2LED48ePr7Vs8eLF8PT0xJNPPon3338fVVVVdW4rOTkZbm5u+ldgYKAJe0hE5mLJCiKih2PWmbGSkhJotVr4+PgYtPv4+ODkyZN1rqdWqxEQEIDy8nLIZDJ89NFHeOaZZ4z2Xb9+PVxcXPD8888btE+dOhXdu3eHh4cH9u/fjzlz5uDy5ctYunSp0XHmzJmDxMRE/e8ajYaBjKgRsGQFEdHDaZLSFi4uLsjLy8OtW7eQmZmJxMREtGnTptYlTABYu3YtRo8eDWdnZ4P2u4NV165dIZfL8de//hXJyclQKBS1xlEoFEbbiajhsWQFEVH9mRXGvLy8IJPJUFxcbNBeXFx833u5pFIp2rVrBwAIDQ3FiRMnkJycXCuMff/998jPz8eWLVseOBelUomqqioUFBSgQ4cO5uwGERERkdUw654xuVyOsLAwZGZm6tt0Oh0yMzMRGRlp8jg6nc7g5voaa9asQVhYGLp16/bAMfLy8iCVSo0+wUlEDYMlK4iIGp/ZlykTExMRFxeH8PBwREREICUlBWVlZfqnK8eOHYuAgAAkJycDqL6RPjw8HG3btkV5eTm++eYbbNiwAStXrjQYV6PR4PPPP8cHH3xQa5s5OTk4ePAgoqOj4eLigpycHEyfPh0vvfQSHn300frsNxE9AEtWEBE1DbPD2IgRI3D16lXMmzcPRUVFCA0NRUZGhv6m/vPnz0Mq/eOEW1lZGSZOnIjCwkI0b94cHTt2xGeffYYRI0YYjLt582YIITBq1Kha21QoFNi8eTPmz5+P8vJyhISEYPr06Qb3kRFRw2LJCiKipiERQghLT6IpaDQauLm5Qa1Ww9XV1dLTIbJ6d58Z02p5ZoyIyFymZg9+UTgRGcWSFURETYNhjIjqxJIVRESNr15fFE5EREREDYNhjMgBsWQFEZH1YBgjcjA1N+anplb/ZCAjIrIshjEiB2OsZAUREVkOwxiRg4mO/iOIabXVT0oSEZHl8GlKIgfDkhVERNaFYYzIAbFkBRGR9eBlSiIiIiILYhgjsiMsWUFEZHsYxojsBEtWEBHZJoYxIjvBkhVERLaJYYzITrBkBRGRbeLTlER2giUriIhsE8MYkR1hyQoiItvDy5REREREFsQwRmQDWLKCiMh+MYwRWTmWrCAism8MY0RWjiUriIjsG8MYkZVjyQoiIvvGpymJrBxLVhAR2TeGMSIbwJIVRET2i5cpiYiIiCyIYYzIgliygoiIGMaILIQlK4iICGAYI7IYlqwgIiKAYYzIYliygoiIAD5NSWQxLFlBREQAwxiRRbFkBRER8TIlERERkQUxjBE1ApasICIiUzGMETUwlqwgIiJz1CuMrVixAsHBwXB2doZSqcShQ4fq7JuWlobw8HC4u7ujZcuWCA0NxYYNGwz6jBs3DhKJxOA1YMAAgz7Xr1/H6NGj4erqCnd3d4wfPx63bt2qz/SJGhVLVhARkTnMDmNbtmxBYmIikpKScOTIEXTr1g0xMTG4cuWK0f4eHh6YO3cucnJy8MsvvyA+Ph7x8fH49ttvDfoNGDAAly9f1r82bdpksHz06NE4duwYdu7cie3bt2Pv3r2YMGGCudMnanQsWUFEROaQCCGEOSsolUr06NEDy5cvBwDodDoEBgZiypQpmD17tkljdO/eHYMGDcKiRYsAVJ8ZKy0txdatW432P3HiBDp37ozDhw8jPDwcAJCRkYGBAweisLAQ/v7+D9ymRqOBm5sb1Go1XF1dTZonUX2lp7NkBRGRozM1e5h1ZqyiogK5ublQqVR/DCCVQqVSIScn54HrCyGQmZmJ/Px89OnTx2BZdnY2vL290aFDB7z66qu4du2afllOTg7c3d31QQwAVCoVpFIpDh48aHRb5eXl0Gg0Bi+iphIbCyxdyiBGREQPZladsZKSEmi1Wvj4+Bi0+/j44OTJk3Wup1arERAQgPLycshkMnz00Ud45pln9MsHDBiA559/HiEhITh9+jTeeOMN/PnPf0ZOTg5kMhmKiorg7e1tOHEnJ3h4eKCoqMjoNpOTk7FgwQJzdo+IiIioyTVJ0VcXFxfk5eXh1q1byMzMRGJiItq0aYOo/95MM3LkSH3fLl26oGvXrmjbti2ys7PRv3//em1zzpw5SExM1P+u0WgQGBj4UPtBlJ5efYN+dDTPehERUcMwK4x5eXlBJpOhuLjYoL24uBi+vr51rieVStGuXTsAQGhoKE6cOIHk5GR9GLtXmzZt4OXlhVOnTqF///7w9fWt9YBAVVUVrl+/Xud2FQoFFAqFGXtHdH81JStkMiAlpfqrjBjIiIjoYZl1z5hcLkdYWBgyMzP1bTqdDpmZmYiMjDR5HJ1Oh/Ly8jqXFxYW4tq1a/Dz8wMAREZGorS0FLm5ufo+u3fvhk6ng1KpNGcXiOqNJSuIiKgxmF3aIjExEatXr8b69etx4sQJvPrqqygrK0N8fDwAYOzYsZgzZ46+f3JyMnbu3IkzZ87gxIkT+OCDD7Bhwwa89NJLAIBbt27hb3/7Gw4cOICCggJkZmZiyJAhaNeuHWJiYgAAnTp1woABA5CQkIBDhw7hhx9+wOTJkzFy5EiTnqQkaggsWUFERI3B7HvGRowYgatXr2LevHkoKipCaGgoMjIy9Df1nz9/HlLpHxmvrKwMEydORGFhIZo3b46OHTvis88+w4gRIwAAMpkMv/zyC9avX4/S0lL4+/vj2WefxaJFiwwuM27cuBGTJ09G//79IZVKMWzYMCxbtuxh95/IZLGx1ZcmWbKCiIgaktl1xmwV64wRERFRU2qUOmNERERE1LAYxoiIiIgsiGGMHF56OjB9evVPIiKipsYwRg6tpnZYamr1TwYyIiJqagxj5NBYO4yIiCyNYYwcGmuHERGRpTXJd1MSWSvWDiMiIktjGCOHFxvLEEZERJbDy5REREREFsQwRnaLJSuIiMgWMIyRXWLJCiIishUMY2SXWLKCiIhsBcMY2SWWrCAiIlvBpynJLrFkBRER2QqGMbJbLFlBRES2gJcpiYiIiCyIYYxsDktWEBGRPWEYI5vCkhVERGRvGMbIprBkBRER2RuGMbIpLFlBRET2hk9Tkk1hyQoiIrI3DGNkc1iygoiI7AkvUxIRERFZEMMYWQ2WrCAiIkfEMEZWgSUriIjIUTGMkVVgyQoiInJUDGNkFViygoiIHBWfpiSrwJIVRETkqBjGyGqwZAURETkiXqYkIiIisiCGMWp0LFlBRERUN4YxalQsWUFERHR/DGPUqFiygoiI6P4YxqhRsWQFERHR/dUrjK1YsQLBwcFwdnaGUqnEoUOH6uyblpaG8PBwuLu7o2XLlggNDcWGDRv0yysrK/H666+jS5cuaNmyJfz9/TF27FhcunTJYJzg4GBIJBKD1+LFi+szfWpCNSUrpk6t/smnJYmIiAxJhBDCnBW2bNmCsWPHYtWqVVAqlUhJScHnn3+O/Px8eHt71+qfnZ2NGzduoGPHjpDL5di+fTtmzJiBHTt2ICYmBmq1Gi+88AISEhLQrVs33LhxA6+99hq0Wi1+/PFH/TjBwcEYP348EhIS9G0uLi5o2bKlSfPWaDRwc3ODWq2Gq6urObtMREREZDZTs4fZYUypVKJHjx5Yvnw5AECn0yEwMBBTpkzB7NmzTRqje/fuGDRoEBYtWmR0+eHDhxEREYFz586hdevWAKrD2LRp0zBt2jSTtlFeXo7y8nL97xqNBoGBgQxjRERE1CRMDWNmXaasqKhAbm4uVCrVHwNIpVCpVMjJyXng+kIIZGZmIj8/H3369Kmzn1qthkQigbu7u0H74sWL4enpiSeffBLvv/8+qqqq6hwjOTkZbm5u+ldgYOCDd5DMwpIVRERED8+sCvwlJSXQarXw8fExaPfx8cHJkyfrXE+tViMgIADl5eWQyWT46KOP8Mwzzxjte+fOHbz++usYNWqUQYqcOnUqunfvDg8PD+zfvx9z5szB5cuXsXTpUqPjzJkzB4mJifrfa86MUcOoKVkhkwEpKbwfjIiIqL6a5OuQXFxckJeXh1u3biEzMxOJiYlo06YNou55tK6yshLDhw+HEAIrV640WHZ3sOratSvkcjn++te/Ijk5GQqFotY2FQqF0XZqGMZKVjCMERERmc+sy5ReXl6QyWQoLi42aC8uLoavr2/dG5FK0a5dO4SGhmLGjBl44YUXkJycbNCnJoidO3cOO3fufOB9XUqlElVVVSgoKDBnF6iBsGQFERFRwzArjMnlcoSFhSEzM1PfptPpkJmZicjISJPH0el0BjfX1wSx3377Dbt27YKnp+cDx8jLy4NUKjX6BCc1PpasICIiahhmX6ZMTExEXFwcwsPDERERgZSUFJSVlSE+Ph4AMHbsWAQEBOjPfCUnJyM8PBxt27ZFeXk5vvnmG2zYsEF/GbKyshIvvPACjhw5gu3bt0Or1aKoqAgA4OHhAblcjpycHBw8eBDR0dFwcXFBTk4Opk+fjpdeegmPPvpoQ70XZKbYWIYwIiKih2V2GBsxYgSuXr2KefPmoaioCKGhocjIyNDf1H/+/HlIpX+ccCsrK8PEiRNRWFiI5s2bo2PHjvjss88wYsQIAMDFixeR/t/H8UJDQw22lZWVhaioKCgUCmzevBnz589HeXk5QkJCMH36dIP7yIiIiIhskdl1xmwVi76aLj29+gb96Gie+SIiIqqvRqkzRvavpmRFamr1T9YQIyIialwMY2TAWMkKIiIiajwMY2SAJSuIiIiaVpMUfSXbUVOyIju7OojxnjEiIqLGxTBGtbBkBRERUdPhZUoiIiIiC2IYcyDp6cD06XxCkoiIyJowjDkIlqwgIiKyTgxjDoIlK4iIiKwTw5iDYMkKIiIi68SnKR0ES1YQERFZJ4YxB8KSFURERNaHlymJiIiILIhhzA6wZAUREZHtYhizcSxZQUREZNsYxmwcS1YQERHZNoYxG8eSFURERLaNT1PaOJasICIism0MY3aAJSuIiIhsFy9TEhEREVkQwxgRERGRBTGMWTHWDyMiIrJ/DGNWivXDiIiIHAPDmJVi/TAiIiLHwDBmpVg/jIiIyDGwtIWVYv0wIiIix8AwZsVYP4yIiMj+8TIlERERkQUxjFkAS1YQERFRDYaxJsaSFURERHQ3hrEmxpIVREREdDeGsSbGkhVERER0Nz5N2cRYsoKIiIjuVq8zYytWrEBwcDCcnZ2hVCpx6NChOvumpaUhPDwc7u7uaNmyJUJDQ7FhwwaDPkIIzJs3D35+fmjevDlUKhV+++03gz7Xr1/H6NGj4erqCnd3d4wfPx63bt2qz/QtLjYWWLqUQYyIiIjqEca2bNmCxMREJCUl4ciRI+jWrRtiYmJw5coVo/09PDwwd+5c5OTk4JdffkF8fDzi4+Px7bff6vu89957WLZsGVatWoWDBw+iZcuWiImJwZ07d/R9Ro8ejWPHjmHnzp3Yvn079u7diwkTJtRjl4mIiIish0QIIcxZQalUokePHli+fDkAQKfTITAwEFOmTMHs2bNNGqN79+4YNGgQFi1aBCEE/P39MWPGDMycORMAoFar4ePjg3Xr1mHkyJE4ceIEOnfujMOHDyM8PBwAkJGRgYEDB6KwsBD+/v4P3KZGo4GbmxvUajVcXV3N2WWTpadX36AfHc2zXkRERI7O1Oxh1pmxiooK5ObmQqVS/TGAVAqVSoWcnJwHri+EQGZmJvLz89GnTx8AwNmzZ1FUVGQwppubG5RKpX7MnJwcuLu764MYAKhUKkilUhw8eNDotsrLy6HRaAxejYklK4iIiKg+zApjJSUl0Gq18PHxMWj38fFBUVFRneup1Wo88sgjkMvlGDRoEFJTU/HMM88AgH69+41ZVFQEb29vg+VOTk7w8PCoc7vJyclwc3PTvwIDA83ZVbOxZAURERHVR5OUtnBxcUFeXh4OHz6Mt99+G4mJichu5LQyZ84cqNVq/evChQuNuj2WrCAiIqL6MKu0hZeXF2QyGYqLiw3ai4uL4evrW+d6UqkU7dq1AwCEhobixIkTSE5ORlRUlH694uJi+Pn5GYwZGhoKAPD19a31gEBVVRWuX79e53YVCgUUCoU5u/dQWLKCiIiI6sOsM2NyuRxhYWHIzMzUt+l0OmRmZiIyMtLkcXQ6HcrLywEAISEh8PX1NRhTo9Hg4MGD+jEjIyNRWlqK3NxcfZ/du3dDp9NBqVSaswuNiiUriIiIyFxmF31NTExEXFwcwsPDERERgZSUFJSVlSE+Ph4AMHbsWAQEBCA5ORlA9b1b4eHhaNu2LcrLy/HNN99gw4YNWLlyJQBAIpFg2rRp+Pvf/4727dsjJCQEb731Fvz9/TF06FAAQKdOnTBgwAAkJCRg1apVqKysxOTJkzFy5EiTnqQkIiIislZmh7ERI0bg6tWrmDdvHoqKihAaGoqMjAz9Dfjnz5+HVPrHCbeysjJMnDgRhYWFaN68OTp27IjPPvsMI0aM0PeZNWsWysrKMGHCBJSWlqJXr17IyMiAs7Ozvs/GjRsxefJk9O/fH1KpFMOGDcOyZcseZt+JiIiILM7sOmO2qinqjBERERHVaJQ6Y0RERETUsBjGiIiIiCyIYYyIiIjIghjGiIiIiCyIYYyIiIjIghjGiIiIiCyIYYyIiIjIghjGiIiIiCyIYYyIiIjIgsz+OiRbVfNFAxqNxsIzISIiIkdQkzke9GVHDhPGbt68CQAIDAy08EyIiIjIkdy8eRNubm51LneY76bU6XS4dOkSXFxcIJFIGmUbGo0GgYGBuHDhAr//0grweFgfHhPrw2NiXXg8rM/DHBMhBG7evAl/f39IpXXfGeYwZ8akUilatWrVJNtydXXlh8iK8HhYHx4T68NjYl14PKxPfY/J/c6I1eAN/EREREQWxDBGREREZEEMYw1IoVAgKSkJCoXC0lMh8HhYIx4T68NjYl14PKxPUxwTh7mBn4iIiMga8cwYERERkQUxjBERERFZEMMYERERkQUxjBERERFZEMMYERERkQUxjJlpxYoVCA4OhrOzM5RKJQ4dOnTf/p9//jk6duwIZ2dndOnSBd98800TzdQxmHM8Vq9ejd69e+PRRx/Fo48+CpVK9cDjR+Yz9zNSY/PmzZBIJBg6dGjjTtABmXtMSktLMWnSJPj5+UGhUODxxx/n/3c1IHOPR0pKCjp06IDmzZsjMDAQ06dPx507d5potvZv7969GDx4MPz9/SGRSLB169YHrpOdnY3u3btDoVCgXbt2WLdu3cNNQpDJNm/eLORyuVi7dq04duyYSEhIEO7u7qK4uNho/x9++EHIZDLx3nvviePHj4s333xTNGvWTBw9erSJZ26fzD0eL774olixYoX46aefxIkTJ8S4ceOEm5ubKCwsbOKZ2y9zj0mNs2fPioCAANG7d28xZMiQppmsgzD3mJSXl4vw8HAxcOBAsW/fPnH27FmRnZ0t8vLymnjm9snc47Fx40ahUCjExo0bxdmzZ8W3334r/Pz8xPTp05t45vbrm2++EXPnzhVpaWkCgPjqq6/u2//MmTOiRYsWIjExURw/flykpqYKmUwmMjIy6j0HhjEzREREiEmTJul/12q1wt/fXyQnJxvtP3z4cDFo0CCDNqVSKf7617826jwdhbnH415VVVXCxcVFrF+/vrGm6HDqc0yqqqrE008/Lf75z3+KuLg4hrEGZu4xWblypWjTpo2oqKhoqik6FHOPx6RJk0S/fv0M2hITE0XPnj0bdZ6OypQwNmvWLPGnP/3JoG3EiBEiJiam3tvlZUoTVVRUIDc3FyqVSt8mlUqhUqmQk5NjdJ2cnByD/gAQExNTZ38yXX2Ox71+//13VFZWwsPDo7Gm6VDqe0wWLlwIb29vjB8/vimm6VDqc0zS09MRGRmJSZMmwcfHB0888QTeeecdaLXappq23arP8Xj66aeRm5urv5R55swZfPPNNxg4cGCTzJlqa4x/250edlKOoqSkBFqtFj4+PgbtPj4+OHnypNF1ioqKjPYvKipqtHk6ivocj3u9/vrr8Pf3r/WhovqpzzHZt28f1qxZg7y8vCaYoeOpzzE5c+YMdu/ejdGjR+Obb77BqVOnMHHiRFRWViIpKakppm236nM8XnzxRZSUlKBXr14QQqCqqgqvvPIK3njjjaaYMhlR17/tGo0Gt2/fRvPmzc0ek2fGyCEtXrwYmzdvxldffQVnZ2dLT8ch3bx5E2PGjMHq1avh5eVl6enQf+l0Onh7e+P//b//h7CwMIwYMQJz587FqlWrLD01h5SdnY133nkHH330EY4cOYK0tDTs2LEDixYtsvTUqAHxzJiJvLy8IJPJUFxcbNBeXFwMX19fo+v4+vqa1Z9MV5/jUWPJkiVYvHgxdu3aha5duzbmNB2Kucfk9OnTKCgowODBg/VtOp0OAODk5IT8/Hy0bdu2cSdt5+rzOfHz80OzZs0gk8n0bZ06dUJRUREqKiogl8sbdc72rD7H46233sKYMWPw8ssvAwC6dOmCsrIyTJgwAXPnzoVUynMqTa2uf9tdXV3rdVYM4Jkxk8nlcoSFhSEzM1PfptPpkJmZicjISKPrREZGGvQHgJ07d9bZn0xXn+MBAO+99x4WLVqEjIwMhIeHN8VUHYa5x6Rjx444evQo8vLy9K/Y2FhER0cjLy8PgYGBTTl9u1Sfz0nPnj1x6tQpfTAGgP/85z/w8/NjEHtI9Tkev//+e63AVROUq+83p6bWKP+21/vWfwe0efNmoVAoxLp168Tx48fFhAkThLu7uygqKhJCCDFmzBgxe/Zsff8ffvhBODk5iSVLlogTJ06IpKQklrZoQOYej8WLFwu5XC6++OILcfnyZf3r5s2bltoFu2PuMbkXn6ZseOYek/PnzwsXFxcxefJkkZ+fL7Zv3y68vb3F3//+d0vtgl0x93gkJSUJFxcXsWnTJnHmzBnx3XffibZt24rhw4dbahfszs2bN8VPP/0kfvrpJwFALF26VPz000/i3LlzQgghZs+eLcaMGaPvX1Pa4m9/+5s4ceKEWLFiBUtbNLXU1FTRunVrIZfLRUREhDhw4IB+Wd++fUVcXJxB/3//+9/i8ccfF3K5XPzpT38SO3bsaOIZ2zdzjkdQUJAAUOuVlJTU9BO3Y+Z+Ru7GMNY4zD0m+/fvF0qlUigUCtGmTRvx9ttvi6qqqiaetf0y53hUVlaK+fPni7Zt2wpnZ2cRGBgoJk6cKG7cuNH0E7dTWVlZRv9tqDkOcXFxom/fvrXWCQ0NFXK5XLRp00Z88sknDzUHiRA8z0lERERkKbxnjIiIiMiCGMaIiIiILIhhjIiIiMiCGMaIiIiILIhhjIiIiMiCGMaIiIiILIhhjIiIiMiCGMaIiIiILIhhjIiIiMiCGMaIiIiILIhhjIiIiMiC/j/EoPkuBoqzKgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ploting our train and test losses at each epochs"
      ],
      "metadata": {
        "id": "Q-_g9mQRfKCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_count , np.array(torch.tensor(trian_loss_value)) ,label=\"trian loss\")\n",
        "plt.plot(epoch_count , test_loss_value ,label=\"test loss\" )\n",
        "plt.title(\"training and test loss curves\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "eDinmuTBNReZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "2506ef38-4c3d-48c0-ef55-351c9d762526"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x79d1c41d8220>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABwQElEQVR4nO3dd3hUVeLG8e+kzKSQSjoEQoeEKiViwxIFRAQsFHFRFHd1UVexsvsTdHVFXXXVFXVlRXQtIFWlCSKgIr1IizQDCSUJAZKQQtrc3x9DBkaChGQmk/J+nmeeTO6cOefMTTQv55x7rskwDAMRERGRBsTD3R0QERERqWkKQCIiItLgKACJiIhIg6MAJCIiIg2OApCIiIg0OApAIiIi0uAoAImIiEiDowAkIiIiDY4CkIiIiDQ4CkAibhYXF8fdd99dpfdeffXVXH311U7tT20zbdo0TCYT+/fvd3dXLtqKFSswmUysWLHC3V0Rkd9QABK5gJ9++olnn32W7Oxsd3dFfsfChQt59tlnXdpGQUEBzz77rAKNSD2gACRyAT/99BPPPfecywLQrl27mDJlSpXeu2TJEpYsWeLkHtVNCxcu5LnnnnNpGwUFBTz33HMKQCL1gAKQiBNZrVZOnTp1Ue+xWCx4e3tXqT2z2YzZbK7Se0Vc5dSpU1itVnd3Q+R3KQCJ/I5nn32WJ554AoAWLVpgMpkc1qOYTCYefPBBPv30UxISErBYLCxevBiAV199lcsuu4zGjRvj6+tL9+7dmTVr1jlt/HYNUPmal1WrVjFu3DjCw8Px9/dnyJAhHD161OG9v10DVL7m5IsvvuAf//gHTZs2xcfHh+uuu469e/ee0/bkyZNp2bIlvr6+9OrVix9++KHS64o+/PBDrr32WiIiIrBYLMTHx/Puu+9W+PluuukmfvzxR3r16oWPjw8tW7bk448/Pqfsjh07uPbaa/H19aVp06a88MILlfpDevfddzN58mQA+8/IZDLZX7darbzxxhskJCTg4+NDZGQkf/rTnzhx4oRDPRs2bKBv376EhYXh6+tLixYtuOeeewDYv38/4eHhADz33HP2Nqoy7TZz5ky6d++Or68vYWFh3HnnnRw6dMihTHp6OqNHj6Zp06ZYLBaio6MZNGiQw1qo3+vvhSxatIg+ffoQEBBAYGAgPXv25LPPPrO/fr61aef7nZs+fTr/93//R5MmTfDz82PTpk2YTCY++uijc+r45ptvMJlMzJ8/337s0KFD3HPPPURGRmKxWEhISGDq1KnnvPff//43CQkJ+Pn5ERISQo8ePRz6LVJZXu7ugEhtdsstt7B7924+//xz/vWvfxEWFgZg/0MI8N133/HFF1/w4IMPEhYWRlxcHABvvvkmN998MyNHjqS4uJjp06dz++23M3/+fAYMGHDBth966CFCQkKYOHEi+/fv54033uDBBx9kxowZF3zvSy+9hIeHB48//jg5OTm88sorjBw5krVr19rLvPvuuzz44INceeWVPProo+zfv5/BgwcTEhJC06ZNL9jGu+++S0JCAjfffDNeXl58/fXX/PnPf8ZqtTJ27FiHsnv37uW2227j3nvv5a677mLq1KncfffddO/enYSEBMD2B/+aa66htLSUp59+Gn9/f95//318fX0v2Jc//elPHD58mKVLl/K///2vwtenTZvG6NGjefjhh0lJSeHtt99m8+bNrFq1Cm9vbzIzM7nhhhsIDw/n6aefJjg4mP379zNnzhzA9jN/9913eeCBBxgyZAi33HILAJ07d75g/85W3o+ePXsyadIkMjIyePPNN1m1ahWbN28mODgYgFtvvZUdO3bw0EMPERcXR2ZmJkuXLiU1NdX+/e/190J9uOeee0hISGD8+PEEBwezefNmFi9ezB133HFRn6fc888/j9ls5vHHH6eoqIj4+HhatmzJF198wV133eVQdsaMGYSEhNC3b18AMjIyuPTSS+3/oAgPD2fRokXce++95Obm8sgjjwAwZcoUHn74YW677Tb+8pe/cOrUKbZu3cratWur3G9pwAwR+V3//Oc/DcBISUk55zXA8PDwMHbs2HHOawUFBQ7fFxcXGx07djSuvfZah+PNmzc37rrrLvv3H374oQEYSUlJhtVqtR9/9NFHDU9PTyM7O9t+rE+fPkafPn3s3y9fvtwAjA4dOhhFRUX242+++aYBGNu2bTMMwzCKioqMxo0bGz179jRKSkrs5aZNm2YADnWez28/n2EYRt++fY2WLVue8/kA4/vvv7cfy8zMNCwWi/HYY4/Zjz3yyCMGYKxdu9ahXFBQ0HnP/9nGjh1rVPS/tB9++MEAjE8//dTh+OLFix2Oz5071wCM9evXn7eNo0ePGoAxceLE3+1LufKfx/Llyw3DsP0OREREGB07djQKCwvt5ebPn28AxoQJEwzDMIwTJ04YgPHPf/7zvHVXpr8Vyc7ONgICAozExESHPhiG4fD79tvfy3Ln+51r2bLlOb8T48ePN7y9vY3jx4/bjxUVFRnBwcHGPffcYz927733GtHR0UZWVpbD+4cPH24EBQXZ6x00aJCRkJBwUZ9X5Hw0BSZSTX369CE+Pv6c42ePXJw4cYKcnByuvPJKNm3aVKl6//jHPzpM41x55ZWUlZVx4MCBC7539OjRDmuDrrzySgB+/fVXwDZ1cuzYMe677z68vM4MBI8cOZKQkJBK9e/sz5eTk0NWVhZ9+vTh119/JScnx6FsfHy8vQ9gG01p166dvT9gW8R86aWX0qtXL4dyI0eOrFR/zmfmzJkEBQVx/fXXk5WVZX90796dRo0asXz5cgD7yMv8+fMpKSmpVpvns2HDBjIzM/nzn/+Mj4+P/fiAAQNo3749CxYsAGzn1mw2s2LFinOm6cpVtb9Lly7l5MmTPP300w59ABx+3y7WXXfddc5o3bBhwygpKXEYlVqyZAnZ2dkMGzYMAMMwmD17NgMHDsQwDIefUd++fcnJybH/NxMcHMzBgwdZv359lfspUk4BSKSaWrRoUeHx+fPnc+mll+Lj40NoaKh9CuW34eB8mjVr5vB9eTA53x/Ei3lveYhq3bq1QzkvLy/7FN6FrFq1iqSkJPz9/QkODiY8PJy//vWvAOd8xt/2p7xPZ3+WAwcO0KZNm3PKtWvXrlL9OZ89e/aQk5NDREQE4eHhDo+8vDwyMzMBW5C99dZbee655wgLC2PQoEF8+OGHFBUVVav9s5Wf94o+U/v27e2vWywWXn75ZRYtWkRkZCRXXXUVr7zyCunp6fbyVe3vvn37AOjYsaOzPhZQ8X8HXbp0oX379g7TtjNmzCAsLIxrr70WgKNHj5Kdnc37779/zs9n9OjRAPaf0VNPPUWjRo3o1asXbdq0YezYsaxatcqpn0MaDq0BEqmmitao/PDDD9x8881cddVVvPPOO0RHR+Pt7c2HH35Y6QWbnp6eFR43DMOl762Mffv2cd1119G+fXtef/11YmNjMZvNLFy4kH/961/nLFx2dX9+j9VqJSIigk8//bTC18vXc5lMJmbNmsWaNWv4+uuv+eabb7jnnnt47bXXWLNmDY0aNXJ5X8/2yCOPMHDgQObNm8c333zDM888w6RJk/juu+/o1q2by/t7vtGgsrKyCn+e51urNWzYMP7xj3+QlZVFQEAAX331FSNGjLCPPJb/rtx5553nrBUqV77OqkOHDuzatYv58+ezePFiZs+ezTvvvMOECRNcvgWC1D8KQCIXUJVpgdmzZ+Pj48M333yDxWKxH//www+d2bUqa968OWBbnHzNNdfYj5eWlrJ///4LLuz9+uuvKSoq4quvvnIY3SmfTqpqn/bs2XPO8V27dlXq/ef7ObVq1Ypvv/2Wyy+/vFILqi+99FIuvfRS/vGPf/DZZ58xcuRIpk+fzpgxY6o1RQRnzvuuXbvsIyDldu3aZX/97L4/9thjPPbYY+zZs4euXbvy2muv8cknn1SqvxVp1aoVANu3bz9nBPBsISEhFe59deDAAVq2bFmpzwu2APTcc88xe/ZsIiMjyc3NZfjw4fbXw8PDCQgIoKysjKSkpAvW5+/vz7Bhwxg2bBjFxcXccsst/OMf/2D8+PHnTOmJ/B5NgYlcgL+/P8BFbYTo6emJyWSirKzMfmz//v3MmzfPyb2rmh49etC4cWOmTJlCaWmp/finn35aqSm28hGAs0dwcnJyqhXwbrzxRtasWcO6devsx44ePXrekZvfOt/PaejQoZSVlfH888+f857S0lJ7+RMnTpwzItW1a1cA+7SSn59fhW1UVo8ePYiIiOC9995zmKpatGgRycnJ9qsDCwoKztlPqlWrVgQEBNjfV5n+VuSGG24gICCASZMmndPG2fW1atWKNWvWUFxcbD82f/580tLSLuIT20ZtOnXqxIwZM5gxYwbR0dFcddVV9tc9PT259dZbmT17Ntu3bz/n/Wdv/XDs2DGH18xmM/Hx8RiG4bJ1W1J/aQRI5AK6d+8OwN/+9jeGDx+Ot7c3AwcOtP/BrciAAQN4/fXX6devH3fccQeZmZlMnjyZ1q1bs3Xr1prq+nmZzWaeffZZHnroIa699lqGDh3K/v37mTZtGq1atbrgSMcNN9yA2Wxm4MCB/OlPfyIvL48pU6YQERHBkSNHqtSnJ598kv/973/069ePv/zlL/bL4Js3b16pc1b+c3r44Yfp27cvnp6eDB8+nD59+vCnP/2JSZMmsWXLFm644Qa8vb3Zs2cPM2fO5M033+S2227jo48+4p133mHIkCG0atWKkydPMmXKFAIDA7nxxhsB2zRPfHw8M2bMoG3btoSGhtKxY8dKr6fx9vbm5ZdfZvTo0fTp04cRI0bYL4OPi4vj0UcfBWD37t1cd911DB06lPj4eLy8vJg7dy4ZGRn20ZPK9LcigYGB/Otf/2LMmDH07NmTO+64g5CQEH7++WcKCgrs+/aMGTOGWbNm0a9fP4YOHcq+ffv45JNP7CNIF2PYsGFMmDABHx8f7r33Xjw8HP/t/dJLL7F8+XISExO57777iI+P5/jx42zatIlvv/2W48ePA7bfu6ioKC6//HIiIyNJTk7m7bffZsCAAQQEBFx0v6SBc9flZyJ1yfPPP280adLE8PDwcLgkGzDGjh1b4Xs++OADo02bNobFYjHat29vfPjhh8bEiRPPuVT7fJfB//by5t9eUm0Y578keebMmQ7vTUlJMQDjww8/dDj+1ltvGc2bNzcsFovRq1cvY9WqVUb37t2Nfv36XfCcfPXVV0bnzp0NHx8fIy4uznj55ZeNqVOnnnPJevPmzY0BAwac8/7f9t0wDGPr1q1Gnz59DB8fH6NJkybG888/b3zwwQeVugy+tLTUeOihh4zw8HDDZDKdc57ff/99o3v37oavr68REBBgdOrUyXjyySeNw4cPG4ZhGJs2bTJGjBhhNGvWzLBYLEZERIRx0003GRs2bHCo56effjK6d+9umM3mC14SX9HPzDAMY8aMGUa3bt0Mi8VihIaGGiNHjjQOHjxofz0rK8sYO3as0b59e8Pf398ICgoyEhMTjS+++MJeprL9PZ+vvvrKuOyyywxfX18jMDDQ6NWrl/H55587lHnttdeMJk2aGBaLxbj88suNDRs2VPp37mx79uwxAAMwfvzxxwrLZGRkGGPHjjViY2MNb29vIyoqyrjuuuuM999/317mP//5j3HVVVcZjRs3NiwWi9GqVSvjiSeeMHJycir1mUXOZjKMGliFKCJ1gtVqJTw8nFtuuaXK9ycTEakLtAZIpIE6derUOWtIPv74Y44fP16pW2GIiNRlGgESaaBWrFjBo48+yu23307jxo3ZtGkTH3zwAR06dGDjxo26yaqI1GtaBC3SQMXFxREbG8tbb73F8ePHCQ0NZdSoUbz00ksKPyJS72kESERERBocrQESERGRBkcBSERERBocrQGqgNVq5fDhwwQEBFR763sRERGpGYZhcPLkSWJiYs7ZcPO3FIAqcPjwYWJjY93dDREREamCtLQ0mjZt+rtlFIAqUL6lelpaGoGBgW7ujYiIiFRGbm4usbGxlbo1igJQBcqnvQIDAxWARERE6pjKLF/RImgRERFpcBSAREREpMFRABIREZEGR2uARESkwbFarRQXF7u7G3KRvL298fT0dEpdCkAiItKgFBcXk5KSgtVqdXdXpAqCg4OJioqq9j59CkAiItJgGIbBkSNH8PT0JDY29oKb5UntYRgGBQUFZGZmAhAdHV2t+hSARESkwSgtLaWgoICYmBj8/Pzc3R25SL6+vgBkZmYSERFRrekwRV8REWkwysrKADCbzW7uiVRVeXAtKSmpVj0KQCIi0uDoPo91l7N+dgpAIiIi0uAoAImIiNQjzz77LF27dnV5O3Fxcbzxxhsub8dV3BqAvv/+ewYOHEhMTAwmk4l58+b9bvm7774bk8l0ziMhIcFe5tlnnz3n9fbt27v4k4iIiLjO1VdfzSOPPFKpso8//jjLli1zbYfqAbcGoPz8fLp06cLkyZMrVf7NN9/kyJEj9kdaWhqhoaHcfvvtDuUSEhIcyv3444+u6H7VHN0FOYfc3QsREalnDMOgtLSURo0a0bhxY3d3p9ZzawDq378/L7zwAkOGDKlU+aCgIKKiouyPDRs2cOLECUaPHu1QzsvLy6FcWFiYK7p/8Rb/FSb3gvVT3N0TERGpI+6++25WrlzJm2++aZ/Z2L9/PytWrMBkMrFo0SK6d++OxWLhxx9/PGcKbP369Vx//fWEhYURFBREnz592LRpk0MbJpOJ//73vwwZMgQ/Pz/atGnDV199dVH9TE1NZdCgQTRq1IjAwECGDh1KRkaG/fWff/6Za665hoCAAAIDA+nevTsbNmwA4MCBAwwcOJCQkBD8/f1JSEhg4cKFVT9plVCn1wB98MEHJCUl0bx5c4fje/bsISYmhpYtWzJy5EhSU1Pd1MPfiO1l+7ptNmgHUhERtzMMg4LiUrc8DMOoVB/ffPNNevfuzX333Wef2YiNjbW//vTTT/PSSy+RnJxM586dz3n/yZMnueuuu/jxxx9Zs2YNbdq04cYbb+TkyZMO5Z577jmGDh3K1q1bufHGGxk5ciTHjx+vVB+tViuDBg3i+PHjrFy5kqVLl/Lrr78ybNgwe5mRI0fStGlT1q9fz8aNG3n66afx9vYGYOzYsRQVFfH999+zbds2Xn75ZRo1alSptquqzm6EePjwYRYtWsRnn33mcDwxMZFp06bRrl07jhw5wnPPPceVV17J9u3bCQgIqLCuoqIiioqK7N/n5ua6ptNt+4I5AHJS4eA6aHapa9oREZFKKSwpI37CN25pe+ff++JnvvCf4aCgIMxmM35+fkRFRZ3z+t///neuv/76877/2muvdfj+/fffJzg4mJUrV3LTTTfZj999992MGDECgBdffJG33nqLdevW0a9fvwv2cdmyZWzbto2UlBR7OPv4449JSEhg/fr19OzZk9TUVJ544gn7utw2bdrY35+amsqtt95Kp06dAGjZsuUF26yuOjsC9NFHHxEcHMzgwYMdjvfv35/bb7+dzp0707dvXxYuXEh2djZffPHFeeuaNGkSQUFB9sfZydqpvH2hw+lftm0zXdOGiIg0KD169Pjd1zMyMrjvvvto06YNQUFBBAYGkpeXd87syNmjR/7+/gQGBtpvO3EhycnJxMbGOvz9jI+PJzg4mOTkZADGjRvHmDFjSEpK4qWXXmLfvn32sg8//DAvvPACl19+ORMnTmTr1q2Varc66uQIkGEYTJ06lT/84Q8X3M0zODiYtm3bsnfv3vOWGT9+POPGjbN/n5ub67oQ1Ok2+Plz2DEP+r0Ent6uaUdERC7I19uTnX/v67a2ncHf3/93X7/rrrs4duwYb775Js2bN8disdC7d2+Ki4sdypVPR5UzmUxOvWHss88+yx133MGCBQtYtGgREydOZPr06QwZMoQxY8bQt29fFixYwJIlS5g0aRKvvfYaDz30kNPa/606OQK0cuVK9u7dy7333nvBsnl5eezbt+93b5pmsVgIDAx0eLhMi6vBLwwKsuDXla5rR0RELshkMuFn9nLL42J2NDabzfbbeFysVatW8fDDD3PjjTeSkJCAxWIhKyurSnWdT4cOHUhLSyMtLc1+bOfOnWRnZxMfH28/1rZtWx599FGWLFnCLbfcwocffmh/LTY2lvvvv585c+bw2GOPMWWKay8YcmsAysvLY8uWLWzZsgWAlJQUtmzZYh+WGz9+PKNGjTrnfR988AGJiYl07NjxnNcef/xxVq5cyf79+/npp58YMmQInp6e9nlNt/P0go632J5rGkxERCohLi6OtWvXsn//frKysi5qZKZNmzb873//Izk5mbVr1zJy5Ej7TUWdJSkpiU6dOjFy5Eg2bdrEunXrGDVqFH369KFHjx4UFhby4IMPsmLFCg4cOMCqVatYv349HTp0AOCRRx7hm2++ISUlhU2bNrF8+XL7a67i1gC0YcMGunXrRrdu3QDb/GC3bt2YMGECAEeOHDlnjjInJ4fZs2efd/Tn4MGDjBgxgnbt2jF06FAaN27MmjVrCA8Pd+2HuRidTu9b9Mt8KC5wb19ERKTWe/zxx/H09CQ+Pp7w8PCLurr5gw8+4MSJE1xyySX84Q9/4OGHHyYiIsKp/TOZTHz55ZeEhIRw1VVXkZSURMuWLZkxYwYAnp6eHDt2jFGjRtG2bVuGDh1K//79ee655wDbTWrHjh1Lhw4d6NevH23btuWdd95xah/P6bNR2evwGpDc3FyCgoLIyclxzXSYYcCbnSE7FW778MyIkIiIuNSpU6dISUmhRYsW+Pj4uLs7UgW/9zO8mL/fdXINUJ1nMp0ZBdo2y719ERERaYAUgNyl4222r3uWQOEJ9/ZFRESkgVEAcpfIeIhIAGsJ7Ly47cZFRESkehSA3KnT6VGg7ZoGExERqUkKQO7U8Vbb15QfIPeIe/siIiLSgCgAuVNIc4i9FDBgxxx390ZERKTBUAByt/JpMG2KKCIiUmMUgNwtYQiYPOHwZsg6//3KRERExHkUgNzNPwxaXWN7rsXQIiIiNUIBqDawb4o407ZLtIiISC1x9dVX88gjj7i7G06nAFQbtB8AXj5wbC8c+dndvRERkVrGFSHk7rvvZvDgwU6tsy5RAKoNLAHQrr/tuRZDi4iIuJwCUG1RPg22fTZYy9zbFxERqTXuvvtuVq5cyZtvvonJZMJkMrF//34Atm/fTv/+/WnUqBGRkZH84Q9/ICsry/7eWbNm0alTJ3x9fWncuDFJSUnk5+fz7LPP8tFHH/Hll1/a61yxYkWl+nPixAlGjRpFSEgIfn5+9O/fnz179thfP3DgAAMHDiQkJAR/f38SEhJYuHCh/b0jR44kPDwcX19f2rRpw4cffui0c3UxvNzSqpyrdRL4BMHJI3DgJ2hxpbt7JCJS/xkGlBS4p21vP9vNsS/gzTffZPfu3XTs2JG///3vAISHh5Odnc21117LmDFj+Ne//kVhYSFPPfUUQ4cO5bvvvuPIkSOMGDGCV155hSFDhnDy5El++OEHDMPg8ccfJzk5mdzcXHsACQ0NrVS37777bvbs2cNXX31FYGAgTz31FDfeeCM7d+7E29ubsWPHUlxczPfff4+/vz87d+6kUaNGADzzzDPs3LmTRYsWERYWxt69eyksLKziCaweBaDawssC8YNg08e2aTAFIBER1yspgBdj3NP2Xw+D2f+CxYKCgjCbzfj5+REVFWU//vbbb9OtWzdefPFF+7GpU6cSGxvL7t27ycvLo7S0lFtuuYXmzZsD0KlTJ3tZX19fioqKHOq8kPLgs2rVKi677DIAPv30U2JjY5k3bx633347qamp3Hrrrfa2WrZsaX9/amoq3bp1o0ePHgDExcVVum1n0xRYbVI+DbbzSygtcm9fRESkVvv5559Zvnw5jRo1sj/at28PwL59++jSpQvXXXcdnTp14vbbb2fKlCmcOHGiWm0mJyfj5eVFYmKi/Vjjxo1p164dycnJADz88MO88MILXH755UycOJGtW7fayz7wwANMnz6drl278uSTT/LTTz9Vqz/VoRGg2qT55dAoCvLSYe8yaH+ju3skIlK/efvZRmLc1XY15OXlMXDgQF5++eVzXouOjsbT05OlS5fy008/sWTJEv7973/zt7/9jbVr19KiRYtqtf17xowZQ9++fVmwYAFLlixh0qRJvPbaazz00EP079+fAwcOsHDhQpYuXcp1113H2LFjefXVV13Wn/PRCFBt4uF55gapuhpMRMT1TCbbNJQ7HpVY/1PObDZTVuZ4gcwll1zCjh07iIuLo3Xr1g4Pf3//0x/PxOWXX85zzz3H5s2bMZvNzJ0797x1XkiHDh0oLS1l7dq19mPHjh1j165dxMfH24/FxsZy//33M2fOHB577DGmTJlify08PJy77rqLTz75hDfeeIP333//ovrgLApAtU35vcF2LYKiPPf2RUREaoW4uDjWrl3L/v37ycrKwmq1MnbsWI4fP86IESNYv349+/bt45tvvmH06NGUlZWxdu1aXnzxRTZs2EBqaipz5szh6NGjdOjQwV7n1q1b2bVrF1lZWZSUlFywH23atGHQoEHcd999/Pjjj/z888/ceeedNGnShEGDBgHwyCOP8M0335CSksKmTZtYvny5vc0JEybw5ZdfsnfvXnbs2MH8+fPtr9U0BaDaJqYbhLaC0kLYtdDdvRERkVrg8ccfx9PTk/j4eMLDw0lNTSUmJoZVq1ZRVlbGDTfcQKdOnXjkkUcIDg7Gw8ODwMBAvv/+e2688Ubatm3L//3f//Haa6/Rv79t37n77ruPdu3a0aNHD8LDw1m1alWl+vLhhx/SvXt3brrpJnr37o1hGCxcuBBvb28AysrKGDt2LB06dKBfv360bduWd955B7CNOo0fP57OnTtz1VVX4enpyfTp011z0i7AZBi698Jv5ebmEhQURE5ODoGBgTXfgeWTYOVL0OYGGKmpMBERZzl16hQpKSm0aNECHx8fd3dHquD3foYX8/dbI0C1Ufk02N5lkJ/1+2VFRETkoikA1UZhbSC6KxhlsHOeu3sjIiJS7ygA1Vb2O8TPcm8/RERE6iEFoNqq4y2ACVJXQ3aau3sjIiJSrygA1VaBMRB3he359tnu7YuISD2j63/qLmf97BSAarPyxdCaBhMRcQpPT08AiouL3dwTqaqCAtvNa8svu68q3QqjNutwMyx4HDK2QWYyRLhnsygRkfrCy8sLPz8/jh49ire3Nx4eGgeoKwzDoKCggMzMTIKDg+1htqoUgGozv1Boc71tQ8Rts+C6Z9zdIxGROs1kMhEdHU1KSgoHDhxwd3ekCoKDgy/qDvbnowBU23W67XQAmgnX/t9F3TtGRETOZTabadOmjabB6iBvb+9qj/yUUwCq7dr2B29/yD4ABzdAbE9390hEpM7z8PDQTtANnCY/azuzH7QfYHuuO8SLiIg4hQJQXVC+KeKOOVBW6t6+iIiI1AMKQHVBq2vANxTyj8L+793dGxERkTpPAagu8PSGhCG259oTSEREpNoUgOqK8mmwnV9BSaF7+yIiIlLHKQDVFbGJEBQLxSdhzxJ390ZERKROc2sA+v777xk4cCAxMTGYTCbmzZv3u+VXrFiByWQ655Genu5QbvLkycTFxeHj40NiYiLr1q1z4aeoIR4e0PFW23NdDSYiIlItbg1A+fn5dOnShcmTJ1/U+3bt2sWRI0fsj4iICPtrM2bMYNy4cUycOJFNmzbRpUsX+vbtS2ZmprO7X/PKp8F2L4HCbLd2RUREpC5zawDq378/L7zwAkOGDLmo90VERBAVFWV/nH0vl9dff5377ruP0aNHEx8fz3vvvYefnx9Tp051dvdrXmQChLeHsiL4Zb67eyMiIlJn1ck1QF27diU6Oprrr7+eVatW2Y8XFxezceNGkpKS7Mc8PDxISkpi9erV562vqKiI3Nxch0etZDLpDvEiIiJOUKcCUHR0NO+99x6zZ89m9uzZxMbGcvXVV7Np0yYAsrKyKCsrIzIy0uF9kZGR56wTOtukSZMICgqyP2JjY136Oaql4+kAlLISTma4ty8iIiJ1VJ0KQO3ateNPf/oT3bt357LLLmPq1Klcdtll/Otf/6pWvePHjycnJ8f+SEtLc1KPXSC0BTTtCYYVdsx1d29ERETqpDoVgCrSq1cv9u7dC0BYWBienp5kZDiOjGRkZBAVFXXeOiwWC4GBgQ6PWq18MbSuBhMREamSOh+AtmzZQnR0NABms5nu3buzbNky++tWq5Vly5bRu3dvd3XR+RKGgMkDDm2A47+6uzciIiJ1jpc7G8/Ly7OP3gCkpKSwZcsWQkNDadasGePHj+fQoUN8/PHHALzxxhu0aNGChIQETp06xX//+1++++47liw5szHguHHjuOuuu+jRowe9evXijTfeID8/n9GjR9f453OZRhHQ8mrY9x1smw19nnB3j0REROoUtwagDRs2cM0119i/HzduHAB33XUX06ZN48iRI6SmptpfLy4u5rHHHuPQoUP4+fnRuXNnvv32W4c6hg0bxtGjR5kwYQLp6el07dqVxYsXn7Mwus7reNvpAPQFXPW47QoxERERqRSTYRiGuztR2+Tm5hIUFEROTk7tXQ90Kgf+2ca2J9CffoDozu7ukYiIiFtdzN/vOr8GqMHyCYK2fW3Pt2tPIBERkYuhAFSX2a8Gmw1Wq3v7IiIiUocoANVlbW4ASyDkHoS0Ne7ujYiISJ2hAFSXeftAh5ttz7UnkIiISKUpANV15fcG2zEXSovd2xcREZE6QgGormtxFfhHQOEJ+HW5u3sjIiJSJygA1XUentDxFttzTYOJiIhUigJQfVB+NdgvC6E43719ERERqQMUgOqDJt0hJA5K8mHXInf3RkREpNZTAKoPTKaz9gTSpogiIiIXogBUX5QHoL1LoeC4e/siIiJSyykA1Rfh7SCqE1hLYeeX7u6NiIhIraYAVJ9oGkxERKRSFIDqk4TTl8MfWAU5B93bFxERkVpMAag+CY6FZpcBBmyf4+7eiIiI1FoKQPVN+a0xtmsaTERE5HwUgOqb+MHg4QVHfoaju93dGxERkVpJAai+8W8Mra6zPdcokIiISIUUgOoj+9VgM8Ew3NsXERGRWkgBqD5q1x+8/eD4r3B4k7t7IyIiUusoANVHlkbQ7kbbc+0JJCIicg4FoPrKfjXYbLCWubcvIiIitYwCUH3V6jrwCYa8DNj/o7t7IyIiUqsoANWg3FMlTFuVwo7DOa5vzMsMCYNtz7fNdH17IiIidYgCUA36x/xknv16J1N/3F8zDZZfDbbzKygtqpk2RURE6gAFoBo0vFcsAPO3HuZEfrHrG2x2GQTEQFEO7Fnq+vZERETqCAWgGtQ1NpiEmECKSq3M2lgDNyv18IBOt9qeaxpMRETETgGoBplMJu68tDkAn649gNVaA5sUlk+D7V4Mp3Jd356IiEgdoABUwwZ1jSHA4sX+YwWs2pfl+gajOkNYWyg9Bb8scH17IiIidYACUA3zM3txyyVNAPhkzQHXN2gyQcfTewJpGkxERARQAHKLkaenwb5NziQ955TrGyzfFPHXFZB31PXtiYiI1HIKQG7QNjKAXi1CKbMafL4u1fUNNm4FMZeAUQY757m+PRERkVpOAchNyhdDT1+fSkmZ1fUNnn2HeBERkQZOAchN+iVEEdbITEZuEcuSM1zfYMdbABOkrYUT+13fnoiISC2mAOQmZi8PhvawbYz4yZoamAYLiIIWV9meb5/t+vZERERqMQUgN7ojsRkmE/y4N4uUrHzXN2ifBpvl+rZERERqMQUgN2oa4se17SIA+LQmLonvMBA8zZC5EzJ2uL49ERGRWsqtAej7779n4MCBxMTEYDKZmDdv3u+WnzNnDtdffz3h4eEEBgbSu3dvvvnmG4cyzz77LCaTyeHRvn17F36K6ilfDD1z40FOlZS5tjHfYGhzg+25RoFERKQBc2sAys/Pp0uXLkyePLlS5b///nuuv/56Fi5cyMaNG7nmmmsYOHAgmzdvdiiXkJDAkSNH7I8ff/zRFd13iqvahtM0xJecwhK+/vmw6xss3xNo2ywwauBWHCIiIrWQlzsb79+/P/379690+TfeeMPh+xdffJEvv/ySr7/+mm7dutmPe3l5ERUV5axuupSnh4k7EpvxyuJdfLI2ldtPL4x2mbb9wNwIclIhbR00S3RteyIiIrVQnV4DZLVaOXnyJKGhoQ7H9+zZQ0xMDC1btmTkyJGkpv7+VVZFRUXk5uY6PGrS0B6xeHua+Dktm+2HclzbmLevbS0QaE8gERFpsOp0AHr11VfJy8tj6NCh9mOJiYlMmzaNxYsX8+6775KSksKVV17JyZMnz1vPpEmTCAoKsj9iY108CvMbYY0s9O8YDdTQ/cHKp8F2zIWyEte3JyIiUsvU2QD02Wef8dxzz/HFF18QERFhP96/f39uv/12OnfuTN++fVm4cCHZ2dl88cUX561r/Pjx5OTk2B9paWk18REclC+G/nLLYXJPuTiUtLga/MKgIAt+XenatkRERGqhOhmApk+fzpgxY/jiiy9ISkr63bLBwcG0bduWvXv3nreMxWIhMDDQ4VHTesaF0DayEYUlZczZeNC1jXl6nd4ZGk2DiYhIg1TnAtDnn3/O6NGj+fzzzxkwYMAFy+fl5bFv3z6io6NroHdVZzKZ7KNAn6xNxXD1FVodT0+D/TIfigtc25aIiEgt49YAlJeXx5YtW9iyZQsAKSkpbNmyxb5oefz48YwaNcpe/rPPPmPUqFG89tprJCYmkp6eTnp6Ojk5ZxYOP/7446xcuZL9+/fz008/MWTIEDw9PRkxYkSNfraqGNKtCX5mT/Zm5rE25bhrG4vtBcHNoDgP9nxz4fIiIiL1iFsD0IYNG+jWrZv9EvZx48bRrVs3JkyYAMCRI0ccruB6//33KS0tZezYsURHR9sff/nLX+xlDh48yIgRI2jXrh1Dhw6lcePGrFmzhvDw8Jr9cFUQ4OPN4G5NgBpYDG0ynRkF0qaIIiLSwJgMl8+11D25ubkEBQWRk5NT4+uBdh7O5ca3fsDb08Sqp68lIsDHdY1l7IR3e9tuj/H4bvANcV1bIiIiLnYxf7/r3Bqg+i4+JpBLmgVTUmbwxXoXX40WGQ8RCVBWDMlfu7YtERGRWkQBqBYqXwz9+bo0yqwuHqCz3xpDV4OJiEjDoQBUC93YKZoQP28OZRey/JdM1zbW8Vbb15QfIPeIa9sSERGpJRSAaiEfb0/7PcE+WevixdAhzSE2ETBgxxzXtiUiIlJLKADVUnf0agbAyt1HSTvu4n16Ot1u+6qrwUREpIFQAKql4sL8ubJNGIYBn679/Zu5Vlv8YDB5wuFNcGyfa9sSERGpBRSAarHyxdBfbEijqLTMdQ01CodW19ieaxRIREQaAAWgWuy69hFEB/lwPL+YxdvTXduYfRpsJmhrKBERqecUgGoxL08PRpxeC+TynaHbDwAvHzi2B4787Nq2RERE3EwBqJYb3jMWLw8T6/ef4Jf0XNc1ZAmAdv1tz7UnkIiI1HMKQLVcRKAPNyREAjUwClQ+DbZ9Dlitrm1LRETEjRSA6oA7E22LoeduOkReUanrGmqdBD5BcPIwpP7kunZERETcTAGoDujdqjEtw/3JLy5j3uZDrmvIywIdbrY91zSYiIjUYwpAdYDJZGLk6VGgT9YcwHDlVVrl02A75kFpsevaERERcSMFoDritkua4uPtwS/pJ9mUesJ1DcVdAY2i4FQ27FvmunZERETcSAGojgjy82Zg5xgAPlnjwp2hPTzP3CBV02AiIlJPKQDVIeU7Qy/YeoTj+S6cnup0m+3rLwuhKM917YiIiLiJAlAd0iU2mE5NgiguszJzQ5rrGorpBqGtoLQQdi10XTsiIiJuogBUx9x5qW1n6M/WpWK1umgxtMl0ZhRI02AiIlIPKQDVMTd3aUKAjxcHjhXww94s1zXU8XQA2vcd5B9zXTsiIiJuoABUx/iaPbmte1PAxTtDh7eF6C5gLYWd81zXjoiIiBsoANVB5XsCLUvO4FB2oesast8hfpbr2hAREXEDBaA6qHVEI3q3bIzVgOnrXHhJfMItgMl2W4xsFy66FhERqWEKQHVU+SXx09enUVLmohuXBjWxbYwIsH22a9oQERFxAwWgOuqGhEjCAywcPVnEkh0ZrmvIfjWYpsFERKT+UACqo7w9PRjeMxZw8WLoDjeDhzdkbIPMX1zXjoiISA1SAKrDRvRqhocJVv96jL2ZLtqx2S8UWifZnm/XKJCIiNQPCkB1WEywL9e2jwTg07UuHAU6e1NEV96JXkREpIYoANVx5TtDz954kMLiMtc00q4/ePvDif1waKNr2hAREalBCkB13FVtwmkW6kfuqVK+/vmwaxox+0P7AbbnujWGiIjUAwpAdZyHh4mRibZRoE9cOg12elPE7XOgrNR17YiIiNQABaB64PYesZi9PNh6MIef07Jd00ira8A3FPIzYf/3rmlDRESkhigA1QOh/mYGdIoGXHhJvKc3JAyxPdeeQCIiUscpANUT5Yuhv956mJyCEtc0Uj4Nlvw1lJxyTRsiIiI1QAGonrikWQjtowI4VWJl1qaDrmkkNhECm0JRLuxZ4po2REREaoACUD1hMpns9wf7dO0BDFfs1+PhAZ1utT3X1WAiIlKHKQDVI4O7NcHf7MmvR/NZve+Yaxopnwbb/Q2cynFNGyIiIi7m1gD0/fffM3DgQGJiYjCZTMybN++C71mxYgWXXHIJFouF1q1bM23atHPKTJ48mbi4OHx8fEhMTGTdunXO73wt1MjixZBLmgAuvCQ+siOEt4eyIkie75o2REREXMytASg/P58uXbowefLkSpVPSUlhwIABXHPNNWzZsoVHHnmEMWPG8M0339jLzJgxg3HjxjFx4kQ2bdpEly5d6Nu3L5mZma76GLVK+TTYkh0ZZOa6YKGyyeR4awwREZE6yGS4ZLHIxTOZTMydO5fBgweft8xTTz3FggUL2L59u/3Y8OHDyc7OZvHixQAkJibSs2dP3n77bQCsViuxsbE89NBDPP3005XqS25uLkFBQeTk5BAYGFj1D+Umt737ExsOnGDc9W15+Lo2zm/geAq81RVMHjDuFwiIdH4bIiIiF+li/n7XqTVAq1evJikpyeFY3759Wb16NQDFxcVs3LjRoYyHhwdJSUn2MhUpKioiNzfX4VGX/aG3bRTo83WplJZZnd9AaAto2hMMK+yY6/z6RUREXKxOBaD09HQiIx1HGyIjI8nNzaWwsJCsrCzKysoqLJOenn7eeidNmkRQUJD9ERsb65L+15R+HaNo7G/mSM4pvvvFRVN/HU9Pg23XpogiIlL31KkA5Crjx48nJyfH/khLS3N3l6rF4uXJ7T1sIe5/rtoZOmGIbQrs4HrblJiIiEgdUqcCUFRUFBkZGQ7HMjIyCAwMxNfXl7CwMDw9PSssExUVdd56LRYLgYGBDo+6bmRiM0wm+GFPFvuz8p3fQEAktOhje65RIBERqWPqVADq3bs3y5Ytczi2dOlSevfuDYDZbKZ79+4OZaxWK8uWLbOXaShiQ/3o0zYcgM/WpbqmkfI9gbbOhNqxll5ERKRS3BqA8vLy2LJlC1u2bAFsl7lv2bKF1FTbH+zx48czatQoe/n777+fX3/9lSeffJJffvmFd955hy+++IJHH33UXmbcuHFMmTKFjz76iOTkZB544AHy8/MZPXp0jX622uDORNti6Jkb0jhVUub8BjrcBJ4WyNoFGdsvXF5ERKSWcGsA2rBhA926daNbt26ALbx069aNCRMmAHDkyBF7GAJo0aIFCxYsYOnSpXTp0oXXXnuN//73v/Tt29deZtiwYbz66qtMmDCBrl27smXLFhYvXnzOwuiG4Jr2ETQJ9uVEQQkLtx1xfgM+QdD29LnXnkAiIlKH1Jp9gGqTur4P0Nne/m4Pry7ZzSXNgpnz58ud38DOr+CLP9hukvrINtv9wkRERNyg3u4DJBdvaM9YvDxMbErNZudhF+xv1OYGsARC7kFIW+P8+kVERFxAAaieiwjwoW9H2xVwLrk/mLcPdBhoe75NV4OJiEjdoADUAJQvhp63+RAnT5U4v4Hye4PtmAtlLqhfRETEyRSAGoBLW4bSOqIRBcVlzNt8yPkNxF0F/hFQeBz2LXd+/SIiIk6mANQAmEwm7kxsBsAna1Jx+rp3Ty/oeIvtua4GExGROkABqIG4pXtTfL092ZVxkvX7Tzi/gfJNEX9ZAMUu2HlaRETEiaoUgD766CMWLFhg//7JJ58kODiYyy67jAMHXHTvKamWQB9vBnWNAeATV9wfrEl3CImDknzYtcj59YuIiDhRlQLQiy++iK+vLwCrV69m8uTJvPLKK4SFhTnsyiy1y52X2hZDL9p+hKy8IudWbjKdGQXS1WAiIlLLVSkApaWl0bp1awDmzZvHrbfeyh//+EcmTZrEDz/84NQOivN0bBJEl9hgSsoMvtjggjvedzx9Ndjeb6HguPPrFxERcZIqBaBGjRpx7NgxAJYsWcL1118PgI+PD4WFhc7rnThd+WLoz9amUmZ18mLoiPYQ2QmsJZD8lXPrFhERcaIqBaDrr7+eMWPGMGbMGHbv3s2NN94IwI4dO4iLi3Nm/8TJBnaJIcjXm4MnCvl+91HnN1C+J5CmwUREpBarUgCaPHkyvXv35ujRo8yePZvGjRsDsHHjRkaMGOHUDopz+Xh7clv3poCLFkN3vNX2df+PkOOCPYdEREScQDdDrUB9uhlqRX49mse1r63EZIIfnryGpiF+zm1gan9I/QlueAEue8i5dYuIiJyHy2+GunjxYn788Uf795MnT6Zr167ccccdnDjhgj1mxKlahjfiitZhGAZ8vi7V+Q3Yp8G0KaKIiNROVQpATzzxBLm5tjuLb9u2jccee4wbb7yRlJQUxo0b59QOimvcealtMfSM9WkUl1qdW3n8YPDwgiM/w9Hdzq1bRETECaoUgFJSUoiPjwdg9uzZ3HTTTbz44otMnjyZRYu0CV5dkNQhkshAC1l5xSzeke7cyv0bQ6vrbM+3azG0iIjUPlUKQGazmYKCAgC+/fZbbrjhBgBCQ0PtI0NSu3l5ejC8Z/n9wVywGPrsq8G0zExERGqZKgWgK664gnHjxvH888+zbt06BgwYAMDu3btp2rSpUzsorjOiVzM8PUysSznO7oyTzq283Y3g5QvH98Hhzc6tW0REpJqqFIDefvttvLy8mDVrFu+++y5NmjQBYNGiRfTr18+pHRTXiQryIalDBACfOnsUyNII2tv2h9KeQCIiUtvoMvgK1PfL4M/2w56j/OGDdQRYvFjz1+vwt3g5r/Jdi+Dz4dAoCsbtBA9P59UtIiLyGxfz97vKf+3KysqYN28eycnJACQkJHDzzTfj6ak/cnXJ5a3CiGvsx/5jBXz182FG9GrmvMpbXQc+wZCXbtsYsWUf59UtIiJSDVWaAtu7dy8dOnRg1KhRzJkzhzlz5nDnnXeSkJDAvn37nN1HcSEPDxMjE213if9kzQGcOiDoZYaEwbbn2hNIRERqkSoFoIcffphWrVqRlpbGpk2b2LRpE6mpqbRo0YKHH37Y2X0UF7ute1PMXh7sOJzLlrRs51be6Xbb151fQWmRc+sWERGpoioFoJUrV/LKK68QGhpqP9a4cWNeeuklVq5c6bTOSc0I8TdzU+doAD5Z4+SdoZtdBgExUJQDe791bt0iIiJVVKUAZLFYOHny3Mum8/LyMJvN1e6U1Lw/XGqbBpu/9TDZBcXOq9jDAzreYnuuaTAREaklqhSAbrrpJv74xz+ydu1aDMPAMAzWrFnD/fffz8033+zsPkoN6BobTEJMIEWlVmZtPOjcysunwXYtgiIn7zckIiJSBVUKQG+99RatWrWid+/e+Pj44OPjw2WXXUbr1q154403nNxFqQkmk4k7Lz2zGNpqdeJi6Ogu0LgNlJ6CXxY4r14REZEqqlIACg4O5ssvv2T37t3MmjWLWbNmsXv3bubOnUtwcLCTuyg1ZVDXGAIsXuw/VsCqfVnOq9hkOjMKpGkwERGpBSq9D9CF7vK+fPly+/PXX3+96j0St/Eze3HLJU34aPUBPllzgCvbhDuv8k63wYoXYd9yyDsKjZxYt4iIyEWqdADavLly93MymUxV7oy438hLm/PR6gN8m5xJes4pooJ8nFNx41YQcwkc3gQ750Gv+5xTr4iISBVUOgCdPcIj9VfbyAB6tQhlXcpxPl+XyqPXt3Ve5Z1utwWgbTMVgERExK2qtAZI6rfyxdDT16dSUmZ1XsUJQwATpK2FE06++aqIiMhFUACSc/RLiCKskZmM3CKWJWc4r+LAaGhxpe359tnOq1dEROQiKQDJOcxeHgztEQu4YGdo+9Vgs5xbr4iIyEVQAJIKjejVDJMJftybRUpWvvMq7jAQPM2QuQMydjivXhERkYugACQVig3149p2EQB8usaJ63V8Q6DNDbbnGgUSERE3qRUBaPLkycTFxeHj40NiYiLr1q07b9mrr74ak8l0zmPAgAH2Mnffffc5r/fr168mPkq9Ur4YeubGg5wqKXNexZ1us33dNgsMJ+44LSIiUkluD0AzZsxg3LhxTJw4kU2bNtGlSxf69u1LZmZmheXnzJnDkSNH7I/t27fj6enJ7bff7lCuX79+DuU+//zzmvg49cpVbcNpGuJLTmEJX/982HkVt+0H5kaQkwpp5w+7IiIiruL2APT6669z3333MXr0aOLj43nvvffw8/Nj6tSpFZYPDQ0lKirK/li6dCl+fn7nBCCLxeJQLiQkpCY+Tr3i6WHijsRmAHyy1omLob19bWuBALZrGkxERGqeWwNQcXExGzduJCkpyX7Mw8ODpKQkVq9eXak6PvjgA4YPH46/v7/D8RUrVhAREUG7du144IEHOHbsmFP73lAM7RGLt6eJn9Oy2X4ox3kVdzw9DbZ9DpSVOq9eERGRSnBrAMrKyqKsrIzIyEiH45GRkaSnp1/w/evWrWP79u2MGTPG4Xi/fv34+OOPWbZsGS+//DIrV66kf//+lJVVvI6lqKiI3Nxch4fYhDWy0L9jNGC7S7zTtOwDfmFQkAUpK5xXr4iISCW4fQqsOj744AM6depEr169HI4PHz6cm2++mU6dOjF48GDmz5/P+vXrWbFiRYX1TJo0iaCgIPsjNja2Bnpfd5Qvhv5yy2FyT5U4p1JP79M7Q6OrwUREpMa5NQCFhYXh6elJRobjbsMZGRlERUX97nvz8/OZPn0699577wXbadmyJWFhYezdu7fC18ePH09OTo79kZaWVvkP0QD0jAuhbWQjCkvKmLPxoPMqLt8UMflrKCl0Xr0iIiIX4NYAZDab6d69O8uWLbMfs1qtLFu2jN69e//ue2fOnElRURF33nnnBds5ePAgx44dIzo6usLXLRYLgYGBDg85w2Qy2UeBPlmbiuGsS9dje0FwMyjOg92LnVOniIhIJbh9CmzcuHFMmTKFjz76iOTkZB544AHy8/MZPXo0AKNGjWL8+PHnvO+DDz5g8ODBNG7c2OF4Xl4eTzzxBGvWrGH//v0sW7aMQYMG0bp1a/r27Vsjn6k+GtKtCX5mT/Zm5rE25bhzKjWZziyG1jSYiIjUIC93d2DYsGEcPXqUCRMmkJ6eTteuXVm8eLF9YXRqaioeHo45bdeuXfz4448sWbLknPo8PT3ZunUrH330EdnZ2cTExHDDDTfw/PPPY7FYauQz1UcBPt4M7taEz9am8smaA1zasvGF31QZnW6HH1+HPUugMBt8g51Tr4iIyO8wGU6bz6g/cnNzCQoKIicnR9NhZ9l5OJcb3/oBb08Tq56+logAH+dU/E5vyNwJN78Nl/zBOXWKiEiDczF/v90+BSZ1R3xMIJc0C6akzOCL9U5cKG6/NcZM59UpIiLyOxSA5KKUL4b+fF0aZVYnDR52vNX2NeV7OHnh/Z9ERESqSwFILsqNnaIJ8fPmUHYhy3+p+H5tFy0kDmITAcO2M7SIiIiLKQDJRfHx9uT2HraNIj9Z68Sdocv3BNI0mIiI1AAFILlod/Sy3SB15e6jpB0vcE6l8YPB5AmHN8Gxfc6pU0RE5DwUgOSixYX5c2WbMAwDPnXWXeIbhUOra2zPtSeQiIi4mAKQVEn5YugvNqRRVFrxTWYv2tnTYNqdQUREXEgBSKrkuvYRRAf5cDy/mMXbnXTlVvsB4OUDx/ZA+lbn1CkiIlIBBSCpEi9PD4b3tK0F+mSNkxZDWwKgbT/bcy2GFhERF1IAkiob3isWLw8T6/ef4Jf0XOdUap8Gmw1Wq3PqFBER+Q0FIKmyyEAfbkiw3bPt0zVOWgzd5nqwBMHJw5D6k3PqFBER+Q0FIKmWOxNti6HnbDpIXlFp9Sv0skD8zbbnmgYTEREXUQCSaundqjEtw/3JLy5j3uZDzqm0fBpsxzwoLXZOnSIiImdRAJJqMZlMjDw9CvTJmgMYzrh8Pe4KaBQFp7Jh37Lq1yciIvIbCkBSbbdd0hQfbw9+ST/JptQT1a/QwxM63mJ7rk0RRUTEBRSApNqC/LwZ2DkGgE+ctRi60222r7sWQlGec+oUERE5TQFInKJ8Z+gFW49wPN8J63ZiLoHQllBSALsWVb8+ERGRsygAiVN0iQ2mU5MgisuszNyQVv0KTSbdIV5ERFxGAUic5s5LbTtDf7YuFavVCYuhO56eBtu3DPKPVb8+ERGR0xSAxGlu7tKEAB8vDhwr4Ie9WdWvMLwtRHcBaynsnFf9+kRERE5TABKn8TV7clv3poAT7w9mnwbT1WAiIuI8CkDiVOV7Ai1LzuBwdmH1K0y4BTDZbouRc7D69YmIiKAAJE7WOqIRvVs2xmrA5+uccEl8UBNofrnt+fbZ1a9PREQEBSBxgfJL4qevT6OkzAl3dC/fE0hXg4mIiJMoAInT3ZAQSXiAhaMni1iyI6P6FcYPAg9vSN8Gmb9Uvz4REWnwFIDE6bw9PRjeMxZw0mJov1BonWR7vl2LoUVEpPoUgMQlRvRqhocJVv96jL2ZTriVxdnTYM644aqIiDRoCkDiEjHBvlzbPhKAT9c6YRSoXX/w9ocT++HQxurXJyIiDZoCkLhM+c7QszcepLC4rHqVmf2h/QDbcy2GFhGRalIAEpe5qk04zUL9yD1Vytc/H65+heXTYNvngLWagUpERBo0BSBxGQ8PEyMTbaNAnzhjGqzVteAbCvmZkPJ99esTEZEGSwFIXOr2HrGYvTzYejCHn9Oyq1eZpzckDLY9160xRESkGhSAxKVC/c0M6BQNOOmS+PJ7gyV/BSWnql+fiIg0SApA4nLli6G/3nqYnIKS6lUWeykENoWiXNizxAm9ExGRhkgBSFzukmYhtI8K4FSJlVmbqnlDUw8P6HSr7bmuBhMRkSpSABKXM5lM9vuDfbr2AEZ1NzIsnwbb/Q2cyqlm70REpCFSAJIaMbhbE/zNnvx6NJ/V+45Vr7LIjhDeHsqK4JcFzumgiIg0KLUiAE2ePJm4uDh8fHxITExk3bp15y07bdo0TCaTw8PHx8ehjGEYTJgwgejoaHx9fUlKSmLPnj2u/hjyOxpZvBhySRPACZfEm0zQUXeIFxGRqnN7AJoxYwbjxo1j4sSJbNq0iS5dutC3b18yMzPP+57AwECOHDlifxw44PgH9ZVXXuGtt97ivffeY+3atfj7+9O3b19OndJVQ+5UPg22ZEcGmbnV/FmUrwP6dQXknf93RUREpCJuD0Cvv/469913H6NHjyY+Pp733nsPPz8/pk6det73mEwmoqKi7I/IyEj7a4Zh8MYbb/B///d/DBo0iM6dO/Pxxx9z+PBh5s2bVwOfSM6nfVQgPZqHUGo1mL4+rXqVhbaEJj3AsMKOuc7poIiINBhuDUDFxcVs3LiRpKQk+zEPDw+SkpJYvXr1ed+Xl5dH8+bNiY2NZdCgQezYscP+WkpKCunp6Q51BgUFkZiYeN46i4qKyM3NdXiIa5SPAn2+LpXSMmv1KitfDK1pMBERuUhuDUBZWVmUlZU5jOAAREZGkp6eXuF72rVrx9SpU/nyyy/55JNPsFqtXHbZZRw8aLu8uvx9F1PnpEmTCAoKsj9iY2Or+9HkPPp3iiLU38yRnFN890s1p64ShoDJAw6uh+MpzumgiIg0CG6fArtYvXv3ZtSoUXTt2pU+ffowZ84cwsPD+c9//lPlOsePH09OTo79kZZWzekZOS+LlydDe9gC5idrU6tXWUAktOhje75dt8YQEZHKc2sACgsLw9PTk4yMDIfjGRkZREVFVaoOb29vunXrxt69ewHs77uYOi0WC4GBgQ4PcZ2Ric0wmeD73UfZn5Vfvcrs02CzoLr7C4mISIPh1gBkNpvp3r07y5Ytsx+zWq0sW7aM3r17V6qOsrIytm3bRnS07X5TLVq0ICoqyqHO3Nxc1q5dW+k6xbViQ/3o0zYcgM/WVXMUqMNN4GmBo79Axo4LlxcREaEWTIGNGzeOKVOm8NFHH5GcnMwDDzxAfn4+o0ePBmDUqFGMHz/eXv7vf/87S5Ys4ddff2XTpk3ceeedHDhwgDFjxgC2K8QeeeQRXnjhBb766iu2bdvGqFGjiImJYfDgwe74iFKBOxNti6FnbkjjVElZ1SvyCYK2N9ieazG0iIhUkpe7OzBs2DCOHj3KhAkTSE9Pp2vXrixevNi+iDk1NRUPjzM57cSJE9x3332kp6cTEhJC9+7d+emnn4iPj7eXefLJJ8nPz+ePf/wj2dnZXHHFFSxevPicDRPFfa5pH0GTYF8OZReycNsRbrmkadUr63Q7JH8N22fDdRNt9wsTERH5HSaj2jdmqn9yc3MJCgoiJydH64Fc6O3v9vDqkt1c0iyYOX++vOoVlRTCq21td4gfvRiaa6pTRKQhupi/3/qnsrjN0J6xeHmY2JSazc7D1dh7ydsXOgy0Pdc0mIiIVIICkLhNRIAPfTvarsyr9v3BOp2+N9iOuVBWUs2eiYhIfacAJG5Vvhh63uZDnDxVjeASdxX4R0Dhcdi33Em9ExGR+koBSNzq0pahtI5oREFxGfM2H6p6RZ5e0PEW23NtiigiIhegACRuZTKZuDOxGQCfrEmlWmvyO56eBkueD8UFTuidiIjUVwpA4na3dG+Kr7cnuzJOsuHAiapX1LQHBDeHknzYvch5HRQRkXpHAUjcLtDHm0FdYwD43+pqLIY2mRxvjSEiInIeCkBSK9x5qW0x9KLtR8jKK6p6ReUBaM9SKDjuhJ6JiEh9pAAktULHJkF0iQ2mpMzgiw1pVa8ooj1EdgJrCSR/5bwOiohIvaIAJLVG+WLoz9amUmatxmLo8j2BNA0mIiLnoQAktcbALjEE+Xpz8EQh3+8+WvWKOt5q+7r/R8g97JzOiYhIvaIAJLWGj7cnt3W33RT1kzXVWAwdHAvNLgMM2D7HOZ0TEZF6RQFIapWRp6fBvtuVycET1djLp9PpUSDdG0xERCqgACS1SsvwRlzeujGGAZ+vS616RfFDwMMLjmyBrD1O65+IiNQPCkBS6/zh9CXxM9anUVxqrVol/o2h1bW251oMLSIiv6EAJLVOUodIIgMtZOUV882O9KpXZN8UcSZU5xYbIiJS7ygASa3j5enB8J62tUD/q85i6HY3gpcvHN8Haeuc1DsREakPFICkVhrRqxmeHibWpRxnd8bJqlViaXTmkvgNHzivcyIiUucpAEmtFBXkQ1KHCAA+rc4oUM97bV93zIX8LCf0TERE6gMFIKm1yu8PNmfTIfKLSqtWSZNLIOYSKCuGzf9zYu9ERKQuUwCSWuvyVmHENfbjZFEpX/1cjR2de46xfd0wFaxlzumciIjUaQpAUmt5eJgYmWgbBfpkzQGMql7J1fEW8AmG7FTY+63zOigiInWWApDUard1b4rZy4Mdh3PZkpZdtUq8faHbnbbna//jtL6JiEjdpQAktVqIv5mbOkcD8MmaauwM3XMMmDxg3zJI3+6k3omISF2lACS1Xvli6PlbD5NdUFy1SkJbQPwg2/Of3nJSz0REpK5SAJJar1tsMAkxgRSVWpm18WDVK7r8L7av22bZ1gOJiEiDpQAktZ7JZLKPAn26NhWrtYqLoWO6QYs+YJTBKo0CiYg0ZApAUicM6hpDgMWLlKx8Vu2rxoaGVz1u+7rpI8ipxmiSiIjUaQpAUif4mb245ZImgO2S+CprcRXEXWnbGPH7V53UOxERqWsUgKTOGHl6Guzb5EzSc05VvaJr/mr7uvl/cGJ/9TsmIiJ1jgKQ1BltIwPo1SKUMqvB5+uqsYi5+WXQ8hqwlsKKl53XQRERqTMUgKROKV8MPX19KiVl1qpXdO0ztq8/fw6HNzuhZyIiUpcoAEmd0i8hirBGZjJyi1iWnFH1ipp2h05DAQMWPQ1Vvc2GiIjUSQpAUqeYvTwY2iMWqObO0ABJz4K3H6StgR1zqt85ERGpMxSApM4Z0asZJhP8uDeLlKz8qlcU1ASueNT2fMkzcCrXOR0UEZFaTwFI6pzYUD+ubRcBwKfVuSQe4LKHILg55B6Cbyc6oXciIlIXKABJnVS+GHrmxoOcKimrekXevjDobdvzDVPh15VO6J2IiNR2tSIATZ48mbi4OHx8fEhMTGTdunXnLTtlyhSuvPJKQkJCCAkJISkp6Zzyd999NyaTyeHRr18/V38MqUFXtQ2naYgvOYUlzN96pHqVtbgKetxje/7VQ1CUV/0OiohIreb2ADRjxgzGjRvHxIkT2bRpE126dKFv375kZmZWWH7FihWMGDGC5cuXs3r1amJjY7nhhhs4dOiQQ7l+/fpx5MgR++Pzzz+viY8jNcTTw8Qdic0A+F91p8EArv87BMVC9gFYOqH69YmISK3m9gD0+uuvc9999zF69Gji4+N577338PPzY+rUqRWW//TTT/nzn/9M165dad++Pf/973+xWq0sW7bMoZzFYiEqKsr+CAkJqYmPIzVoaI9YvD1N/JyWzfZDOdWrzBIAN5++QeqGD2DrzOp3UEREai23BqDi4mI2btxIUlKS/ZiHhwdJSUmsXr26UnUUFBRQUlJCaGiow/EVK1YQERFBu3bteOCBBzh27Nh56ygqKiI3N9fhIbVfWCML/TtGA9W8P1i5VtfCladvlvrVQ5C+vfp1iohIreTWAJSVlUVZWRmRkZEOxyMjI0lPT69UHU899RQxMTEOIapfv358/PHHLFu2jJdffpmVK1fSv39/ysoqXiw7adIkgoKC7I/Y2NiqfyipUeWLob/ccpjcUyXVr/Cav0Kr66C0EGaMhMIT1a9TRERqHbdPgVXHSy+9xPTp05k7dy4+Pj7248OHD+fmm2+mU6dODB48mPnz57N+/XpWrFhRYT3jx48nJyfH/khLS6uhTyDV1TMuhLaRjSgsKWPOxoPVr9DDE279LwQ3s90odeZoKC2ufr0iIlKruDUAhYWF4enpSUaG4y0NMjIyiIqK+t33vvrqq7z00kssWbKEzp07/27Zli1bEhYWxt69eyt83WKxEBgY6PCQusFkMtlHgT5Zm4rhjFta+IXCsE9su0T/uhy+/DNYq3HfMRERqXXcGoDMZjPdu3d3WMBcvqC5d+/e533fK6+8wvPPP8/ixYvp0aPHBds5ePAgx44dIzo62in9ltplSLcm+Jk92ZuZx9qU486pNLoLDP0feHjBtpmw8HGFIBGResTtU2Djxo1jypQpfPTRRyQnJ/PAAw+Qn5/P6NGjARg1ahTjx4+3l3/55Zd55plnmDp1KnFxcaSnp5Oenk5enm3vlry8PJ544gnWrFnD/v37WbZsGYMGDaJ169b07dvXLZ9RXCvAx5tBXZsATloMXa5NEgx6BzDZrgxbME4hSESknnB7ABo2bBivvvoqEyZMoGvXrmzZsoXFixfbF0anpqZy5MiZje7effddiouLue2224iOjrY/Xn31VQA8PT3ZunUrN998M23btuXee++le/fu/PDDD1gsFrd8RnG9Oy+17Qn0zY50jp4scl7FXYbB4NMhaOOHMPdPUOrE+kVExC1MhlMWTdQvubm5BAUFkZOTo/VAdcgt76xiU2o2T/Rtx9hrWju38q1fwLwHwFoKcVfCsP+Br/aWEhGpTS7m77fbR4BEnKV8MfRna1Mpszo513ceCnd8AeYA2P8DTLkWjvzs3DZERKTGKABJvXFjp2hC/Lw5lF3I8l8qvpVKtbS+Du5ZDIFN4fiv8N/rYf1/QYOoIiJ1jgKQ1Bs+3p7c3sO2ieUna524GPpsUR3h/h+gbT8oK4IFj8FnQ+GEi9oTERGXUACSeuWOXrbF0Ct3HyXteIFrGvELhRHT4YZ/gKcZ9iyByYnw4xtQ5oTdqEVExOUUgKReiQvz58o2YRgGfLo21XUNmUxw2YNw/ypofoXt1hnfToT3roAdc3W5vIhILacAJPVO+WLoLzakUVRa8f3fnCa8Ldw9Hwa/C76hcPQXmHk3vNsbts0Cq4vbFxGRKlEAknrnuvYRRAf5cDy/mMXbK3dT3WoxmaDrHfDwJujzNFiCbEFo9r3wTm/Y+BGcynF9P0REpNIUgKTe8fL0YHhP21ogp+4MfSG+IXDNeHh0G1zzN/AJhqxd8PXD8Gpb28jQrkVaJyQiUgtoI8QKaCPEui8j9xSXv/QdpVaDxY9cSfsoN/wcT+Xado/e/KktCJXzawwJt0CX4dCku20ESUREqu1i/n4rAFVAAah++POnG1m4LZ1bLmnC60O7uq8jhmHbNHHrDNu6oPyz9igKbQWtk6BZIsQmQlBT9/VTRKSOUwCqJgWg+mHrwWxufnsVJhMs/stVtIsKcHeXoKwUfl0BW6dD8nzb1WNnC2x6JgzFJkJkR/D0cktXRUTqGgWgalIAqj8e+GQji7anc318JFNG9XB3dxwVnbTtIZS6BtLWQvo2MH5z+by3PzTtDrGX2oJR057gE+Se/oqI1HIKQNWkAFR/7M3M44Z/rcRqwOf3XUrvVo3d3aXzK8qDQxttYShtLaSth6LfXj1mgoh4x1GikDitIxIRQQGo2hSA6pe/zd3Gp2tTaRHmz6K/XImPt6e7u1Q5ViscTbaFodS1kLYGTuw/t1yjSIjtZRslik2E6C7gZa7x7oqIuJsCUDUpANUvuadKuP71lWTkFvHA1a14ql97d3ep6k5mnDVCtBYObwHrby6r9/KBmG5nRoiiOtrWFnlo1wsRqd8UgKpJAaj+WbIjnT/+byOeHiY+HZPIpS1r8VTYxSg5BYc320aH0tbZQlHBsXPLmRtBRAcIb2/7GtHBNpXWKFLTZyJSbygAVZMCUP00bsYW5mw+RGN/M189dAVNgn3d3SXnMww4tu90IFoLBzdA1p5zR4nK+QTbgpBDOIoH/3oSEEWkQVEAqiYFoPqpsLiM2977iR2Hc+nYJJDpf+xNI0sDuMS8rASO7YXMnZD5i+02HZk74fiv5151Vs4vDCLjbWEovP3pkNReV6CJSK2mAFRNCkD118ETBdz89iqO5xdzSbNgpt3Ti0Afb3d3yz1KTkHWblsYOvqLLRxl7oTs37l9SECMLQj9NhiZ/Wuu3yIi56EAVE0KQPXb1oPZ/OGDdeQUltC5aRDTRvci1F9XTdkV558ORMlnHkd/gdxD53mDCYKb/WZ9UQcIawfePjXadRFp2BSAqkkBqP7bcTiHP3ywjuP5xTQJ9uXdOy+hc9Ngd3erdjuVc3oKLfmsr8mQl1FxeZMHhLY8E4zKR4zC2oBnAx11ExGXUgCqJgWghmFPxknu+3gD+48V4OVh4s/XtObPV7eqO/sE1Rb5x86EocxkOLrLNpVWeLzi8h7e0Lj1b6bSOtjCkofOvYhUnQJQNSkANRw5hSWMn7OVhdvSAYgIsPDgta0Z1jMWi5f+GFeZYdhGhsqnz85egF2UW/F7vHwgrK3jVFp4ewhurj2MRKRSFICqSQGo4Vmw9QgvLkzmULbt5qRRgT4M6xnLbd2bEhvq5+be1SOGATkHzwpFpwPS0V1QUlDxe7z9baNF4R0cvwY20R5GIuJAAaiaFIAapuJSKzM2pPH2d3vIyC2yH0+ICeS6DpH0btmYbs2CNUXmClYrZO//zcLrXbar1MqKKn6PJej0SFF5KDr98A9XMBJpoBSAqkkBqGErKi3jmx0ZzFifyup9x7Ce9V+It6eJtpEBdIwJol1UAC3D/WkZ1ogmIb54euiPrtOVldr2K7Jfqn86HB3fB9bSit/jG3rm8nz7pfodwC+0ZvsuIjVOAaiaFICk3PH8Yr7dmcGPe7NY/esxjp6seDTCy8NEVJAPMUG+RAf7EB3kS0SAhbAAC2GNzIQ3shDWyEKwnzcmjU5UX2kxHNsDGTvPXJWWufP0zWLP87+0RlGOU2jlwcgSUJM9FxEXUgCqJgUgqYhhGBzKLmT7oRy2HcphX2Y+v2blsf9YAcWl59lR+Te8PEyE+psJ9TcT7Od9+quZUL8z34f4mQnxNxPi502Iv5kAi5dCU2UVF5ze3DH5rBGjnZCTdv73BMWeWXB99iX7Zq39EqlrFICqSQFILkaZ1SAj9xRHcgo5lH2KI9mFHMk5xdGTRRzNKyIrr4isk0XknjrPlM0FeHmYCPbzJtjPFoqCfG1fzxwzn37ubX8e4mfWWqWzFZ08s3dR+WX6GTshL/08bzBBSNxZU2mnR43C2oKXpSZ7LiIXQQGomhSAxBWKSss4llfMsbxiThTYHtkFJRzPLya7oJjjBSW2r/lnjheWlFW5PR9vD4J9zQ7BqDxE/TY8lQerYD9vvD0b0CXnBcfPXJF2dNeZEaOCYxWXN3lC41anR4nOWoDduDV4NoD7yonUcgpA1aQAJLXFqZIyW1jKLyG70BaMyoNTdkExJwpKznpeTE5hCScKSiizVv0/6wCLF0EVhiYzwb7ehPif9fx0iArw8cKjviwCNwzIP+q4h9HRXbYRo6Kcit/jaa54D6OQOG3uKFKDFICqSQFI6jLDMDhZVErO6bB04nRAyj4dlmwBqpjs02Gp/LWcwpIqt+lhgqDTgcgennzPP+JUPirlZ/asO+ubDANyD5+16Dr5zPOS/Irf4+UL4e3OCkWnR42CYnWpvogLKABVkwKQNERlVoOcwrNHln4z4lRoO57zm1Go/OKqT9OZPT1OByZv+3TdmSm7M9NzwX6OU3m1apduqxWyD9hGiY4mn74y7RfbYuzSUxW/xxxwOhiVX412+pYgAVEKRiLVoABUTQpAIpVXVFpGTkGJbUQp/3RIOh2WThQUO4xEnR2eissqd+VcRXy9PSsMRg7h6fR0Xfmi8SBfb7xqcn2Ttcy2h9HZV6Md3QVZe8B6ntE2n+DTYaid7WtkvG3UyL9xzfVbpA5TAKomBSAR1zIMg8KSsopHmipY25RdeOb7aixvIsDH6/S6JW+CTn8N8TOfnr6zbTsQdNbapiA/bwJ9nLwNQVkJHNt7Jhhl7LB9PZECxnlCoX+44xRaRIItJPkGO69fIvWAAlA1KQCJ1E5Wq21902+n6Ry+/83aphMFxZys4hYEAJ4eJoJ9vS9qbVOwnze+3he5vqnk1FmbO5aPGu2A7NTzvycgxnHRdeTpqTSzf5U/r0hdpgBUTQpAIvVLaZnVfoXchdY22Z7bvlZnGwKzl4f9SjmHdU7+3mdtT3B6iu6sY+fs31Sc73gbkKO/2ELSycPnbzy42ZmdrstvBxLWFrx9qvx5ROqCOheAJk+ezD//+U/S09Pp0qUL//73v+nVq9d5y8+cOZNnnnmG/fv306ZNG15++WVuvPFG++uGYTBx4kSmTJlCdnY2l19+Oe+++y5t2rSpVH8UgEQEbNsQ2IKTbSuCC65tOr2IvKSs6v9b9fH2OGta7sxWBMG/WSze2OsU4adSCM7bg3/2XjyzTl+yn3+04opNHhDa8qz7o53ewyisDXh6V7m/IrVJnQpAM2bMYNSoUbz33nskJibyxhtvMHPmTHbt2kVERMQ55X/66SeuuuoqJk2axE033cRnn33Gyy+/zKZNm+jYsSMAL7/8MpMmTeKjjz6iRYsWPPPMM2zbto2dO3fi43PhfwEpAIlIVRmGQX5xmcPWA2dGmRzXN2UXnglT2YXV27/J3+xJsJ+ZZj4FxHsfph1pNLemElN8gPDCffiU5lb8Rg9v20aOER3OWoDdwRaWtIeR1DF1KgAlJibSs2dP3n77bQCsViuxsbE89NBDPP300+eUHzZsGPn5+cyfP99+7NJLL6Vr16689957GIZBTEwMjz32GI8//jgAOTk5REZGMm3aNIYPH37BPikAiUhNK9+/KTv/TGDKLt/c8qyNMMvXO5WPTOUUlnDh/4sbhJNNO4+DtDUdpK0pjbYeB2ljOkSAqbDCd5R6WMj2b0FBUBtKGrfDCO/A3OR8NqZkVurzmE7flNbg3HVQJozzHr/Y91yovvO95/f6XZV2oO7129ltXUw7AztHccf1l2Nq3PKi2r6Qi/n77da924uLi9m4cSPjx4+3H/Pw8CApKYnVq1dX+J7Vq1czbtw4h2N9+/Zl3rx5AKSkpJCenk5SUpL99aCgIBITE1m9enWFAaioqIiiojN3+c7NPc+/lEREXMRkMhHo402gjzfNqPyNWK1Wg9xTZ21yWXj2OifbqJNtpCmCnII4vi0oYVZB8el70xk0IYs2p4NRO4802pgO0cZ0CF9rEWEnf4GTv8DBrwF4AsDsko8vDc0vsD5/ND3vfcNtXXBrAMrKyqKsrIzIyEiH45GRkfzyyy8Vvic9Pb3C8unp6fbXy4+dr8xvTZo0ieeee65Kn0FExJ08PEyn1wiZiaPyV3+VllnJPVXqsP1AdkEJ6wqK+bbgFKbs/fjl7CUk/1ciC/cRVXyAQOMkbl80egEmcHsfq9KH2tDvqqjqZy0wLBRbQl3Qo8rT3fuA8ePHO4wq5ebmEhsb68YeiYi4lpenB6H+ZkL9zzekE1+j/ZGGJfVYAZc3rvxIpyu49bbPYWFheHp6kpGR4XA8IyODqKioCt8TFRX1u+XLv15MnRaLhcDAQIeHiIiIuEYzN4cfcHMAMpvNdO/enWXLltmPWa1Wli1bRu/evSt8T+/evR3KAyxdutRevkWLFkRFRTmUyc3NZe3ateetU0RERBoWt0+BjRs3jrvuuosePXrQq1cv3njjDfLz8xk9ejQAo0aNokmTJkyaNAmAv/zlL/Tp04fXXnuNAQMGMH36dDZs2MD7778P2BYSPvLII7zwwgu0adPGfhl8TEwMgwcPdtfHFBERkVrE7QFo2LBhHD16lAkTJpCenk7Xrl1ZvHixfRFzamoqHh5nBqouu+wyPvvsM/7v//6Pv/71r7Rp04Z58+bZ9wACePLJJ8nPz+ePf/wj2dnZXHHFFSxevLhSewCJiIhI/ef2fYBqI+0DJCIiUvdczN9vt64BEhEREXEHBSARERFpcBSAREREpMFRABIREZEGRwFIREREGhwFIBEREWlwFIBERESkwVEAEhERkQZHAUhEREQaHLffCqM2Kt8cOzc31809ERERkcoq/7tdmZtcKABV4OTJkwDExsa6uSciIiJysU6ePElQUNDvltG9wCpgtVo5fPgwAQEBmEwmp9adm5tLbGwsaWlpus+YC+k81wyd55qh81wzdJ5rjqvOtWEYnDx5kpiYGIcbqVdEI0AV8PDwoGnTpi5tIzAwUP+B1QCd55qh81wzdJ5rhs5zzXHFub7QyE85LYIWERGRBkcBSERERBocBaAaZrFYmDhxIhaLxd1dqdd0nmuGznPN0HmuGTrPNac2nGstghYREZEGRyNAIiIi0uAoAImIiEiDowAkIiIiDY4CkIiIiDQ4CkA1aPLkycTFxeHj40NiYiLr1q1zd5dqte+//56BAwcSExODyWRi3rx5Dq8bhsGECROIjo7G19eXpKQk9uzZ41Dm+PHjjBw5ksDAQIKDg7n33nvJy8tzKLN161auvPJKfHx8iI2N5ZVXXnH1R6s1Jk2aRM+ePQkICCAiIoLBgweza9cuhzKnTp1i7NixNG7cmEaNGnHrrbeSkZHhUCY1NZUBAwbg5+dHREQETzzxBKWlpQ5lVqxYwSWXXILFYqF169ZMmzbN1R+vVnn33Xfp3LmzfeO33r17s2jRIvvrOs+u8dJLL2EymXjkkUfsx3Suq+/ZZ5/FZDI5PNq3b29/vU6cY0NqxPTp0w2z2WxMnTrV2LFjh3HfffcZwcHBRkZGhru7VmstXLjQ+Nvf/mbMmTPHAIy5c+c6vP7SSy8ZQUFBxrx584yff/7ZuPnmm40WLVoYhYWF9jL9+vUzunTpYqxZs8b44YcfjNatWxsjRoywv56Tk2NERkYaI0eONLZv3258/vnnhq+vr/Gf//ynpj6mW/Xt29f48MMPje3btxtbtmwxbrzxRqNZs2ZGXl6evcz9999vxMbGGsuWLTM2bNhgXHrppcZll11mf720tNTo2LGjkZSUZGzevNlYuHChERYWZowfP95e5tdffzX8/PyMcePGGTt37jT+/e9/G56ensbixYtr9PO601dffWUsWLDA2L17t7Fr1y7jr3/9q+Ht7W1s377dMAydZ1dYt26dERcXZ3Tu3Nn4y1/+Yj+uc119EydONBISEowjR47YH0ePHrW/XhfOsQJQDenVq5cxduxY+/dlZWVGTEyMMWnSJDf2qu74bQCyWq1GVFSU8c9//tN+LDs727BYLMbnn39uGIZh7Ny50wCM9evX28ssWrTIMJlMxqFDhwzDMIx33nnHCAkJMYqKiuxlnnrqKaNdu3Yu/kS1U2ZmpgEYK1euNAzDdk69vb2NmTNn2sskJycbgLF69WrDMGxB1cPDw0hPT7eXeffdd43AwED7eX3yySeNhIQEh7aGDRtm9O3b19UfqVYLCQkx/vvf/+o8u8DJkyeNNm3aGEuXLjX69OljD0A6184xceJEo0uXLhW+VlfOsabAakBxcTEbN24kKSnJfszDw4OkpCRWr17txp7VXSkpKaSnpzuc06CgIBITE+3ndPXq1QQHB9OjRw97maSkJDw8PFi7dq29zFVXXYXZbLaX6du3L7t27eLEiRM19Glqj5ycHABCQ0MB2LhxIyUlJQ7nuX379jRr1szhPHfq1InIyEh7mb59+5Kbm8uOHTvsZc6uo7xMQ/39LysrY/r06eTn59O7d2+dZxcYO3YsAwYMOOd86Fw7z549e4iJiaFly5aMHDmS1NRUoO6cYwWgGpCVlUVZWZnDDxogMjKS9PR0N/Wqbis/b793TtPT04mIiHB43cvLi9DQUIcyFdVxdhsNhdVq5ZFHHuHyyy+nY8eOgO0cmM1mgoODHcr+9jxf6Byer0xubi6FhYWu+Di10rZt22jUqBEWi4X777+fuXPnEh8fr/PsZNOnT2fTpk1MmjTpnNd0rp0jMTGRadOmsXjxYt59911SUlK48sorOXnyZJ05x7obvIgAtn8xb9++nR9//NHdXam32rVrx5YtW8jJyWHWrFncddddrFy50t3dqlfS0tL4y1/+wtKlS/Hx8XF3d+qt/v3725937tyZxMREmjdvzhdffIGvr68be1Z5GgGqAWFhYXh6ep6zAj4jI4OoqCg39apuKz9vv3dOo6KiyMzMdHi9tLSU48ePO5SpqI6z22gIHnzwQebPn8/y5ctp2rSp/XhUVBTFxcVkZ2c7lP/teb7QOTxfmcDAwDrzP0tnMJvNtG7dmu7duzNp0iS6dOnCm2++qfPsRBs3biQzM5NLLrkELy8vvLy8WLlyJW+99RZeXl5ERkbqXLtAcHAwbdu2Ze/evXXm91kBqAaYzWa6d+/OsmXL7MesVivLli2jd+/ebuxZ3dWiRQuioqIczmlubi5r1661n9PevXuTnZ3Nxo0b7WW+++47rFYriYmJ9jLff/89JSUl9jJLly6lXbt2hISE1NCncR/DMHjwwQeZO3cu3333HS1atHB4vXv37nh7ezuc5127dpGamupwnrdt2+YQNpcuXUpgYCDx8fH2MmfXUV6mof/+W61WioqKdJ6d6LrrrmPbtm1s2bLF/ujRowcjR460P9e5dr68vDz27dtHdHR03fl9dspSarmg6dOnGxaLxZg2bZqxc+dO449//KMRHBzssAJeHJ08edLYvHmzsXnzZgMwXn/9dWPz5s3GgQMHDMOwXQYfHBxsfPnll8bWrVuNQYMGVXgZfLdu3Yy1a9caP/74o9GmTRuHy+Czs7ONyMhI4w9/+IOxfft2Y/r06Yafn1+DuQz+gQceMIKCgowVK1Y4XM5aUFBgL3P//fcbzZo1M7777jtjw4YNRu/evY3evXvbXy+/nPWGG24wtmzZYixevNgIDw+v8HLWJ554wkhOTjYmT57coC4ZNgzDePrpp42VK1caKSkpxtatW42nn37aMJlMxpIlSwzD0Hl2pbOvAjMMnWtneOyxx4wVK1YYKSkpxqpVq4ykpCQjLCzMyMzMNAyjbpxjBaAa9O9//9to1qyZYTabjV69ehlr1qxxd5dqteXLlxvAOY+77rrLMAzbpfDPPPOMERkZaVgsFuO6664zdu3a5VDHsWPHjBEjRhiNGjUyAgMDjdGjRxsnT550KPPzzz8bV1xxhWGxWIwmTZoYL730Uk19RLer6PwCxocffmgvU1hYaPz5z382QkJCDD8/P2PIkCHGkSNHHOrZv3+/0b9/f8PX19cICwszHnvsMaOkpMShzPLly42uXbsaZrPZaNmypUMbDcE999xjNG/e3DCbzUZ4eLhx3XXX2cOPYeg8u9JvA5DOdfUNGzbMiI6ONsxms9GkSRNj2LBhxt69e+2v14VzbDIMw3DOWJKIiIhI3aA1QCIiItLgKACJiIhIg6MAJCIiIg2OApCIiIg0OApAIiIi0uAoAImIiEiDowAkIiIiDY4CkIhIBVasWIHJZDrnfkYiUj8oAImIiEiDowAkIiIiDY4CkIjUSlarlUmTJtGiRQt8fX3p0qULs2bNAs5MTy1YsIDOnTvj4+PDpZdeyvbt2x3qmD17NgkJCVgsFuLi4njttdccXi8qKuKpp54iNjYWi8VC69at+eCDDxzKbNy4kR49euDn58dll13Grl277K/9/PPPXHPNNQQEBBAYGEj37t3ZsGGDi86IiDiTApCI1EqTJk3i448/5r333mPHjh08+uij3HnnnaxcudJe5oknnuC1115j/fr1hIeHM3DgQEpKSgBbcBk6dCjDhw9n27ZtPPvsszzzzDNMmzbN/v5Ro0bx+eef89Zbb5GcnMx//vMfGjVq5NCPv/3tb7z22mts2LABLy8v7rnnHvtrI0eOpGnTpqxfv56NGzfy9NNP4+3t7doTIyLO4bTbqoqIOMmpU6cMPz8/46effnI4fu+99xojRowwli9fbgDG9OnT7a8dO3bM8PX1NWbMmGEYhmHccccdxvXXX+/w/ieeeMKIj483DMMwdu3aZQDG0qVLK+xDeRvffvut/diCBQsMwCgsLDQMwzACAgKMadOmVf8Di0iN0wiQiNQ6e/fupaCggOuvv55GjRrZHx9//DH79u2zl+vdu7f9eWhoKO3atSM5ORmA5ORkLr/8cod6L7/8cvbs2UNZWRlbtmzB09OTPn36/G5fOnfubH8eHR0NQGZmJgDjxo1jzJgxJCUl8dJLLzn0TURqNwUgEal18vLyAFiwYAFbtmyxP3bu3GlfB1Rdvr6+lSp39pSWyWQCbOuTAJ599ll27NjBgAED+O6774iPj2fu3LlO6Z+IuJYCkIjUOvHx8VgsFlJTU2ndurXDIzY21l5uzZo19ucnTpxg9+7ddOjQAYAOHTqwatUqh3pXrVpF27Zt8fT0pFOnTlitVoc1RVXRtm1bHn30UZYsWcItt9zChx9+WK36RKRmeLm7AyIivxUQEMDjjz/Oo48+itVq5YorriAnJ4dVq1YRGBhI8+bNAfj73/9O48aNiYyM5G9/+xthYWEMHjwYgMcee4yePXvy/PPPM2zYMFavXs3bb7/NO++8A0BcXBx33XUX99xzD2+99RZdunThwIEDZGZmMnTo0Av2sbCwkCeeeILbbruNFi1acPDgQdavX8+tt97qsvMiIk7k7kVIIiIVsVqtxhtvvGG0a9fO8Pb2NsLDw42+ffsaK1eutC9Q/vrrr42EhATDbDYbvXr1Mn7++WeHOmbNmmXEx8cb3t7eRrNmzYx//vOfDq8XFhYajz76qBEdHW2YzWajdevWxtSpUw3DOLMI+sSJE/bymzdvNgAjJSXFKCoqMoYPH27ExsYaZrPZiImJMR588EH7AmkRqd1MhmEYbs5gIiIXZcWKFVxzzTWcOHGC4OBgd3dHROogrQESERGRBkcBSERERBocTYGJiIhIg6MRIBEREWlwFIBERESkwVEAEhERkQZHAUhEREQaHAUgERERaXAUgERERKTBUQASERGRBkcBSERERBocBSARERFpcP4fB17cpHfpXjYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbApwocIFIGC",
        "outputId": "f4aea16c-5c42-41da-d55e-249312bf3a49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}},\n",
              " 'param_groups': [{'lr': 0.001,\n",
              "   'momentum': 0,\n",
              "   'dampening': 0,\n",
              "   'weight_decay': 0,\n",
              "   'nesterov': False,\n",
              "   'maximize': False,\n",
              "   'foreach': None,\n",
              "   'differentiable': False,\n",
              "   'params': [0, 1]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# savin the model parameters\n",
        "\n",
        "from pathlib import Path\n",
        "# 1.create model directory\n",
        "model_path = Path(\"models\")\n",
        "model_path.mkdir(parents=True , exist_ok= True)\n",
        "\n",
        "# 2.crate model save path\n",
        "model_name = \"FirstModel.pth\"\n",
        "model_save_path = model_path / model_name\n",
        "model_save_path\n",
        "\n",
        "\n",
        "torch.save(model0.state_dict() , model_save_path)"
      ],
      "metadata": {
        "id": "v68MAA4KMDNJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LienearRegressionModel()\n",
        "model.load_state_dict(torch.load(f=model_save_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsbPDDvQkKJL",
        "outputId": "714d74d2-2acd-4dd2-e674-f96ccd5f1bf2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LienearRegressionModel()"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second model\n"
      ],
      "metadata": {
        "id": "SNw7RnFJqwGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "x = torch.arange(start , end , step).unsqueeze(dim=1)\n",
        "y = weight*x + bias"
      ],
      "metadata": {
        "id": "Yef9appSlK8O"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trainX , testX ,trainY  , testY = train_test_split(x , y , test_size=0.2)"
      ],
      "metadata": {
        "id": "H3G8sBlVrYta"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the data\n",
        "plot_predictions(trainX , trainY , testX , testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "e481tycKsOO_",
        "outputId": "40a4d64e-7dff-4e16-dc99-a8acef201771"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "building a model with linear layer"
      ],
      "metadata": {
        "id": "K1CNmaesftP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class linearRegressionModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_layer = nn.Linear(in_features=1,\n",
        "                                  out_features=1)\n",
        "\n",
        "  def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "    return self.linear_layer(x)\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model1 = linearRegressionModelV2()\n",
        "\n",
        "model1.state_dict()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Td9wmuHs7A5",
        "outputId": "c16a73f7-64e1-489c-8f79-f0aa51940086"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
              "             ('linear_layer.bias', tensor([0.8300]))])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        " pred = model1(testX)\n",
        "\n",
        "plot_predictions(trainX , trainY , testX , testY , predictions=pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "hE5AI8jav7O8",
        "outputId": "c89cada4-7f8e-4422-e819-d71dca5f5d08"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(model1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGAyXAKDwTxd",
        "outputId": "a53ab8b4-9e58-414b-ae1d-bdfa863dc60d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "qfhITx6Cyrdp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.to(device)"
      ],
      "metadata": {
        "id": "o3OjOCB-zMNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046db6f6-b851-46de-ed9e-2e4adc778d23"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "linearRegressionModelV2(\n",
              "  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(params=model1.parameters()  , lr=0.001)"
      ],
      "metadata": {
        "id": "BfEtlUrgzafw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=1000\n",
        "for epoch in range(epochs ):\n",
        "\n",
        "  model1.train()\n",
        "\n",
        "  predY = model1(trainX)\n",
        "\n",
        "\n",
        "  loss = loss_fn(predY , trainY)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model1.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_p = model1(testX)\n",
        "    test_loss = loss_fn(test_p , testX)\n",
        "\n",
        "  print(f\"loss: {loss} \\n test loss:{test_loss} \\n ------------------\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEpYB6-M0KQU",
        "outputId": "818350c0-8bd6-4381-ce9a-f7dd15a942d6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.5597601532936096 \n",
            " test loss:0.6860388517379761 \n",
            " ------------------\n",
            "loss: 0.5585476160049438 \n",
            " test loss:0.6847595572471619 \n",
            " ------------------\n",
            "loss: 0.5573351979255676 \n",
            " test loss:0.6834802627563477 \n",
            " ------------------\n",
            "loss: 0.5561226606369019 \n",
            " test loss:0.6822009086608887 \n",
            " ------------------\n",
            "loss: 0.5549101829528809 \n",
            " test loss:0.6809214353561401 \n",
            " ------------------\n",
            "loss: 0.5536977052688599 \n",
            " test loss:0.6796422004699707 \n",
            " ------------------\n",
            "loss: 0.5524851083755493 \n",
            " test loss:0.6783629059791565 \n",
            " ------------------\n",
            "loss: 0.5512726902961731 \n",
            " test loss:0.6770834922790527 \n",
            " ------------------\n",
            "loss: 0.5500602126121521 \n",
            " test loss:0.6758041381835938 \n",
            " ------------------\n",
            "loss: 0.5488476753234863 \n",
            " test loss:0.6745247840881348 \n",
            " ------------------\n",
            "loss: 0.5476351976394653 \n",
            " test loss:0.6732454895973206 \n",
            " ------------------\n",
            "loss: 0.5464226603507996 \n",
            " test loss:0.6719661355018616 \n",
            " ------------------\n",
            "loss: 0.5452101826667786 \n",
            " test loss:0.6706868410110474 \n",
            " ------------------\n",
            "loss: 0.5439976453781128 \n",
            " test loss:0.6694074273109436 \n",
            " ------------------\n",
            "loss: 0.5427851676940918 \n",
            " test loss:0.6681281328201294 \n",
            " ------------------\n",
            "loss: 0.5415726900100708 \n",
            " test loss:0.6668487787246704 \n",
            " ------------------\n",
            "loss: 0.540360152721405 \n",
            " test loss:0.6655694246292114 \n",
            " ------------------\n",
            "loss: 0.539147675037384 \n",
            " test loss:0.6642900705337524 \n",
            " ------------------\n",
            "loss: 0.537935197353363 \n",
            " test loss:0.6630107164382935 \n",
            " ------------------\n",
            "loss: 0.5367226600646973 \n",
            " test loss:0.6617314219474792 \n",
            " ------------------\n",
            "loss: 0.5355101823806763 \n",
            " test loss:0.6604520678520203 \n",
            " ------------------\n",
            "loss: 0.5342976450920105 \n",
            " test loss:0.659172773361206 \n",
            " ------------------\n",
            "loss: 0.5330852270126343 \n",
            " test loss:0.6578933596611023 \n",
            " ------------------\n",
            "loss: 0.5318726301193237 \n",
            " test loss:0.6566140651702881 \n",
            " ------------------\n",
            "loss: 0.5306602120399475 \n",
            " test loss:0.6553347110748291 \n",
            " ------------------\n",
            "loss: 0.5294476747512817 \n",
            " test loss:0.6540554165840149 \n",
            " ------------------\n",
            "loss: 0.528235137462616 \n",
            " test loss:0.6527760028839111 \n",
            " ------------------\n",
            "loss: 0.527022659778595 \n",
            " test loss:0.6514967083930969 \n",
            " ------------------\n",
            "loss: 0.525810182094574 \n",
            " test loss:0.6502173542976379 \n",
            " ------------------\n",
            "loss: 0.524597704410553 \n",
            " test loss:0.648938000202179 \n",
            " ------------------\n",
            "loss: 0.5233851671218872 \n",
            " test loss:0.64765864610672 \n",
            " ------------------\n",
            "loss: 0.5221726298332214 \n",
            " test loss:0.646379292011261 \n",
            " ------------------\n",
            "loss: 0.5209602117538452 \n",
            " test loss:0.645099937915802 \n",
            " ------------------\n",
            "loss: 0.5197476148605347 \n",
            " test loss:0.6438206434249878 \n",
            " ------------------\n",
            "loss: 0.5185351371765137 \n",
            " test loss:0.6425412893295288 \n",
            " ------------------\n",
            "loss: 0.5173226594924927 \n",
            " test loss:0.6412619352340698 \n",
            " ------------------\n",
            "loss: 0.5161101818084717 \n",
            " test loss:0.6399825811386108 \n",
            " ------------------\n",
            "loss: 0.5148977041244507 \n",
            " test loss:0.6387032270431519 \n",
            " ------------------\n",
            "loss: 0.5136851668357849 \n",
            " test loss:0.6374238729476929 \n",
            " ------------------\n",
            "loss: 0.5124726891517639 \n",
            " test loss:0.6361445188522339 \n",
            " ------------------\n",
            "loss: 0.5112601518630981 \n",
            " test loss:0.6348652243614197 \n",
            " ------------------\n",
            "loss: 0.5100476145744324 \n",
            " test loss:0.6335858106613159 \n",
            " ------------------\n",
            "loss: 0.5088351368904114 \n",
            " test loss:0.6323065161705017 \n",
            " ------------------\n",
            "loss: 0.5076226592063904 \n",
            " test loss:0.6310271620750427 \n",
            " ------------------\n",
            "loss: 0.5064101815223694 \n",
            " test loss:0.6297478675842285 \n",
            " ------------------\n",
            "loss: 0.5051976442337036 \n",
            " test loss:0.6284685730934143 \n",
            " ------------------\n",
            "loss: 0.5039852261543274 \n",
            " test loss:0.6271891593933105 \n",
            " ------------------\n",
            "loss: 0.5027726888656616 \n",
            " test loss:0.6259098649024963 \n",
            " ------------------\n",
            "loss: 0.5015602111816406 \n",
            " test loss:0.6246304512023926 \n",
            " ------------------\n",
            "loss: 0.5003476738929749 \n",
            " test loss:0.6233511567115784 \n",
            " ------------------\n",
            "loss: 0.49913516640663147 \n",
            " test loss:0.6220718026161194 \n",
            " ------------------\n",
            "loss: 0.4979226589202881 \n",
            " test loss:0.6207924485206604 \n",
            " ------------------\n",
            "loss: 0.4967101514339447 \n",
            " test loss:0.6195131540298462 \n",
            " ------------------\n",
            "loss: 0.4954977035522461 \n",
            " test loss:0.6182337999343872 \n",
            " ------------------\n",
            "loss: 0.4942851662635803 \n",
            " test loss:0.6169544458389282 \n",
            " ------------------\n",
            "loss: 0.4930726885795593 \n",
            " test loss:0.6156750917434692 \n",
            " ------------------\n",
            "loss: 0.49186015129089355 \n",
            " test loss:0.6143957376480103 \n",
            " ------------------\n",
            "loss: 0.4906476140022278 \n",
            " test loss:0.6131163835525513 \n",
            " ------------------\n",
            "loss: 0.48943519592285156 \n",
            " test loss:0.6118370890617371 \n",
            " ------------------\n",
            "loss: 0.4882226884365082 \n",
            " test loss:0.6105576753616333 \n",
            " ------------------\n",
            "loss: 0.4870101809501648 \n",
            " test loss:0.6092783808708191 \n",
            " ------------------\n",
            "loss: 0.4857977032661438 \n",
            " test loss:0.6079990267753601 \n",
            " ------------------\n",
            "loss: 0.4845851957798004 \n",
            " test loss:0.6067197322845459 \n",
            " ------------------\n",
            "loss: 0.48337268829345703 \n",
            " test loss:0.6054403781890869 \n",
            " ------------------\n",
            "loss: 0.48216015100479126 \n",
            " test loss:0.6041610240936279 \n",
            " ------------------\n",
            "loss: 0.48094767332077026 \n",
            " test loss:0.6028816103935242 \n",
            " ------------------\n",
            "loss: 0.47973519563674927 \n",
            " test loss:0.6016022562980652 \n",
            " ------------------\n",
            "loss: 0.4785226881504059 \n",
            " test loss:0.6003230214118958 \n",
            " ------------------\n",
            "loss: 0.4773101806640625 \n",
            " test loss:0.599043607711792 \n",
            " ------------------\n",
            "loss: 0.4760977327823639 \n",
            " test loss:0.597764253616333 \n",
            " ------------------\n",
            "loss: 0.47488516569137573 \n",
            " test loss:0.596484899520874 \n",
            " ------------------\n",
            "loss: 0.47367268800735474 \n",
            " test loss:0.5952056050300598 \n",
            " ------------------\n",
            "loss: 0.47246018052101135 \n",
            " test loss:0.5939262509346008 \n",
            " ------------------\n",
            "loss: 0.47124767303466797 \n",
            " test loss:0.5926469564437866 \n",
            " ------------------\n",
            "loss: 0.4700351655483246 \n",
            " test loss:0.5913675427436829 \n",
            " ------------------\n",
            "loss: 0.4688226580619812 \n",
            " test loss:0.5900882482528687 \n",
            " ------------------\n",
            "loss: 0.4676101803779602 \n",
            " test loss:0.5888088941574097 \n",
            " ------------------\n",
            "loss: 0.4663976728916168 \n",
            " test loss:0.5875295400619507 \n",
            " ------------------\n",
            "loss: 0.46518516540527344 \n",
            " test loss:0.5862501859664917 \n",
            " ------------------\n",
            "loss: 0.46397265791893005 \n",
            " test loss:0.5849708318710327 \n",
            " ------------------\n",
            "loss: 0.46276015043258667 \n",
            " test loss:0.583691418170929 \n",
            " ------------------\n",
            "loss: 0.4615476727485657 \n",
            " test loss:0.5824121236801147 \n",
            " ------------------\n",
            "loss: 0.4603351950645447 \n",
            " test loss:0.5811328291893005 \n",
            " ------------------\n",
            "loss: 0.4591226577758789 \n",
            " test loss:0.5798534750938416 \n",
            " ------------------\n",
            "loss: 0.4579101502895355 \n",
            " test loss:0.5785741209983826 \n",
            " ------------------\n",
            "loss: 0.45669761300086975 \n",
            " test loss:0.5772948265075684 \n",
            " ------------------\n",
            "loss: 0.45548516511917114 \n",
            " test loss:0.5760154724121094 \n",
            " ------------------\n",
            "loss: 0.45427265763282776 \n",
            " test loss:0.5747361183166504 \n",
            " ------------------\n",
            "loss: 0.4530601501464844 \n",
            " test loss:0.5734568238258362 \n",
            " ------------------\n",
            "loss: 0.45184770226478577 \n",
            " test loss:0.5721774697303772 \n",
            " ------------------\n",
            "loss: 0.4506351947784424 \n",
            " test loss:0.5708981156349182 \n",
            " ------------------\n",
            "loss: 0.4494226574897766 \n",
            " test loss:0.569618821144104 \n",
            " ------------------\n",
            "loss: 0.4482101798057556 \n",
            " test loss:0.568339467048645 \n",
            " ------------------\n",
            "loss: 0.4469977021217346 \n",
            " test loss:0.567060112953186 \n",
            " ------------------\n",
            "loss: 0.44578519463539124 \n",
            " test loss:0.5657806992530823 \n",
            " ------------------\n",
            "loss: 0.44457268714904785 \n",
            " test loss:0.5645014047622681 \n",
            " ------------------\n",
            "loss: 0.44336017966270447 \n",
            " test loss:0.5632220506668091 \n",
            " ------------------\n",
            "loss: 0.4421476721763611 \n",
            " test loss:0.5619426965713501 \n",
            " ------------------\n",
            "loss: 0.4409351944923401 \n",
            " test loss:0.5606633424758911 \n",
            " ------------------\n",
            "loss: 0.4397227168083191 \n",
            " test loss:0.5593839883804321 \n",
            " ------------------\n",
            "loss: 0.4385101795196533 \n",
            " test loss:0.5581046342849731 \n",
            " ------------------\n",
            "loss: 0.43729767203330994 \n",
            " test loss:0.5568252801895142 \n",
            " ------------------\n",
            "loss: 0.43608516454696655 \n",
            " test loss:0.5555459856987 \n",
            " ------------------\n",
            "loss: 0.4348726272583008 \n",
            " test loss:0.554266631603241 \n",
            " ------------------\n",
            "loss: 0.43366020917892456 \n",
            " test loss:0.552987277507782 \n",
            " ------------------\n",
            "loss: 0.4324476718902588 \n",
            " test loss:0.5517079830169678 \n",
            " ------------------\n",
            "loss: 0.4312352240085602 \n",
            " test loss:0.5504286289215088 \n",
            " ------------------\n",
            "loss: 0.430022656917572 \n",
            " test loss:0.5491492748260498 \n",
            " ------------------\n",
            "loss: 0.428810179233551 \n",
            " test loss:0.5478699207305908 \n",
            " ------------------\n",
            "loss: 0.42759767174720764 \n",
            " test loss:0.5465905070304871 \n",
            " ------------------\n",
            "loss: 0.42638522386550903 \n",
            " test loss:0.5453112125396729 \n",
            " ------------------\n",
            "loss: 0.4251726567745209 \n",
            " test loss:0.5440318584442139 \n",
            " ------------------\n",
            "loss: 0.4239601492881775 \n",
            " test loss:0.5427525639533997 \n",
            " ------------------\n",
            "loss: 0.4227477014064789 \n",
            " test loss:0.5414732098579407 \n",
            " ------------------\n",
            "loss: 0.4215351641178131 \n",
            " test loss:0.5401939153671265 \n",
            " ------------------\n",
            "loss: 0.4203227162361145 \n",
            " test loss:0.5389145016670227 \n",
            " ------------------\n",
            "loss: 0.41911014914512634 \n",
            " test loss:0.5376352071762085 \n",
            " ------------------\n",
            "loss: 0.41789764165878296 \n",
            " test loss:0.5363558530807495 \n",
            " ------------------\n",
            "loss: 0.41668519377708435 \n",
            " test loss:0.5350764989852905 \n",
            " ------------------\n",
            "loss: 0.4154726564884186 \n",
            " test loss:0.5337972044944763 \n",
            " ------------------\n",
            "loss: 0.41426020860671997 \n",
            " test loss:0.5325177907943726 \n",
            " ------------------\n",
            "loss: 0.4130476415157318 \n",
            " test loss:0.5312384963035583 \n",
            " ------------------\n",
            "loss: 0.4118351340293884 \n",
            " test loss:0.5299591422080994 \n",
            " ------------------\n",
            "loss: 0.4106226861476898 \n",
            " test loss:0.5286797881126404 \n",
            " ------------------\n",
            "loss: 0.40941017866134644 \n",
            " test loss:0.5274004936218262 \n",
            " ------------------\n",
            "loss: 0.40819770097732544 \n",
            " test loss:0.5261210799217224 \n",
            " ------------------\n",
            "loss: 0.40698519349098206 \n",
            " test loss:0.5248417258262634 \n",
            " ------------------\n",
            "loss: 0.40577268600463867 \n",
            " test loss:0.5235623717308044 \n",
            " ------------------\n",
            "loss: 0.4045601487159729 \n",
            " test loss:0.5222830772399902 \n",
            " ------------------\n",
            "loss: 0.4033476710319519 \n",
            " test loss:0.521003782749176 \n",
            " ------------------\n",
            "loss: 0.4021351933479309 \n",
            " test loss:0.5197243690490723 \n",
            " ------------------\n",
            "loss: 0.4009226858615875 \n",
            " test loss:0.5184450745582581 \n",
            " ------------------\n",
            "loss: 0.39971017837524414 \n",
            " test loss:0.5171657204627991 \n",
            " ------------------\n",
            "loss: 0.39849767088890076 \n",
            " test loss:0.5158863663673401 \n",
            " ------------------\n",
            "loss: 0.3972851634025574 \n",
            " test loss:0.5146070718765259 \n",
            " ------------------\n",
            "loss: 0.396072655916214 \n",
            " test loss:0.5133277177810669 \n",
            " ------------------\n",
            "loss: 0.394860178232193 \n",
            " test loss:0.5120483636856079 \n",
            " ------------------\n",
            "loss: 0.3936476409435272 \n",
            " test loss:0.5107689499855042 \n",
            " ------------------\n",
            "loss: 0.3924351632595062 \n",
            " test loss:0.5094896554946899 \n",
            " ------------------\n",
            "loss: 0.39122268557548523 \n",
            " test loss:0.508210301399231 \n",
            " ------------------\n",
            "loss: 0.39001017808914185 \n",
            " test loss:0.506930947303772 \n",
            " ------------------\n",
            "loss: 0.3887976408004761 \n",
            " test loss:0.505651593208313 \n",
            " ------------------\n",
            "loss: 0.38758519291877747 \n",
            " test loss:0.504372239112854 \n",
            " ------------------\n",
            "loss: 0.3863726854324341 \n",
            " test loss:0.5030929446220398 \n",
            " ------------------\n",
            "loss: 0.3851601183414459 \n",
            " test loss:0.5018135905265808 \n",
            " ------------------\n",
            "loss: 0.3839476704597473 \n",
            " test loss:0.5005342364311218 \n",
            " ------------------\n",
            "loss: 0.38273516297340393 \n",
            " test loss:0.49925488233566284 \n",
            " ------------------\n",
            "loss: 0.3815227150917053 \n",
            " test loss:0.49797558784484863 \n",
            " ------------------\n",
            "loss: 0.38031017780303955 \n",
            " test loss:0.49669623374938965 \n",
            " ------------------\n",
            "loss: 0.37909767031669617 \n",
            " test loss:0.49541687965393066 \n",
            " ------------------\n",
            "loss: 0.37788519263267517 \n",
            " test loss:0.4941375255584717 \n",
            " ------------------\n",
            "loss: 0.3766726553440094 \n",
            " test loss:0.4928581714630127 \n",
            " ------------------\n",
            "loss: 0.3754601776599884 \n",
            " test loss:0.4915788769721985 \n",
            " ------------------\n",
            "loss: 0.3742476999759674 \n",
            " test loss:0.4902995228767395 \n",
            " ------------------\n",
            "loss: 0.373035192489624 \n",
            " test loss:0.4890201687812805 \n",
            " ------------------\n",
            "loss: 0.37182265520095825 \n",
            " test loss:0.4877408444881439 \n",
            " ------------------\n",
            "loss: 0.37061017751693726 \n",
            " test loss:0.48646149039268494 \n",
            " ------------------\n",
            "loss: 0.36939769983291626 \n",
            " test loss:0.48518213629722595 \n",
            " ------------------\n",
            "loss: 0.3681851923465729 \n",
            " test loss:0.48390284180641174 \n",
            " ------------------\n",
            "loss: 0.3669726848602295 \n",
            " test loss:0.482623428106308 \n",
            " ------------------\n",
            "loss: 0.3657602071762085 \n",
            " test loss:0.48134416341781616 \n",
            " ------------------\n",
            "loss: 0.3645476698875427 \n",
            " test loss:0.4800647795200348 \n",
            " ------------------\n",
            "loss: 0.36333519220352173 \n",
            " test loss:0.4787854552268982 \n",
            " ------------------\n",
            "loss: 0.36212268471717834 \n",
            " test loss:0.4775061011314392 \n",
            " ------------------\n",
            "loss: 0.36091017723083496 \n",
            " test loss:0.476226806640625 \n",
            " ------------------\n",
            "loss: 0.3596976399421692 \n",
            " test loss:0.47494736313819885 \n",
            " ------------------\n",
            "loss: 0.3584851622581482 \n",
            " test loss:0.47366800904273987 \n",
            " ------------------\n",
            "loss: 0.3572726845741272 \n",
            " test loss:0.47238874435424805 \n",
            " ------------------\n",
            "loss: 0.3560601770877838 \n",
            " test loss:0.4711093306541443 \n",
            " ------------------\n",
            "loss: 0.35484766960144043 \n",
            " test loss:0.4698300361633301 \n",
            " ------------------\n",
            "loss: 0.35363516211509705 \n",
            " test loss:0.46855074167251587 \n",
            " ------------------\n",
            "loss: 0.35242268443107605 \n",
            " test loss:0.4672714173793793 \n",
            " ------------------\n",
            "loss: 0.35121017694473267 \n",
            " test loss:0.4659919738769531 \n",
            " ------------------\n",
            "loss: 0.3499976694583893 \n",
            " test loss:0.4647126793861389 \n",
            " ------------------\n",
            "loss: 0.3487851917743683 \n",
            " test loss:0.46343332529067993 \n",
            " ------------------\n",
            "loss: 0.3475726544857025 \n",
            " test loss:0.46215400099754333 \n",
            " ------------------\n",
            "loss: 0.3463602066040039 \n",
            " test loss:0.46087464690208435 \n",
            " ------------------\n",
            "loss: 0.34514766931533813 \n",
            " test loss:0.45959529280662537 \n",
            " ------------------\n",
            "loss: 0.34393519163131714 \n",
            " test loss:0.4583159387111664 \n",
            " ------------------\n",
            "loss: 0.34272268414497375 \n",
            " test loss:0.4570366442203522 \n",
            " ------------------\n",
            "loss: 0.34151017665863037 \n",
            " test loss:0.4557572305202484 \n",
            " ------------------\n",
            "loss: 0.3402976393699646 \n",
            " test loss:0.4544779658317566 \n",
            " ------------------\n",
            "loss: 0.3390851616859436 \n",
            " test loss:0.4531986117362976 \n",
            " ------------------\n",
            "loss: 0.3378726840019226 \n",
            " test loss:0.45191922783851624 \n",
            " ------------------\n",
            "loss: 0.3366601765155792 \n",
            " test loss:0.45063990354537964 \n",
            " ------------------\n",
            "loss: 0.3354476988315582 \n",
            " test loss:0.44936054944992065 \n",
            " ------------------\n",
            "loss: 0.33423519134521484 \n",
            " test loss:0.44808125495910645 \n",
            " ------------------\n",
            "loss: 0.3330226540565491 \n",
            " test loss:0.4468018412590027 \n",
            " ------------------\n",
            "loss: 0.3318101763725281 \n",
            " test loss:0.4455225467681885 \n",
            " ------------------\n",
            "loss: 0.3305976688861847 \n",
            " test loss:0.4442431926727295 \n",
            " ------------------\n",
            "loss: 0.3293851912021637 \n",
            " test loss:0.4429638981819153 \n",
            " ------------------\n",
            "loss: 0.3281726539134979 \n",
            " test loss:0.4416845440864563 \n",
            " ------------------\n",
            "loss: 0.32696014642715454 \n",
            " test loss:0.4404051899909973 \n",
            " ------------------\n",
            "loss: 0.32574766874313354 \n",
            " test loss:0.4391258656978607 \n",
            " ------------------\n",
            "loss: 0.32453519105911255 \n",
            " test loss:0.43784648180007935 \n",
            " ------------------\n",
            "loss: 0.32332268357276917 \n",
            " test loss:0.43656715750694275 \n",
            " ------------------\n",
            "loss: 0.3221101760864258 \n",
            " test loss:0.43528780341148376 \n",
            " ------------------\n",
            "loss: 0.3208976984024048 \n",
            " test loss:0.4340084493160248 \n",
            " ------------------\n",
            "loss: 0.319685161113739 \n",
            " test loss:0.4327290952205658 \n",
            " ------------------\n",
            "loss: 0.318472683429718 \n",
            " test loss:0.4314497411251068 \n",
            " ------------------\n",
            "loss: 0.317260205745697 \n",
            " test loss:0.4301703870296478 \n",
            " ------------------\n",
            "loss: 0.31604769825935364 \n",
            " test loss:0.4288910925388336 \n",
            " ------------------\n",
            "loss: 0.31483516097068787 \n",
            " test loss:0.42761173844337463 \n",
            " ------------------\n",
            "loss: 0.31362268328666687 \n",
            " test loss:0.42633241415023804 \n",
            " ------------------\n",
            "loss: 0.3124101758003235 \n",
            " test loss:0.42505303025245667 \n",
            " ------------------\n",
            "loss: 0.3111976683139801 \n",
            " test loss:0.42377370595932007 \n",
            " ------------------\n",
            "loss: 0.3099851906299591 \n",
            " test loss:0.42249441146850586 \n",
            " ------------------\n",
            "loss: 0.3087726831436157 \n",
            " test loss:0.4212150573730469 \n",
            " ------------------\n",
            "loss: 0.3075602054595947 \n",
            " test loss:0.4199356436729431 \n",
            " ------------------\n",
            "loss: 0.30634766817092896 \n",
            " test loss:0.4186563491821289 \n",
            " ------------------\n",
            "loss: 0.30513519048690796 \n",
            " test loss:0.4173769950866699 \n",
            " ------------------\n",
            "loss: 0.30392271280288696 \n",
            " test loss:0.41609764099121094 \n",
            " ------------------\n",
            "loss: 0.3027101755142212 \n",
            " test loss:0.41481828689575195 \n",
            " ------------------\n",
            "loss: 0.3014976680278778 \n",
            " test loss:0.41353899240493774 \n",
            " ------------------\n",
            "loss: 0.3002851605415344 \n",
            " test loss:0.41225963830947876 \n",
            " ------------------\n",
            "loss: 0.2990726828575134 \n",
            " test loss:0.41098031401634216 \n",
            " ------------------\n",
            "loss: 0.29786014556884766 \n",
            " test loss:0.4097009599208832 \n",
            " ------------------\n",
            "loss: 0.29664766788482666 \n",
            " test loss:0.4084216058254242 \n",
            " ------------------\n",
            "loss: 0.2954351603984833 \n",
            " test loss:0.4071422517299652 \n",
            " ------------------\n",
            "loss: 0.2942226827144623 \n",
            " test loss:0.405862957239151 \n",
            " ------------------\n",
            "loss: 0.2930101752281189 \n",
            " test loss:0.404583603143692 \n",
            " ------------------\n",
            "loss: 0.2917976677417755 \n",
            " test loss:0.4033042788505554 \n",
            " ------------------\n",
            "loss: 0.2905851900577545 \n",
            " test loss:0.40202492475509644 \n",
            " ------------------\n",
            "loss: 0.28937268257141113 \n",
            " test loss:0.40074554085731506 \n",
            " ------------------\n",
            "loss: 0.28816017508506775 \n",
            " test loss:0.39946624636650085 \n",
            " ------------------\n",
            "loss: 0.28694766759872437 \n",
            " test loss:0.3981868624687195 \n",
            " ------------------\n",
            "loss: 0.28573518991470337 \n",
            " test loss:0.3969075381755829 \n",
            " ------------------\n",
            "loss: 0.2845226526260376 \n",
            " test loss:0.3956281840801239 \n",
            " ------------------\n",
            "loss: 0.2833101749420166 \n",
            " test loss:0.3943488299846649 \n",
            " ------------------\n",
            "loss: 0.2820976674556732 \n",
            " test loss:0.3930695056915283 \n",
            " ------------------\n",
            "loss: 0.2808852195739746 \n",
            " test loss:0.3917901813983917 \n",
            " ------------------\n",
            "loss: 0.27967268228530884 \n",
            " test loss:0.39051079750061035 \n",
            " ------------------\n",
            "loss: 0.27846017479896545 \n",
            " test loss:0.38923144340515137 \n",
            " ------------------\n",
            "loss: 0.27724769711494446 \n",
            " test loss:0.3879520893096924 \n",
            " ------------------\n",
            "loss: 0.2760351598262787 \n",
            " test loss:0.3866727948188782 \n",
            " ------------------\n",
            "loss: 0.2748226821422577 \n",
            " test loss:0.3853934407234192 \n",
            " ------------------\n",
            "loss: 0.2736101746559143 \n",
            " test loss:0.3841141164302826 \n",
            " ------------------\n",
            "loss: 0.2723976671695709 \n",
            " test loss:0.3828347325325012 \n",
            " ------------------\n",
            "loss: 0.27118515968322754 \n",
            " test loss:0.3815554082393646 \n",
            " ------------------\n",
            "loss: 0.26997268199920654 \n",
            " test loss:0.38027602434158325 \n",
            " ------------------\n",
            "loss: 0.26876020431518555 \n",
            " test loss:0.37899670004844666 \n",
            " ------------------\n",
            "loss: 0.2675476670265198 \n",
            " test loss:0.37771737575531006 \n",
            " ------------------\n",
            "loss: 0.2663351893424988 \n",
            " test loss:0.37643805146217346 \n",
            " ------------------\n",
            "loss: 0.2651226818561554 \n",
            " test loss:0.3751586675643921 \n",
            " ------------------\n",
            "loss: 0.2639101445674896 \n",
            " test loss:0.3738793730735779 \n",
            " ------------------\n",
            "loss: 0.262697696685791 \n",
            " test loss:0.3726000189781189 \n",
            " ------------------\n",
            "loss: 0.26148515939712524 \n",
            " test loss:0.3713206648826599 \n",
            " ------------------\n",
            "loss: 0.26027268171310425 \n",
            " test loss:0.3700413703918457 \n",
            " ------------------\n",
            "loss: 0.25906020402908325 \n",
            " test loss:0.36876195669174194 \n",
            " ------------------\n",
            "loss: 0.25784769654273987 \n",
            " test loss:0.36748266220092773 \n",
            " ------------------\n",
            "loss: 0.2566351890563965 \n",
            " test loss:0.36620327830314636 \n",
            " ------------------\n",
            "loss: 0.2554226815700531 \n",
            " test loss:0.36492395401000977 \n",
            " ------------------\n",
            "loss: 0.2542101740837097 \n",
            " test loss:0.36364462971687317 \n",
            " ------------------\n",
            "loss: 0.2529976963996887 \n",
            " test loss:0.3623652756214142 \n",
            " ------------------\n",
            "loss: 0.25178518891334534 \n",
            " test loss:0.3610859215259552 \n",
            " ------------------\n",
            "loss: 0.25057268142700195 \n",
            " test loss:0.3598065972328186 \n",
            " ------------------\n",
            "loss: 0.24936015903949738 \n",
            " test loss:0.3585272431373596 \n",
            " ------------------\n",
            "loss: 0.24814768135547638 \n",
            " test loss:0.357247918844223 \n",
            " ------------------\n",
            "loss: 0.246935173869133 \n",
            " test loss:0.35596856474876404 \n",
            " ------------------\n",
            "loss: 0.2457226812839508 \n",
            " test loss:0.35468921065330505 \n",
            " ------------------\n",
            "loss: 0.24451017379760742 \n",
            " test loss:0.35340991616249084 \n",
            " ------------------\n",
            "loss: 0.24329766631126404 \n",
            " test loss:0.35213056206703186 \n",
            " ------------------\n",
            "loss: 0.24208517372608185 \n",
            " test loss:0.35085123777389526 \n",
            " ------------------\n",
            "loss: 0.24087266623973846 \n",
            " test loss:0.3495718836784363 \n",
            " ------------------\n",
            "loss: 0.23966018855571747 \n",
            " test loss:0.3482924997806549 \n",
            " ------------------\n",
            "loss: 0.2384476661682129 \n",
            " test loss:0.3470131754875183 \n",
            " ------------------\n",
            "loss: 0.2372351586818695 \n",
            " test loss:0.34573379158973694 \n",
            " ------------------\n",
            "loss: 0.23602266609668732 \n",
            " test loss:0.34445449709892273 \n",
            " ------------------\n",
            "loss: 0.23481015861034393 \n",
            " test loss:0.34317511320114136 \n",
            " ------------------\n",
            "loss: 0.23359768092632294 \n",
            " test loss:0.3418957591056824 \n",
            " ------------------\n",
            "loss: 0.23238515853881836 \n",
            " test loss:0.3406164050102234 \n",
            " ------------------\n",
            "loss: 0.23117265105247498 \n",
            " test loss:0.3393371105194092 \n",
            " ------------------\n",
            "loss: 0.22996017336845398 \n",
            " test loss:0.3380577564239502 \n",
            " ------------------\n",
            "loss: 0.2287476509809494 \n",
            " test loss:0.336778461933136 \n",
            " ------------------\n",
            "loss: 0.2275351583957672 \n",
            " test loss:0.3354990780353546 \n",
            " ------------------\n",
            "loss: 0.2263226956129074 \n",
            " test loss:0.3342197835445404 \n",
            " ------------------\n",
            "loss: 0.22511017322540283 \n",
            " test loss:0.33294039964675903 \n",
            " ------------------\n",
            "loss: 0.22389766573905945 \n",
            " test loss:0.33166107535362244 \n",
            " ------------------\n",
            "loss: 0.22268517315387726 \n",
            " test loss:0.33038175106048584 \n",
            " ------------------\n",
            "loss: 0.22147269546985626 \n",
            " test loss:0.32910236716270447 \n",
            " ------------------\n",
            "loss: 0.2202601432800293 \n",
            " test loss:0.32782304286956787 \n",
            " ------------------\n",
            "loss: 0.2190476655960083 \n",
            " test loss:0.3265437185764313 \n",
            " ------------------\n",
            "loss: 0.21783515810966492 \n",
            " test loss:0.3252643346786499 \n",
            " ------------------\n",
            "loss: 0.21662268042564392 \n",
            " test loss:0.3239850103855133 \n",
            " ------------------\n",
            "loss: 0.21541018784046173 \n",
            " test loss:0.3227056562900543 \n",
            " ------------------\n",
            "loss: 0.21419768035411835 \n",
            " test loss:0.3214263319969177 \n",
            " ------------------\n",
            "loss: 0.21298515796661377 \n",
            " test loss:0.32014697790145874 \n",
            " ------------------\n",
            "loss: 0.21177268028259277 \n",
            " test loss:0.31886765360832214 \n",
            " ------------------\n",
            "loss: 0.2105601578950882 \n",
            " test loss:0.31758832931518555 \n",
            " ------------------\n",
            "loss: 0.2093476802110672 \n",
            " test loss:0.3163089454174042 \n",
            " ------------------\n",
            "loss: 0.2081352025270462 \n",
            " test loss:0.3150296211242676 \n",
            " ------------------\n",
            "loss: 0.20692268013954163 \n",
            " test loss:0.313750296831131 \n",
            " ------------------\n",
            "loss: 0.20571017265319824 \n",
            " test loss:0.3124709725379944 \n",
            " ------------------\n",
            "loss: 0.20449769496917725 \n",
            " test loss:0.3111916184425354 \n",
            " ------------------\n",
            "loss: 0.20328518748283386 \n",
            " test loss:0.3099122643470764 \n",
            " ------------------\n",
            "loss: 0.20207266509532928 \n",
            " test loss:0.30863288044929504 \n",
            " ------------------\n",
            "loss: 0.2008601725101471 \n",
            " test loss:0.30735355615615845 \n",
            " ------------------\n",
            "loss: 0.1996476948261261 \n",
            " test loss:0.30607423186302185 \n",
            " ------------------\n",
            "loss: 0.19843518733978271 \n",
            " test loss:0.30479487776756287 \n",
            " ------------------\n",
            "loss: 0.19722267985343933 \n",
            " test loss:0.3035155236721039 \n",
            " ------------------\n",
            "loss: 0.19601018726825714 \n",
            " test loss:0.3022361993789673 \n",
            " ------------------\n",
            "loss: 0.19479767978191376 \n",
            " test loss:0.3009568750858307 \n",
            " ------------------\n",
            "loss: 0.19358517229557037 \n",
            " test loss:0.2996774911880493 \n",
            " ------------------\n",
            "loss: 0.1923726499080658 \n",
            " test loss:0.2983981668949127 \n",
            " ------------------\n",
            "loss: 0.1911601722240448 \n",
            " test loss:0.29711881279945374 \n",
            " ------------------\n",
            "loss: 0.18994766473770142 \n",
            " test loss:0.29583945870399475 \n",
            " ------------------\n",
            "loss: 0.18873518705368042 \n",
            " test loss:0.29456013441085815 \n",
            " ------------------\n",
            "loss: 0.18752267956733704 \n",
            " test loss:0.29328078031539917 \n",
            " ------------------\n",
            "loss: 0.18631018698215485 \n",
            " test loss:0.2920014560222626 \n",
            " ------------------\n",
            "loss: 0.18509766459465027 \n",
            " test loss:0.2907220721244812 \n",
            " ------------------\n",
            "loss: 0.18388517200946808 \n",
            " test loss:0.289442777633667 \n",
            " ------------------\n",
            "loss: 0.1826726794242859 \n",
            " test loss:0.28816336393356323 \n",
            " ------------------\n",
            "loss: 0.1814601719379425 \n",
            " test loss:0.286884069442749 \n",
            " ------------------\n",
            "loss: 0.18024767935276031 \n",
            " test loss:0.2856047451496124 \n",
            " ------------------\n",
            "loss: 0.17903517186641693 \n",
            " test loss:0.28432539105415344 \n",
            " ------------------\n",
            "loss: 0.17782267928123474 \n",
            " test loss:0.28304603695869446 \n",
            " ------------------\n",
            "loss: 0.17661015689373016 \n",
            " test loss:0.28176671266555786 \n",
            " ------------------\n",
            "loss: 0.17539767920970917 \n",
            " test loss:0.2804873585700989 \n",
            " ------------------\n",
            "loss: 0.17418517172336578 \n",
            " test loss:0.2792080342769623 \n",
            " ------------------\n",
            "loss: 0.1729726791381836 \n",
            " test loss:0.2779287099838257 \n",
            " ------------------\n",
            "loss: 0.1717601716518402 \n",
            " test loss:0.2766493260860443 \n",
            " ------------------\n",
            "loss: 0.17054767906665802 \n",
            " test loss:0.2753700315952301 \n",
            " ------------------\n",
            "loss: 0.16933517158031464 \n",
            " test loss:0.27409061789512634 \n",
            " ------------------\n",
            "loss: 0.16812267899513245 \n",
            " test loss:0.27281129360198975 \n",
            " ------------------\n",
            "loss: 0.16691017150878906 \n",
            " test loss:0.27153199911117554 \n",
            " ------------------\n",
            "loss: 0.16569767892360687 \n",
            " test loss:0.27025261521339417 \n",
            " ------------------\n",
            "loss: 0.16448518633842468 \n",
            " test loss:0.26897329092025757 \n",
            " ------------------\n",
            "loss: 0.1632726788520813 \n",
            " test loss:0.2676939368247986 \n",
            " ------------------\n",
            "loss: 0.1620601862668991 \n",
            " test loss:0.266414612531662 \n",
            " ------------------\n",
            "loss: 0.16084767878055573 \n",
            " test loss:0.265135258436203 \n",
            " ------------------\n",
            "loss: 0.15963518619537354 \n",
            " test loss:0.2638559341430664 \n",
            " ------------------\n",
            "loss: 0.15842267870903015 \n",
            " test loss:0.2625765800476074 \n",
            " ------------------\n",
            "loss: 0.15721017122268677 \n",
            " test loss:0.2612972557544708 \n",
            " ------------------\n",
            "loss: 0.15599766373634338 \n",
            " test loss:0.26001787185668945 \n",
            " ------------------\n",
            "loss: 0.1547851711511612 \n",
            " test loss:0.25873854756355286 \n",
            " ------------------\n",
            "loss: 0.153572678565979 \n",
            " test loss:0.25745922327041626 \n",
            " ------------------\n",
            "loss: 0.15236017107963562 \n",
            " test loss:0.2561798393726349 \n",
            " ------------------\n",
            "loss: 0.15114767849445343 \n",
            " test loss:0.2549005150794983 \n",
            " ------------------\n",
            "loss: 0.14993518590927124 \n",
            " test loss:0.2536211609840393 \n",
            " ------------------\n",
            "loss: 0.14872266352176666 \n",
            " test loss:0.2523418068885803 \n",
            " ------------------\n",
            "loss: 0.14751017093658447 \n",
            " test loss:0.2510624825954437 \n",
            " ------------------\n",
            "loss: 0.1462976634502411 \n",
            " test loss:0.24978315830230713 \n",
            " ------------------\n",
            "loss: 0.1450851857662201 \n",
            " test loss:0.24850377440452576 \n",
            " ------------------\n",
            "loss: 0.1438726782798767 \n",
            " test loss:0.24722445011138916 \n",
            " ------------------\n",
            "loss: 0.14266018569469452 \n",
            " test loss:0.24594514071941376 \n",
            " ------------------\n",
            "loss: 0.14144769310951233 \n",
            " test loss:0.24466577172279358 \n",
            " ------------------\n",
            "loss: 0.14023515582084656 \n",
            " test loss:0.2433864176273346 \n",
            " ------------------\n",
            "loss: 0.13902267813682556 \n",
            " test loss:0.2421070784330368 \n",
            " ------------------\n",
            "loss: 0.13781017065048218 \n",
            " test loss:0.2408277541399002 \n",
            " ------------------\n",
            "loss: 0.1365976780653 \n",
            " test loss:0.23954840004444122 \n",
            " ------------------\n",
            "loss: 0.1353851854801178 \n",
            " test loss:0.23826906085014343 \n",
            " ------------------\n",
            "loss: 0.13417267799377441 \n",
            " test loss:0.23698973655700684 \n",
            " ------------------\n",
            "loss: 0.13296017050743103 \n",
            " test loss:0.23571035265922546 \n",
            " ------------------\n",
            "loss: 0.13174769282341003 \n",
            " test loss:0.23443102836608887 \n",
            " ------------------\n",
            "loss: 0.13053517043590546 \n",
            " test loss:0.23315167427062988 \n",
            " ------------------\n",
            "loss: 0.12932267785072327 \n",
            " test loss:0.2318723499774933 \n",
            " ------------------\n",
            "loss: 0.12811018526554108 \n",
            " test loss:0.2305929958820343 \n",
            " ------------------\n",
            "loss: 0.1268976777791977 \n",
            " test loss:0.2293136566877365 \n",
            " ------------------\n",
            "loss: 0.1256851851940155 \n",
            " test loss:0.22803433239459991 \n",
            " ------------------\n",
            "loss: 0.12447266280651093 \n",
            " test loss:0.22675497829914093 \n",
            " ------------------\n",
            "loss: 0.12326017767190933 \n",
            " test loss:0.22547563910484314 \n",
            " ------------------\n",
            "loss: 0.12204767763614655 \n",
            " test loss:0.22419628500938416 \n",
            " ------------------\n",
            "loss: 0.12083518505096436 \n",
            " test loss:0.22291696071624756 \n",
            " ------------------\n",
            "loss: 0.11962268501520157 \n",
            " test loss:0.22163763642311096 \n",
            " ------------------\n",
            "loss: 0.11841018497943878 \n",
            " test loss:0.22035828232765198 \n",
            " ------------------\n",
            "loss: 0.1171976774930954 \n",
            " test loss:0.219078928232193 \n",
            " ------------------\n",
            "loss: 0.11598517000675201 \n",
            " test loss:0.2177995890378952 \n",
            " ------------------\n",
            "loss: 0.11477269232273102 \n",
            " test loss:0.2165202647447586 \n",
            " ------------------\n",
            "loss: 0.11356017738580704 \n",
            " test loss:0.21524091064929962 \n",
            " ------------------\n",
            "loss: 0.11234768480062485 \n",
            " test loss:0.21396155655384064 \n",
            " ------------------\n",
            "loss: 0.11113518476486206 \n",
            " test loss:0.21268220245838165 \n",
            " ------------------\n",
            "loss: 0.10992268472909927 \n",
            " test loss:0.21140286326408386 \n",
            " ------------------\n",
            "loss: 0.10871019214391708 \n",
            " test loss:0.21012353897094727 \n",
            " ------------------\n",
            "loss: 0.1074976697564125 \n",
            " test loss:0.20884418487548828 \n",
            " ------------------\n",
            "loss: 0.10628517717123032 \n",
            " test loss:0.20756487548351288 \n",
            " ------------------\n",
            "loss: 0.10507269203662872 \n",
            " test loss:0.2062855213880539 \n",
            " ------------------\n",
            "loss: 0.10386017709970474 \n",
            " test loss:0.2050061672925949 \n",
            " ------------------\n",
            "loss: 0.10264768451452255 \n",
            " test loss:0.20372681319713593 \n",
            " ------------------\n",
            "loss: 0.10143516957759857 \n",
            " test loss:0.20244748890399933 \n",
            " ------------------\n",
            "loss: 0.10022268444299698 \n",
            " test loss:0.20116814970970154 \n",
            " ------------------\n",
            "loss: 0.0990101769566536 \n",
            " test loss:0.19988879561424255 \n",
            " ------------------\n",
            "loss: 0.09779766947031021 \n",
            " test loss:0.19860945641994476 \n",
            " ------------------\n",
            "loss: 0.09658516943454742 \n",
            " test loss:0.19733010232448578 \n",
            " ------------------\n",
            "loss: 0.09537268429994583 \n",
            " test loss:0.196050763130188 \n",
            " ------------------\n",
            "loss: 0.09416018426418304 \n",
            " test loss:0.1947714239358902 \n",
            " ------------------\n",
            "loss: 0.09294767677783966 \n",
            " test loss:0.1934920996427536 \n",
            " ------------------\n",
            "loss: 0.09173519164323807 \n",
            " test loss:0.19221273064613342 \n",
            " ------------------\n",
            "loss: 0.09052268415689468 \n",
            " test loss:0.19093337655067444 \n",
            " ------------------\n",
            "loss: 0.0893101692199707 \n",
            " test loss:0.18965405225753784 \n",
            " ------------------\n",
            "loss: 0.08809767663478851 \n",
            " test loss:0.18837471306324005 \n",
            " ------------------\n",
            "loss: 0.08688517659902573 \n",
            " test loss:0.18709535896778107 \n",
            " ------------------\n",
            "loss: 0.08567266166210175 \n",
            " test loss:0.18581603467464447 \n",
            " ------------------\n",
            "loss: 0.08446018397808075 \n",
            " test loss:0.18453669548034668 \n",
            " ------------------\n",
            "loss: 0.08324767649173737 \n",
            " test loss:0.1832573413848877 \n",
            " ------------------\n",
            "loss: 0.08203517645597458 \n",
            " test loss:0.1819780021905899 \n",
            " ------------------\n",
            "loss: 0.08082269132137299 \n",
            " test loss:0.18069866299629211 \n",
            " ------------------\n",
            "loss: 0.0796101838350296 \n",
            " test loss:0.17941930890083313 \n",
            " ------------------\n",
            "loss: 0.07839768379926682 \n",
            " test loss:0.17813998460769653 \n",
            " ------------------\n",
            "loss: 0.07718517631292343 \n",
            " test loss:0.17686063051223755 \n",
            " ------------------\n",
            "loss: 0.07597267627716064 \n",
            " test loss:0.17558130621910095 \n",
            " ------------------\n",
            "loss: 0.07476017624139786 \n",
            " test loss:0.17430195212364197 \n",
            " ------------------\n",
            "loss: 0.07354767620563507 \n",
            " test loss:0.17302259802818298 \n",
            " ------------------\n",
            "loss: 0.07233518362045288 \n",
            " test loss:0.171743243932724 \n",
            " ------------------\n",
            "loss: 0.07112269103527069 \n",
            " test loss:0.1704639196395874 \n",
            " ------------------\n",
            "loss: 0.06991018354892731 \n",
            " test loss:0.16918456554412842 \n",
            " ------------------\n",
            "loss: 0.06869767606258392 \n",
            " test loss:0.16790524125099182 \n",
            " ------------------\n",
            "loss: 0.06748517602682114 \n",
            " test loss:0.16662588715553284 \n",
            " ------------------\n",
            "loss: 0.06627267599105835 \n",
            " test loss:0.16534654796123505 \n",
            " ------------------\n",
            "loss: 0.06506018340587616 \n",
            " test loss:0.16406719386577606 \n",
            " ------------------\n",
            "loss: 0.06387317180633545 \n",
            " test loss:0.16286754608154297 \n",
            " ------------------\n",
            "loss: 0.06280092895030975 \n",
            " test loss:0.1616678535938263 \n",
            " ------------------\n",
            "loss: 0.061764735728502274 \n",
            " test loss:0.16054728627204895 \n",
            " ------------------\n",
            "loss: 0.06082223728299141 \n",
            " test loss:0.15942667424678802 \n",
            " ------------------\n",
            "loss: 0.059879738837480545 \n",
            " test loss:0.15830610692501068 \n",
            " ------------------\n",
            "loss: 0.05893724039196968 \n",
            " test loss:0.15718552470207214 \n",
            " ------------------\n",
            "loss: 0.057994741946458817 \n",
            " test loss:0.1560649424791336 \n",
            " ------------------\n",
            "loss: 0.05705224350094795 \n",
            " test loss:0.15494434535503387 \n",
            " ------------------\n",
            "loss: 0.056139588356018066 \n",
            " test loss:0.15390102565288544 \n",
            " ------------------\n",
            "loss: 0.05531533807516098 \n",
            " test loss:0.1528577208518982 \n",
            " ------------------\n",
            "loss: 0.05450698733329773 \n",
            " test loss:0.15189103782176971 \n",
            " ------------------\n",
            "loss: 0.0537913553416729 \n",
            " test loss:0.15092435479164124 \n",
            " ------------------\n",
            "loss: 0.05307571217417717 \n",
            " test loss:0.14995768666267395 \n",
            " ------------------\n",
            "loss: 0.05240381509065628 \n",
            " test loss:0.14906711876392365 \n",
            " ------------------\n",
            "loss: 0.05178748443722725 \n",
            " test loss:0.14817652106285095 \n",
            " ------------------\n",
            "loss: 0.05117116495966911 \n",
            " test loss:0.14728593826293945 \n",
            " ------------------\n",
            "loss: 0.050554823130369186 \n",
            " test loss:0.14639535546302795 \n",
            " ------------------\n",
            "loss: 0.049938514828681946 \n",
            " test loss:0.14550477266311646 \n",
            " ------------------\n",
            "loss: 0.04934404045343399 \n",
            " test loss:0.14468905329704285 \n",
            " ------------------\n",
            "loss: 0.04881756752729416 \n",
            " test loss:0.14387331902980804 \n",
            " ------------------\n",
            "loss: 0.04829108342528343 \n",
            " test loss:0.14318391680717468 \n",
            " ------------------\n",
            "loss: 0.04778474569320679 \n",
            " test loss:0.14260080456733704 \n",
            " ------------------\n",
            "loss: 0.047339461743831635 \n",
            " test loss:0.1420176923274994 \n",
            " ------------------\n",
            "loss: 0.046894170343875885 \n",
            " test loss:0.14143459498882294 \n",
            " ------------------\n",
            "loss: 0.04645396023988724 \n",
            " test loss:0.14090776443481445 \n",
            " ------------------\n",
            "loss: 0.04608141630887985 \n",
            " test loss:0.14038096368312836 \n",
            " ------------------\n",
            "loss: 0.04570886120200157 \n",
            " test loss:0.13985414803028107 \n",
            " ------------------\n",
            "loss: 0.045336317270994186 \n",
            " test loss:0.13932731747627258 \n",
            " ------------------\n",
            "loss: 0.044963765889406204 \n",
            " test loss:0.1388005018234253 \n",
            " ------------------\n",
            "loss: 0.044591210782527924 \n",
            " test loss:0.138273686170578 \n",
            " ------------------\n",
            "loss: 0.04421865940093994 \n",
            " test loss:0.13774684071540833 \n",
            " ------------------\n",
            "loss: 0.04384611174464226 \n",
            " test loss:0.1372237205505371 \n",
            " ------------------\n",
            "loss: 0.04349048063158989 \n",
            " test loss:0.136876180768013 \n",
            " ------------------\n",
            "loss: 0.043182358145713806 \n",
            " test loss:0.1365286409854889 \n",
            " ------------------\n",
            "loss: 0.042874228209257126 \n",
            " test loss:0.1361810863018036 \n",
            " ------------------\n",
            "loss: 0.04256610944867134 \n",
            " test loss:0.1358335316181183 \n",
            " ------------------\n",
            "loss: 0.042264144867658615 \n",
            " test loss:0.1355244219303131 \n",
            " ------------------\n",
            "loss: 0.04201263189315796 \n",
            " test loss:0.13521529734134674 \n",
            " ------------------\n",
            "loss: 0.041761111468076706 \n",
            " test loss:0.13490620255470276 \n",
            " ------------------\n",
            "loss: 0.04150959104299545 \n",
            " test loss:0.1345970630645752 \n",
            " ------------------\n",
            "loss: 0.0412580780684948 \n",
            " test loss:0.13428795337677002 \n",
            " ------------------\n",
            "loss: 0.041012682020664215 \n",
            " test loss:0.13401702046394348 \n",
            " ------------------\n",
            "loss: 0.040810178965330124 \n",
            " test loss:0.13374608755111694 \n",
            " ------------------\n",
            "loss: 0.040607668459415436 \n",
            " test loss:0.1334751695394516 \n",
            " ------------------\n",
            "loss: 0.040405161678791046 \n",
            " test loss:0.13320425152778625 \n",
            " ------------------\n",
            "loss: 0.04020263999700546 \n",
            " test loss:0.1329333335161209 \n",
            " ------------------\n",
            "loss: 0.04000013321638107 \n",
            " test loss:0.13266238570213318 \n",
            " ------------------\n",
            "loss: 0.03979763016104698 \n",
            " test loss:0.13239148259162903 \n",
            " ------------------\n",
            "loss: 0.039595119655132294 \n",
            " test loss:0.1321205347776413 \n",
            " ------------------\n",
            "loss: 0.039392609149217606 \n",
            " test loss:0.13184961676597595 \n",
            " ------------------\n",
            "loss: 0.03919009119272232 \n",
            " test loss:0.13157868385314941 \n",
            " ------------------\n",
            "loss: 0.038987595587968826 \n",
            " test loss:0.13130776584148407 \n",
            " ------------------\n",
            "loss: 0.03878508135676384 \n",
            " test loss:0.13103684782981873 \n",
            " ------------------\n",
            "loss: 0.038596175611019135 \n",
            " test loss:0.13080361485481262 \n",
            " ------------------\n",
            "loss: 0.038435325026512146 \n",
            " test loss:0.1305704116821289 \n",
            " ------------------\n",
            "loss: 0.038274478167295456 \n",
            " test loss:0.1303372085094452 \n",
            " ------------------\n",
            "loss: 0.038113635033369064 \n",
            " test loss:0.13010399043560028 \n",
            " ------------------\n",
            "loss: 0.037952788174152374 \n",
            " test loss:0.12987078726291656 \n",
            " ------------------\n",
            "loss: 0.03779194504022598 \n",
            " test loss:0.12963756918907166 \n",
            " ------------------\n",
            "loss: 0.03763110190629959 \n",
            " test loss:0.12940435111522675 \n",
            " ------------------\n",
            "loss: 0.03748525306582451 \n",
            " test loss:0.12920860946178436 \n",
            " ------------------\n",
            "loss: 0.03735903650522232 \n",
            " test loss:0.12901289761066437 \n",
            " ------------------\n",
            "loss: 0.037232816219329834 \n",
            " test loss:0.12881717085838318 \n",
            " ------------------\n",
            "loss: 0.037106603384017944 \n",
            " test loss:0.128621444106102 \n",
            " ------------------\n",
            "loss: 0.03698038309812546 \n",
            " test loss:0.1284257024526596 \n",
            " ------------------\n",
            "loss: 0.03685416653752327 \n",
            " test loss:0.12822997570037842 \n",
            " ------------------\n",
            "loss: 0.03672794625163078 \n",
            " test loss:0.12803423404693604 \n",
            " ------------------\n",
            "loss: 0.036601729691028595 \n",
            " test loss:0.12783852219581604 \n",
            " ------------------\n",
            "loss: 0.03647550195455551 \n",
            " test loss:0.12764278054237366 \n",
            " ------------------\n",
            "loss: 0.03634928539395332 \n",
            " test loss:0.12744705379009247 \n",
            " ------------------\n",
            "loss: 0.036223072558641434 \n",
            " test loss:0.12725132703781128 \n",
            " ------------------\n",
            "loss: 0.03609684854745865 \n",
            " test loss:0.1270555853843689 \n",
            " ------------------\n",
            "loss: 0.03597063198685646 \n",
            " test loss:0.1268598586320877 \n",
            " ------------------\n",
            "loss: 0.03584441542625427 \n",
            " test loss:0.12666413187980652 \n",
            " ------------------\n",
            "loss: 0.035718195140361786 \n",
            " test loss:0.12646839022636414 \n",
            " ------------------\n",
            "loss: 0.0355919748544693 \n",
            " test loss:0.12627266347408295 \n",
            " ------------------\n",
            "loss: 0.03546575456857681 \n",
            " test loss:0.12607693672180176 \n",
            " ------------------\n",
            "loss: 0.03533952683210373 \n",
            " test loss:0.12588119506835938 \n",
            " ------------------\n",
            "loss: 0.03521331399679184 \n",
            " test loss:0.12568548321723938 \n",
            " ------------------\n",
            "loss: 0.03508709743618965 \n",
            " test loss:0.125489741563797 \n",
            " ------------------\n",
            "loss: 0.034960873425006866 \n",
            " test loss:0.1252940148115158 \n",
            " ------------------\n",
            "loss: 0.03483465686440468 \n",
            " test loss:0.12509827315807343 \n",
            " ------------------\n",
            "loss: 0.03470844030380249 \n",
            " test loss:0.12490256130695343 \n",
            " ------------------\n",
            "loss: 0.034582220017910004 \n",
            " test loss:0.12470681965351105 \n",
            " ------------------\n",
            "loss: 0.03445599973201752 \n",
            " test loss:0.12451108545064926 \n",
            " ------------------\n",
            "loss: 0.034342385828495026 \n",
            " test loss:0.12435215711593628 \n",
            " ------------------\n",
            "loss: 0.03424428030848503 \n",
            " test loss:0.1241932138800621 \n",
            " ------------------\n",
            "loss: 0.03414618968963623 \n",
            " test loss:0.12403428554534912 \n",
            " ------------------\n",
            "loss: 0.03404809162020683 \n",
            " test loss:0.12387535721063614 \n",
            " ------------------\n",
            "loss: 0.033949993550777435 \n",
            " test loss:0.12371642887592316 \n",
            " ------------------\n",
            "loss: 0.033851902931928635 \n",
            " test loss:0.12355747073888779 \n",
            " ------------------\n",
            "loss: 0.03375379741191864 \n",
            " test loss:0.123398557305336 \n",
            " ------------------\n",
            "loss: 0.03365570306777954 \n",
            " test loss:0.12323962152004242 \n",
            " ------------------\n",
            "loss: 0.03355760499835014 \n",
            " test loss:0.12308068573474884 \n",
            " ------------------\n",
            "loss: 0.033459506928920746 \n",
            " test loss:0.12292174994945526 \n",
            " ------------------\n",
            "loss: 0.033368438482284546 \n",
            " test loss:0.1227993592619896 \n",
            " ------------------\n",
            "loss: 0.033292006701231 \n",
            " test loss:0.12267696857452393 \n",
            " ------------------\n",
            "loss: 0.03321558237075806 \n",
            " test loss:0.12255458533763885 \n",
            " ------------------\n",
            "loss: 0.033139150589704514 \n",
            " test loss:0.12243218719959259 \n",
            " ------------------\n",
            "loss: 0.03306271508336067 \n",
            " test loss:0.12230978906154633 \n",
            " ------------------\n",
            "loss: 0.032986290752887726 \n",
            " test loss:0.12218741327524185 \n",
            " ------------------\n",
            "loss: 0.03290986269712448 \n",
            " test loss:0.12206502258777618 \n",
            " ------------------\n",
            "loss: 0.03283343464136124 \n",
            " test loss:0.12194262444972992 \n",
            " ------------------\n",
            "loss: 0.03275700658559799 \n",
            " test loss:0.12182022631168365 \n",
            " ------------------\n",
            "loss: 0.03268057852983475 \n",
            " test loss:0.12169784307479858 \n",
            " ------------------\n",
            "loss: 0.0326041579246521 \n",
            " test loss:0.12157545983791351 \n",
            " ------------------\n",
            "loss: 0.03252772241830826 \n",
            " test loss:0.12145304679870605 \n",
            " ------------------\n",
            "loss: 0.032451286911964417 \n",
            " test loss:0.12133067846298218 \n",
            " ------------------\n",
            "loss: 0.03237485885620117 \n",
            " test loss:0.12120828777551651 \n",
            " ------------------\n",
            "loss: 0.03230638802051544 \n",
            " test loss:0.12112220376729965 \n",
            " ------------------\n",
            "loss: 0.03224536031484604 \n",
            " test loss:0.12103613466024399 \n",
            " ------------------\n",
            "loss: 0.032184332609176636 \n",
            " test loss:0.12095006555318832 \n",
            " ------------------\n",
            "loss: 0.03212329372763634 \n",
            " test loss:0.12086401134729385 \n",
            " ------------------\n",
            "loss: 0.032062266021966934 \n",
            " test loss:0.12077794224023819 \n",
            " ------------------\n",
            "loss: 0.03200124576687813 \n",
            " test loss:0.12069185823202133 \n",
            " ------------------\n",
            "loss: 0.031940218061208725 \n",
            " test loss:0.12060581147670746 \n",
            " ------------------\n",
            "loss: 0.03187919408082962 \n",
            " test loss:0.1205197349190712 \n",
            " ------------------\n",
            "loss: 0.03181815892457962 \n",
            " test loss:0.12043367326259613 \n",
            " ------------------\n",
            "loss: 0.03175712376832962 \n",
            " test loss:0.12034759670495987 \n",
            " ------------------\n",
            "loss: 0.031696103513240814 \n",
            " test loss:0.1202615275979042 \n",
            " ------------------\n",
            "loss: 0.031635068356990814 \n",
            " test loss:0.12017546594142914 \n",
            " ------------------\n",
            "loss: 0.031574051827192307 \n",
            " test loss:0.12008939683437347 \n",
            " ------------------\n",
            "loss: 0.031513016670942307 \n",
            " test loss:0.12000332772731781 \n",
            " ------------------\n",
            "loss: 0.031451981514692307 \n",
            " test loss:0.11991725862026215 \n",
            " ------------------\n",
            "loss: 0.0313909649848938 \n",
            " test loss:0.11983118951320648 \n",
            " ------------------\n",
            "loss: 0.031329937279224396 \n",
            " test loss:0.11974513530731201 \n",
            " ------------------\n",
            "loss: 0.031268902122974396 \n",
            " test loss:0.11965906620025635 \n",
            " ------------------\n",
            "loss: 0.031207865104079247 \n",
            " test loss:0.11957299709320068 \n",
            " ------------------\n",
            "loss: 0.03114684298634529 \n",
            " test loss:0.11948692798614502 \n",
            " ------------------\n",
            "loss: 0.031090546399354935 \n",
            " test loss:0.11943693459033966 \n",
            " ------------------\n",
            "loss: 0.031038811430335045 \n",
            " test loss:0.1193869486451149 \n",
            " ------------------\n",
            "loss: 0.030987069010734558 \n",
            " test loss:0.11933697760105133 \n",
            " ------------------\n",
            "loss: 0.030935322865843773 \n",
            " test loss:0.11928699165582657 \n",
            " ------------------\n",
            "loss: 0.030883586034178734 \n",
            " test loss:0.119237020611763 \n",
            " ------------------\n",
            "loss: 0.030831843614578247 \n",
            " test loss:0.11918703466653824 \n",
            " ------------------\n",
            "loss: 0.03078010119497776 \n",
            " test loss:0.11913704872131348 \n",
            " ------------------\n",
            "loss: 0.030728355050086975 \n",
            " test loss:0.11908706277608871 \n",
            " ------------------\n",
            "loss: 0.030676618218421936 \n",
            " test loss:0.11903709173202515 \n",
            " ------------------\n",
            "loss: 0.0306248776614666 \n",
            " test loss:0.11898709833621979 \n",
            " ------------------\n",
            "loss: 0.030573133379220963 \n",
            " test loss:0.11893711239099503 \n",
            " ------------------\n",
            "loss: 0.030521396547555923 \n",
            " test loss:0.11888714134693146 \n",
            " ------------------\n",
            "loss: 0.030469650402665138 \n",
            " test loss:0.11883716285228729 \n",
            " ------------------\n",
            "loss: 0.0304179135710001 \n",
            " test loss:0.11878718435764313 \n",
            " ------------------\n",
            "loss: 0.030366171151399612 \n",
            " test loss:0.11873719841241837 \n",
            " ------------------\n",
            "loss: 0.030314426869153976 \n",
            " test loss:0.1186872124671936 \n",
            " ------------------\n",
            "loss: 0.030262688174843788 \n",
            " test loss:0.11863724142313004 \n",
            " ------------------\n",
            "loss: 0.03021094761788845 \n",
            " test loss:0.11858724057674408 \n",
            " ------------------\n",
            "loss: 0.030159205198287964 \n",
            " test loss:0.1185372844338417 \n",
            " ------------------\n",
            "loss: 0.030107462778687477 \n",
            " test loss:0.11848728358745575 \n",
            " ------------------\n",
            "loss: 0.030055725947022438 \n",
            " test loss:0.11843731254339218 \n",
            " ------------------\n",
            "loss: 0.03000398352742195 \n",
            " test loss:0.11838731914758682 \n",
            " ------------------\n",
            "loss: 0.029952239245176315 \n",
            " test loss:0.11833734810352325 \n",
            " ------------------\n",
            "loss: 0.029900502413511276 \n",
            " test loss:0.11828736215829849 \n",
            " ------------------\n",
            "loss: 0.02984875999391079 \n",
            " test loss:0.11823737621307373 \n",
            " ------------------\n",
            "loss: 0.029797017574310303 \n",
            " test loss:0.11818739026784897 \n",
            " ------------------\n",
            "loss: 0.029745277017354965 \n",
            " test loss:0.1181374192237854 \n",
            " ------------------\n",
            "loss: 0.02969353273510933 \n",
            " test loss:0.11808745563030243 \n",
            " ------------------\n",
            "loss: 0.029641788452863693 \n",
            " test loss:0.11803746223449707 \n",
            " ------------------\n",
            "loss: 0.029590051621198654 \n",
            " test loss:0.11798746883869171 \n",
            " ------------------\n",
            "loss: 0.029538314789533615 \n",
            " test loss:0.11793749034404755 \n",
            " ------------------\n",
            "loss: 0.02948657236993313 \n",
            " test loss:0.11788751929998398 \n",
            " ------------------\n",
            "loss: 0.029434820637106895 \n",
            " test loss:0.11783753335475922 \n",
            " ------------------\n",
            "loss: 0.029383087530732155 \n",
            " test loss:0.11778756231069565 \n",
            " ------------------\n",
            "loss: 0.029331350699067116 \n",
            " test loss:0.1177375540137291 \n",
            " ------------------\n",
            "loss: 0.029279600828886032 \n",
            " test loss:0.11768758296966553 \n",
            " ------------------\n",
            "loss: 0.029227863997220993 \n",
            " test loss:0.11763759702444077 \n",
            " ------------------\n",
            "loss: 0.029176119714975357 \n",
            " test loss:0.117587611079216 \n",
            " ------------------\n",
            "loss: 0.029126718640327454 \n",
            " test loss:0.11757349967956543 \n",
            " ------------------\n",
            "loss: 0.029078315943479538 \n",
            " test loss:0.11755935102701187 \n",
            " ------------------\n",
            "loss: 0.029029905796051025 \n",
            " test loss:0.11754520982503891 \n",
            " ------------------\n",
            "loss: 0.028981482610106468 \n",
            " test loss:0.11753108352422714 \n",
            " ------------------\n",
            "loss: 0.02893308363854885 \n",
            " test loss:0.11751697212457657 \n",
            " ------------------\n",
            "loss: 0.02888467349112034 \n",
            " test loss:0.11750282347202301 \n",
            " ------------------\n",
            "loss: 0.028836261481046677 \n",
            " test loss:0.11748869717121124 \n",
            " ------------------\n",
            "loss: 0.028787845745682716 \n",
            " test loss:0.11747455596923828 \n",
            " ------------------\n",
            "loss: 0.02873944118618965 \n",
            " test loss:0.11746042966842651 \n",
            " ------------------\n",
            "loss: 0.028691023588180542 \n",
            " test loss:0.11744628846645355 \n",
            " ------------------\n",
            "loss: 0.02864261344075203 \n",
            " test loss:0.11743215471506119 \n",
            " ------------------\n",
            "loss: 0.028594207018613815 \n",
            " test loss:0.11741803586483002 \n",
            " ------------------\n",
            "loss: 0.028545791283249855 \n",
            " test loss:0.11740388721227646 \n",
            " ------------------\n",
            "loss: 0.02849738858640194 \n",
            " test loss:0.1173897534608841 \n",
            " ------------------\n",
            "loss: 0.02844897471368313 \n",
            " test loss:0.11737562716007233 \n",
            " ------------------\n",
            "loss: 0.028400558978319168 \n",
            " test loss:0.11736148595809937 \n",
            " ------------------\n",
            "loss: 0.028352146968245506 \n",
            " test loss:0.1173473596572876 \n",
            " ------------------\n",
            "loss: 0.028303736820816994 \n",
            " test loss:0.11733324825763702 \n",
            " ------------------\n",
            "loss: 0.02825533226132393 \n",
            " test loss:0.11731909215450287 \n",
            " ------------------\n",
            "loss: 0.028206920251250267 \n",
            " test loss:0.1173049658536911 \n",
            " ------------------\n",
            "loss: 0.028158504515886307 \n",
            " test loss:0.11729083210229874 \n",
            " ------------------\n",
            "loss: 0.028110098093748093 \n",
            " test loss:0.11727670580148697 \n",
            " ------------------\n",
            "loss: 0.02806168794631958 \n",
            " test loss:0.1172625795006752 \n",
            " ------------------\n",
            "loss: 0.02801327034831047 \n",
            " test loss:0.11724843829870224 \n",
            " ------------------\n",
            "loss: 0.02796485461294651 \n",
            " test loss:0.11723430454730988 \n",
            " ------------------\n",
            "loss: 0.027916451916098595 \n",
            " test loss:0.11722016334533691 \n",
            " ------------------\n",
            "loss: 0.027868038043379784 \n",
            " test loss:0.11720603704452515 \n",
            " ------------------\n",
            "loss: 0.02781963348388672 \n",
            " test loss:0.11719191074371338 \n",
            " ------------------\n",
            "loss: 0.027771223336458206 \n",
            " test loss:0.11717776209115982 \n",
            " ------------------\n",
            "loss: 0.027722811326384544 \n",
            " test loss:0.11716364324092865 \n",
            " ------------------\n",
            "loss: 0.027674401178956032 \n",
            " test loss:0.11714950948953629 \n",
            " ------------------\n",
            "loss: 0.02762598730623722 \n",
            " test loss:0.11713536083698273 \n",
            " ------------------\n",
            "loss: 0.027577579021453857 \n",
            " test loss:0.11712124198675156 \n",
            " ------------------\n",
            "loss: 0.027529168874025345 \n",
            " test loss:0.11710710823535919 \n",
            " ------------------\n",
            "loss: 0.027480756863951683 \n",
            " test loss:0.11709298193454742 \n",
            " ------------------\n",
            "loss: 0.02743234671652317 \n",
            " test loss:0.11707882583141327 \n",
            " ------------------\n",
            "loss: 0.02738393470644951 \n",
            " test loss:0.1170647144317627 \n",
            " ------------------\n",
            "loss: 0.027335524559020996 \n",
            " test loss:0.11705057322978973 \n",
            " ------------------\n",
            "loss: 0.027287106961011887 \n",
            " test loss:0.11703643947839737 \n",
            " ------------------\n",
            "loss: 0.02723870240151882 \n",
            " test loss:0.11702229827642441 \n",
            " ------------------\n",
            "loss: 0.02719029225409031 \n",
            " test loss:0.11700818687677383 \n",
            " ------------------\n",
            "loss: 0.027141878381371498 \n",
            " test loss:0.11699403822422028 \n",
            " ------------------\n",
            "loss: 0.027093470096588135 \n",
            " test loss:0.11697991192340851 \n",
            " ------------------\n",
            "loss: 0.027045059949159622 \n",
            " test loss:0.11696577072143555 \n",
            " ------------------\n",
            "loss: 0.026996653527021408 \n",
            " test loss:0.11695164442062378 \n",
            " ------------------\n",
            "loss: 0.026948237791657448 \n",
            " test loss:0.11693751811981201 \n",
            " ------------------\n",
            "loss: 0.026899833232164383 \n",
            " test loss:0.11692339181900024 \n",
            " ------------------\n",
            "loss: 0.026851415634155273 \n",
            " test loss:0.11690926551818848 \n",
            " ------------------\n",
            "loss: 0.02680301107466221 \n",
            " test loss:0.11689510196447372 \n",
            " ------------------\n",
            "loss: 0.026754599064588547 \n",
            " test loss:0.11688097566366196 \n",
            " ------------------\n",
            "loss: 0.026706192642450333 \n",
            " test loss:0.11686684936285019 \n",
            " ------------------\n",
            "loss: 0.026657769456505775 \n",
            " test loss:0.11685272306203842 \n",
            " ------------------\n",
            "loss: 0.02660936489701271 \n",
            " test loss:0.11683858931064606 \n",
            " ------------------\n",
            "loss: 0.0265609510242939 \n",
            " test loss:0.1168244332075119 \n",
            " ------------------\n",
            "loss: 0.026512539014220238 \n",
            " test loss:0.11681032180786133 \n",
            " ------------------\n",
            "loss: 0.026464125141501427 \n",
            " test loss:0.11679619550704956 \n",
            " ------------------\n",
            "loss: 0.026415720582008362 \n",
            " test loss:0.116782046854496 \n",
            " ------------------\n",
            "loss: 0.02636730670928955 \n",
            " test loss:0.11676790565252304 \n",
            " ------------------\n",
            "loss: 0.026318902149796486 \n",
            " test loss:0.11675377935171127 \n",
            " ------------------\n",
            "loss: 0.026270490139722824 \n",
            " test loss:0.1167396530508995 \n",
            " ------------------\n",
            "loss: 0.026222074404358864 \n",
            " test loss:0.11672551929950714 \n",
            " ------------------\n",
            "loss: 0.0261736661195755 \n",
            " test loss:0.11671137809753418 \n",
            " ------------------\n",
            "loss: 0.026125255972146988 \n",
            " test loss:0.1166972666978836 \n",
            " ------------------\n",
            "loss: 0.026076842099428177 \n",
            " test loss:0.11668314039707184 \n",
            " ------------------\n",
            "loss: 0.026028435677289963 \n",
            " test loss:0.11666898429393768 \n",
            " ------------------\n",
            "loss: 0.02598002552986145 \n",
            " test loss:0.11665485799312592 \n",
            " ------------------\n",
            "loss: 0.025931615382432938 \n",
            " test loss:0.11664072424173355 \n",
            " ------------------\n",
            "loss: 0.025883203372359276 \n",
            " test loss:0.11662658303976059 \n",
            " ------------------\n",
            "loss: 0.025834793224930763 \n",
            " test loss:0.11661247164011002 \n",
            " ------------------\n",
            "loss: 0.0257863812148571 \n",
            " test loss:0.11659834533929825 \n",
            " ------------------\n",
            "loss: 0.02573797106742859 \n",
            " test loss:0.11658419668674469 \n",
            " ------------------\n",
            "loss: 0.025689560920000076 \n",
            " test loss:0.11657005548477173 \n",
            " ------------------\n",
            "loss: 0.025641147047281265 \n",
            " test loss:0.11655594408512115 \n",
            " ------------------\n",
            "loss: 0.02559274062514305 \n",
            " test loss:0.116541787981987 \n",
            " ------------------\n",
            "loss: 0.025544334203004837 \n",
            " test loss:0.11652765423059464 \n",
            " ------------------\n",
            "loss: 0.025495916604995728 \n",
            " test loss:0.11651355028152466 \n",
            " ------------------\n",
            "loss: 0.025447506457567215 \n",
            " test loss:0.1164993867278099 \n",
            " ------------------\n",
            "loss: 0.02539909817278385 \n",
            " test loss:0.11648526042699814 \n",
            " ------------------\n",
            "loss: 0.02535068430006504 \n",
            " test loss:0.11647112667560577 \n",
            " ------------------\n",
            "loss: 0.02530227228999138 \n",
            " test loss:0.116457000374794 \n",
            " ------------------\n",
            "loss: 0.025253858417272568 \n",
            " test loss:0.11644287407398224 \n",
            " ------------------\n",
            "loss: 0.025205451995134354 \n",
            " test loss:0.11642874777317047 \n",
            " ------------------\n",
            "loss: 0.025157034397125244 \n",
            " test loss:0.11641459167003632 \n",
            " ------------------\n",
            "loss: 0.02510862983763218 \n",
            " test loss:0.11640048027038574 \n",
            " ------------------\n",
            "loss: 0.025060217827558517 \n",
            " test loss:0.11638633161783218 \n",
            " ------------------\n",
            "loss: 0.025011807680130005 \n",
            " test loss:0.11637220531702042 \n",
            " ------------------\n",
            "loss: 0.024963397532701492 \n",
            " test loss:0.11635806411504745 \n",
            " ------------------\n",
            "loss: 0.02491498365998268 \n",
            " test loss:0.11634393036365509 \n",
            " ------------------\n",
            "loss: 0.02486657351255417 \n",
            " test loss:0.11632981151342392 \n",
            " ------------------\n",
            "loss: 0.024818168953061104 \n",
            " test loss:0.11631567776203156 \n",
            " ------------------\n",
            "loss: 0.024769753217697144 \n",
            " test loss:0.1163015365600586 \n",
            " ------------------\n",
            "loss: 0.02472134307026863 \n",
            " test loss:0.11628739535808563 \n",
            " ------------------\n",
            "loss: 0.02467293292284012 \n",
            " test loss:0.11627328395843506 \n",
            " ------------------\n",
            "loss: 0.024624522775411606 \n",
            " test loss:0.1162591427564621 \n",
            " ------------------\n",
            "loss: 0.024576110765337944 \n",
            " test loss:0.11624500900506973 \n",
            " ------------------\n",
            "loss: 0.02452770061790943 \n",
            " test loss:0.11623086780309677 \n",
            " ------------------\n",
            "loss: 0.02447929047048092 \n",
            " test loss:0.116216741502285 \n",
            " ------------------\n",
            "loss: 0.024430878460407257 \n",
            " test loss:0.11620261520147324 \n",
            " ------------------\n",
            "loss: 0.024382462725043297 \n",
            " test loss:0.11618848890066147 \n",
            " ------------------\n",
            "loss: 0.02433406189084053 \n",
            " test loss:0.11617434024810791 \n",
            " ------------------\n",
            "loss: 0.024285638704895973 \n",
            " test loss:0.11616022884845734 \n",
            " ------------------\n",
            "loss: 0.024237236008048058 \n",
            " test loss:0.11614607274532318 \n",
            " ------------------\n",
            "loss: 0.024188827723264694 \n",
            " test loss:0.11613193899393082 \n",
            " ------------------\n",
            "loss: 0.024140408262610435 \n",
            " test loss:0.11611781269311905 \n",
            " ------------------\n",
            "loss: 0.02409200370311737 \n",
            " test loss:0.11610369384288788 \n",
            " ------------------\n",
            "loss: 0.02404359169304371 \n",
            " test loss:0.11608954519033432 \n",
            " ------------------\n",
            "loss: 0.023995183408260345 \n",
            " test loss:0.11607541888952255 \n",
            " ------------------\n",
            "loss: 0.023946769535541534 \n",
            " test loss:0.11606128513813019 \n",
            " ------------------\n",
            "loss: 0.02389836125075817 \n",
            " test loss:0.11604715883731842 \n",
            " ------------------\n",
            "loss: 0.02384994551539421 \n",
            " test loss:0.11603301763534546 \n",
            " ------------------\n",
            "loss: 0.023801537230610847 \n",
            " test loss:0.1160188764333725 \n",
            " ------------------\n",
            "loss: 0.023753128945827484 \n",
            " test loss:0.11600475013256073 \n",
            " ------------------\n",
            "loss: 0.02370471879839897 \n",
            " test loss:0.11599061638116837 \n",
            " ------------------\n",
            "loss: 0.023656312376260757 \n",
            " test loss:0.1159764751791954 \n",
            " ------------------\n",
            "loss: 0.023607898503541946 \n",
            " test loss:0.11596234887838364 \n",
            " ------------------\n",
            "loss: 0.023559482768177986 \n",
            " test loss:0.11594821512699127 \n",
            " ------------------\n",
            "loss: 0.023511070758104324 \n",
            " test loss:0.1159340888261795 \n",
            " ------------------\n",
            "loss: 0.023462656885385513 \n",
            " test loss:0.11591996997594833 \n",
            " ------------------\n",
            "loss: 0.02341425232589245 \n",
            " test loss:0.11590582132339478 \n",
            " ------------------\n",
            "loss: 0.023365838453173637 \n",
            " test loss:0.11589169502258301 \n",
            " ------------------\n",
            "loss: 0.023317424580454826 \n",
            " test loss:0.11587755382061005 \n",
            " ------------------\n",
            "loss: 0.02326902188360691 \n",
            " test loss:0.11586342751979828 \n",
            " ------------------\n",
            "loss: 0.02322060987353325 \n",
            " test loss:0.11584930121898651 \n",
            " ------------------\n",
            "loss: 0.023172196000814438 \n",
            " test loss:0.11583516746759415 \n",
            " ------------------\n",
            "loss: 0.02312377840280533 \n",
            " test loss:0.11582102626562119 \n",
            " ------------------\n",
            "loss: 0.023075375705957413 \n",
            " test loss:0.11580689251422882 \n",
            " ------------------\n",
            "loss: 0.0230269618332386 \n",
            " test loss:0.11579276621341705 \n",
            " ------------------\n",
            "loss: 0.022978557273745537 \n",
            " test loss:0.11577863991260529 \n",
            " ------------------\n",
            "loss: 0.022930147126317024 \n",
            " test loss:0.11576448380947113 \n",
            " ------------------\n",
            "loss: 0.022881735116243362 \n",
            " test loss:0.11575037240982056 \n",
            " ------------------\n",
            "loss: 0.02283332124352455 \n",
            " test loss:0.1157362312078476 \n",
            " ------------------\n",
            "loss: 0.02278490923345089 \n",
            " test loss:0.11572208255529404 \n",
            " ------------------\n",
            "loss: 0.022736500948667526 \n",
            " test loss:0.11570797115564346 \n",
            " ------------------\n",
            "loss: 0.022688092663884163 \n",
            " test loss:0.1156938299536705 \n",
            " ------------------\n",
            "loss: 0.0226396806538105 \n",
            " test loss:0.11567971855401993 \n",
            " ------------------\n",
            "loss: 0.02259126678109169 \n",
            " test loss:0.11566555500030518 \n",
            " ------------------\n",
            "loss: 0.022542856633663177 \n",
            " test loss:0.11565142869949341 \n",
            " ------------------\n",
            "loss: 0.022494448348879814 \n",
            " test loss:0.11563730239868164 \n",
            " ------------------\n",
            "loss: 0.022446032613515854 \n",
            " test loss:0.11562316119670868 \n",
            " ------------------\n",
            "loss: 0.02239762432873249 \n",
            " test loss:0.11560902744531631 \n",
            " ------------------\n",
            "loss: 0.022349214181303978 \n",
            " test loss:0.11559490859508514 \n",
            " ------------------\n",
            "loss: 0.022300798445940018 \n",
            " test loss:0.11558075994253159 \n",
            " ------------------\n",
            "loss: 0.022252393886446953 \n",
            " test loss:0.11556663364171982 \n",
            " ------------------\n",
            "loss: 0.02220398373901844 \n",
            " test loss:0.11555249989032745 \n",
            " ------------------\n",
            "loss: 0.022155575454235077 \n",
            " test loss:0.11553837358951569 \n",
            " ------------------\n",
            "loss: 0.022107163444161415 \n",
            " test loss:0.11552424728870392 \n",
            " ------------------\n",
            "loss: 0.022058751434087753 \n",
            " test loss:0.11551012098789215 \n",
            " ------------------\n",
            "loss: 0.02201033942401409 \n",
            " test loss:0.11549599468708038 \n",
            " ------------------\n",
            "loss: 0.021961933001875877 \n",
            " test loss:0.11548183858394623 \n",
            " ------------------\n",
            "loss: 0.021913522854447365 \n",
            " test loss:0.11546770483255386 \n",
            " ------------------\n",
            "loss: 0.021865114569664 \n",
            " test loss:0.1154535785317421 \n",
            " ------------------\n",
            "loss: 0.021816695109009743 \n",
            " test loss:0.11543945223093033 \n",
            " ------------------\n",
            "loss: 0.02176828682422638 \n",
            " test loss:0.11542531102895737 \n",
            " ------------------\n",
            "loss: 0.02171987108886242 \n",
            " test loss:0.11541116237640381 \n",
            " ------------------\n",
            "loss: 0.021671462804079056 \n",
            " test loss:0.11539705097675323 \n",
            " ------------------\n",
            "loss: 0.021623048931360245 \n",
            " test loss:0.11538292467594147 \n",
            " ------------------\n",
            "loss: 0.02157464250922203 \n",
            " test loss:0.11536876857280731 \n",
            " ------------------\n",
            "loss: 0.021526234224438667 \n",
            " test loss:0.11535463482141495 \n",
            " ------------------\n",
            "loss: 0.021477824077010155 \n",
            " test loss:0.11534051597118378 \n",
            " ------------------\n",
            "loss: 0.021429408341646194 \n",
            " test loss:0.11532638967037201 \n",
            " ------------------\n",
            "loss: 0.021380998194217682 \n",
            " test loss:0.11531224101781845 \n",
            " ------------------\n",
            "loss: 0.02133258804678917 \n",
            " test loss:0.11529810726642609 \n",
            " ------------------\n",
            "loss: 0.021284179762005806 \n",
            " test loss:0.11528398841619492 \n",
            " ------------------\n",
            "loss: 0.021235764026641846 \n",
            " test loss:0.11526985466480255 \n",
            " ------------------\n",
            "loss: 0.02118735760450363 \n",
            " test loss:0.11525571346282959 \n",
            " ------------------\n",
            "loss: 0.02113894745707512 \n",
            " test loss:0.11524158716201782 \n",
            " ------------------\n",
            "loss: 0.021090537309646606 \n",
            " test loss:0.11522744596004486 \n",
            " ------------------\n",
            "loss: 0.021042125299572945 \n",
            " test loss:0.1152133122086525 \n",
            " ------------------\n",
            "loss: 0.02099371701478958 \n",
            " test loss:0.11519919335842133 \n",
            " ------------------\n",
            "loss: 0.02094530686736107 \n",
            " test loss:0.11518506705760956 \n",
            " ------------------\n",
            "loss: 0.020896894857287407 \n",
            " test loss:0.115170918405056 \n",
            " ------------------\n",
            "loss: 0.020848479121923447 \n",
            " test loss:0.11515678465366364 \n",
            " ------------------\n",
            "loss: 0.020800068974494934 \n",
            " test loss:0.11514266580343246 \n",
            " ------------------\n",
            "loss: 0.02075166441500187 \n",
            " test loss:0.1151285320520401 \n",
            " ------------------\n",
            "loss: 0.020703259855508804 \n",
            " test loss:0.11511437594890594 \n",
            " ------------------\n",
            "loss: 0.020654838532209396 \n",
            " test loss:0.11510027945041656 \n",
            " ------------------\n",
            "loss: 0.020606430247426033 \n",
            " test loss:0.11508611589670181 \n",
            " ------------------\n",
            "loss: 0.02055801823735237 \n",
            " test loss:0.11507198959589005 \n",
            " ------------------\n",
            "loss: 0.02050960436463356 \n",
            " test loss:0.11505784839391708 \n",
            " ------------------\n",
            "loss: 0.020461194217205048 \n",
            " test loss:0.11504372209310532 \n",
            " ------------------\n",
            "loss: 0.020412782207131386 \n",
            " test loss:0.11502959579229355 \n",
            " ------------------\n",
            "loss: 0.02036437578499317 \n",
            " test loss:0.11501546949148178 \n",
            " ------------------\n",
            "loss: 0.020315958186984062 \n",
            " test loss:0.11500132083892822 \n",
            " ------------------\n",
            "loss: 0.020267553627490997 \n",
            " test loss:0.11498720943927765 \n",
            " ------------------\n",
            "loss: 0.020219139754772186 \n",
            " test loss:0.1149730533361435 \n",
            " ------------------\n",
            "loss: 0.020170729607343674 \n",
            " test loss:0.11495892703533173 \n",
            " ------------------\n",
            "loss: 0.02012231945991516 \n",
            " test loss:0.11494479328393936 \n",
            " ------------------\n",
            "loss: 0.02007390931248665 \n",
            " test loss:0.1149306520819664 \n",
            " ------------------\n",
            "loss: 0.020025499165058136 \n",
            " test loss:0.11491654068231583 \n",
            " ------------------\n",
            "loss: 0.019977090880274773 \n",
            " test loss:0.11490239948034286 \n",
            " ------------------\n",
            "loss: 0.019928673282265663 \n",
            " test loss:0.1148882657289505 \n",
            " ------------------\n",
            "loss: 0.0198802649974823 \n",
            " test loss:0.11487412452697754 \n",
            " ------------------\n",
            "loss: 0.019831854850053787 \n",
            " test loss:0.11486001312732697 \n",
            " ------------------\n",
            "loss: 0.019783444702625275 \n",
            " test loss:0.114845871925354 \n",
            " ------------------\n",
            "loss: 0.019735034555196762 \n",
            " test loss:0.11483173072338104 \n",
            " ------------------\n",
            "loss: 0.01968662440776825 \n",
            " test loss:0.11481759697198868 \n",
            " ------------------\n",
            "loss: 0.019638214260339737 \n",
            " test loss:0.11480347067117691 \n",
            " ------------------\n",
            "loss: 0.019589800387620926 \n",
            " test loss:0.11478934437036514 \n",
            " ------------------\n",
            "loss: 0.019541386514902115 \n",
            " test loss:0.11477521806955338 \n",
            " ------------------\n",
            "loss: 0.019492987543344498 \n",
            " test loss:0.11476106941699982 \n",
            " ------------------\n",
            "loss: 0.019444560632109642 \n",
            " test loss:0.11474695056676865 \n",
            " ------------------\n",
            "loss: 0.019396159797906876 \n",
            " test loss:0.11473280191421509 \n",
            " ------------------\n",
            "loss: 0.019347749650478363 \n",
            " test loss:0.11471866071224213 \n",
            " ------------------\n",
            "loss: 0.019299332052469254 \n",
            " test loss:0.11470453441143036 \n",
            " ------------------\n",
            "loss: 0.01925092749297619 \n",
            " test loss:0.11469042301177979 \n",
            " ------------------\n",
            "loss: 0.019202513620257378 \n",
            " test loss:0.11467627435922623 \n",
            " ------------------\n",
            "loss: 0.019154105335474014 \n",
            " test loss:0.11466214805841446 \n",
            " ------------------\n",
            "loss: 0.019105691462755203 \n",
            " test loss:0.1146480068564415 \n",
            " ------------------\n",
            "loss: 0.01905728504061699 \n",
            " test loss:0.11463388055562973 \n",
            " ------------------\n",
            "loss: 0.019008871167898178 \n",
            " test loss:0.11461974680423737 \n",
            " ------------------\n",
            "loss: 0.018960459157824516 \n",
            " test loss:0.1146056056022644 \n",
            " ------------------\n",
            "loss: 0.018912050873041153 \n",
            " test loss:0.11459147930145264 \n",
            " ------------------\n",
            "loss: 0.018863637000322342 \n",
            " test loss:0.11457733809947968 \n",
            " ------------------\n",
            "loss: 0.018815234303474426 \n",
            " test loss:0.11456320434808731 \n",
            " ------------------\n",
            "loss: 0.018766818568110466 \n",
            " test loss:0.11454907804727554 \n",
            " ------------------\n",
            "loss: 0.018718402832746506 \n",
            " test loss:0.11453493684530258 \n",
            " ------------------\n",
            "loss: 0.018669994547963142 \n",
            " test loss:0.11452081054449081 \n",
            " ------------------\n",
            "loss: 0.01862158253788948 \n",
            " test loss:0.11450669914484024 \n",
            " ------------------\n",
            "loss: 0.018573176115751266 \n",
            " test loss:0.11449255049228668 \n",
            " ------------------\n",
            "loss: 0.018524762243032455 \n",
            " test loss:0.11447842419147491 \n",
            " ------------------\n",
            "loss: 0.018476348370313644 \n",
            " test loss:0.11446428298950195 \n",
            " ------------------\n",
            "loss: 0.01842794381082058 \n",
            " test loss:0.11445015668869019 \n",
            " ------------------\n",
            "loss: 0.018379531800746918 \n",
            " test loss:0.11443603038787842 \n",
            " ------------------\n",
            "loss: 0.018331117928028107 \n",
            " test loss:0.11442188918590546 \n",
            " ------------------\n",
            "loss: 0.018282700330018997 \n",
            " test loss:0.11440775543451309 \n",
            " ------------------\n",
            "loss: 0.01823429763317108 \n",
            " test loss:0.11439361423254013 \n",
            " ------------------\n",
            "loss: 0.01818588748574257 \n",
            " test loss:0.11437948793172836 \n",
            " ------------------\n",
            "loss: 0.018137481063604355 \n",
            " test loss:0.1143653616309166 \n",
            " ------------------\n",
            "loss: 0.018089069053530693 \n",
            " test loss:0.11435121297836304 \n",
            " ------------------\n",
            "loss: 0.018040655180811882 \n",
            " test loss:0.11433710157871246 \n",
            " ------------------\n",
            "loss: 0.01799224317073822 \n",
            " test loss:0.1143229603767395 \n",
            " ------------------\n",
            "loss: 0.017943833023309708 \n",
            " test loss:0.11430881172418594 \n",
            " ------------------\n",
            "loss: 0.017895424738526344 \n",
            " test loss:0.11429469287395477 \n",
            " ------------------\n",
            "loss: 0.01784701459109783 \n",
            " test loss:0.11428055912256241 \n",
            " ------------------\n",
            "loss: 0.01779860444366932 \n",
            " test loss:0.11426644027233124 \n",
            " ------------------\n",
            "loss: 0.017750192433595657 \n",
            " test loss:0.11425228416919708 \n",
            " ------------------\n",
            "loss: 0.017701778560876846 \n",
            " test loss:0.11423815786838531 \n",
            " ------------------\n",
            "loss: 0.017653372138738632 \n",
            " test loss:0.11422403156757355 \n",
            " ------------------\n",
            "loss: 0.017604954540729523 \n",
            " test loss:0.11420989036560059 \n",
            " ------------------\n",
            "loss: 0.01755654811859131 \n",
            " test loss:0.11419574916362762 \n",
            " ------------------\n",
            "loss: 0.017508139833807945 \n",
            " test loss:0.11418163776397705 \n",
            " ------------------\n",
            "loss: 0.017459722235798836 \n",
            " test loss:0.11416748911142349 \n",
            " ------------------\n",
            "loss: 0.01741131767630577 \n",
            " test loss:0.11415336281061172 \n",
            " ------------------\n",
            "loss: 0.01736290566623211 \n",
            " test loss:0.11413922160863876 \n",
            " ------------------\n",
            "loss: 0.017314497381448746 \n",
            " test loss:0.114125095307827 \n",
            " ------------------\n",
            "loss: 0.017266083508729935 \n",
            " test loss:0.11411098390817642 \n",
            " ------------------\n",
            "loss: 0.017217673361301422 \n",
            " test loss:0.11409684270620346 \n",
            " ------------------\n",
            "loss: 0.01716926321387291 \n",
            " test loss:0.1140827164053917 \n",
            " ------------------\n",
            "loss: 0.017120856791734695 \n",
            " test loss:0.11406856775283813 \n",
            " ------------------\n",
            "loss: 0.017072442919015884 \n",
            " test loss:0.11405442655086517 \n",
            " ------------------\n",
            "loss: 0.01702403649687767 \n",
            " test loss:0.1140403002500534 \n",
            " ------------------\n",
            "loss: 0.01697561703622341 \n",
            " test loss:0.11402617394924164 \n",
            " ------------------\n",
            "loss: 0.016927208751440048 \n",
            " test loss:0.11401204019784927 \n",
            " ------------------\n",
            "loss: 0.016878794878721237 \n",
            " test loss:0.11399789154529572 \n",
            " ------------------\n",
            "loss: 0.016830386593937874 \n",
            " test loss:0.11398377269506454 \n",
            " ------------------\n",
            "loss: 0.016781972721219063 \n",
            " test loss:0.11396964639425278 \n",
            " ------------------\n",
            "loss: 0.01673356629908085 \n",
            " test loss:0.11395549774169922 \n",
            " ------------------\n",
            "loss: 0.016685158014297485 \n",
            " test loss:0.11394135653972626 \n",
            " ------------------\n",
            "loss: 0.016636747866868973 \n",
            " test loss:0.11392724514007568 \n",
            " ------------------\n",
            "loss: 0.016588333994150162 \n",
            " test loss:0.11391311883926392 \n",
            " ------------------\n",
            "loss: 0.01653992012143135 \n",
            " test loss:0.11389897018671036 \n",
            " ------------------\n",
            "loss: 0.016491513699293137 \n",
            " test loss:0.1138848289847374 \n",
            " ------------------\n",
            "loss: 0.016443101689219475 \n",
            " test loss:0.11387071758508682 \n",
            " ------------------\n",
            "loss: 0.016394687816500664 \n",
            " test loss:0.11385657638311386 \n",
            " ------------------\n",
            "loss: 0.0163462795317173 \n",
            " test loss:0.1138424426317215 \n",
            " ------------------\n",
            "loss: 0.016297871246933937 \n",
            " test loss:0.11382831633090973 \n",
            " ------------------\n",
            "loss: 0.016249459236860275 \n",
            " test loss:0.11381417512893677 \n",
            " ------------------\n",
            "loss: 0.016201045364141464 \n",
            " test loss:0.1138000339269638 \n",
            " ------------------\n",
            "loss: 0.01615263894200325 \n",
            " test loss:0.11378592252731323 \n",
            " ------------------\n",
            "loss: 0.016104228794574738 \n",
            " test loss:0.11377179622650146 \n",
            " ------------------\n",
            "loss: 0.016055816784501076 \n",
            " test loss:0.1137576475739479 \n",
            " ------------------\n",
            "loss: 0.016007402911782265 \n",
            " test loss:0.11374350637197495 \n",
            " ------------------\n",
            "loss: 0.015958990901708603 \n",
            " test loss:0.11372939497232437 \n",
            " ------------------\n",
            "loss: 0.015910586342215538 \n",
            " test loss:0.11371525377035141 \n",
            " ------------------\n",
            "loss: 0.015862181782722473 \n",
            " test loss:0.11370110511779785 \n",
            " ------------------\n",
            "loss: 0.015813764184713364 \n",
            " test loss:0.11368700116872787 \n",
            " ------------------\n",
            "loss: 0.015765352174639702 \n",
            " test loss:0.11367283761501312 \n",
            " ------------------\n",
            "loss: 0.01571694202721119 \n",
            " test loss:0.11365871131420135 \n",
            " ------------------\n",
            "loss: 0.01566852629184723 \n",
            " test loss:0.11364457756280899 \n",
            " ------------------\n",
            "loss: 0.015620119869709015 \n",
            " test loss:0.11363045126199722 \n",
            " ------------------\n",
            "loss: 0.015571704134345055 \n",
            " test loss:0.11361632496118546 \n",
            " ------------------\n",
            "loss: 0.01552329771220684 \n",
            " test loss:0.11360219866037369 \n",
            " ------------------\n",
            "loss: 0.015474881045520306 \n",
            " test loss:0.11358805000782013 \n",
            " ------------------\n",
            "loss: 0.015426474623382092 \n",
            " test loss:0.11357393115758896 \n",
            " ------------------\n",
            "loss: 0.01537806261330843 \n",
            " test loss:0.1135597825050354 \n",
            " ------------------\n",
            "loss: 0.015329653397202492 \n",
            " test loss:0.11354565620422363 \n",
            " ------------------\n",
            "loss: 0.015281240455806255 \n",
            " test loss:0.11353151500225067 \n",
            " ------------------\n",
            "loss: 0.015232833102345467 \n",
            " test loss:0.1135173812508583 \n",
            " ------------------\n",
            "loss: 0.01518442202359438 \n",
            " test loss:0.11350326240062714 \n",
            " ------------------\n",
            "loss: 0.015136012807488441 \n",
            " test loss:0.11348912864923477 \n",
            " ------------------\n",
            "loss: 0.015087594278156757 \n",
            " test loss:0.11347498744726181 \n",
            " ------------------\n",
            "loss: 0.015039187856018543 \n",
            " test loss:0.11346085369586945 \n",
            " ------------------\n",
            "loss: 0.014990774914622307 \n",
            " test loss:0.11344673484563828 \n",
            " ------------------\n",
            "loss: 0.014942367561161518 \n",
            " test loss:0.11343260109424591 \n",
            " ------------------\n",
            "loss: 0.014893956482410431 \n",
            " test loss:0.11341845989227295 \n",
            " ------------------\n",
            "loss: 0.014845548197627068 \n",
            " test loss:0.11340431869029999 \n",
            " ------------------\n",
            "loss: 0.014797133393585682 \n",
            " test loss:0.11339019238948822 \n",
            " ------------------\n",
            "loss: 0.014748726971447468 \n",
            " test loss:0.11337606608867645 \n",
            " ------------------\n",
            "loss: 0.014700308442115784 \n",
            " test loss:0.11336193978786469 \n",
            " ------------------\n",
            "loss: 0.014651911333203316 \n",
            " test loss:0.11334779113531113 \n",
            " ------------------\n",
            "loss: 0.01460348628461361 \n",
            " test loss:0.11333367973566055 \n",
            " ------------------\n",
            "loss: 0.014555083587765694 \n",
            " test loss:0.113319531083107 \n",
            " ------------------\n",
            "loss: 0.014506672509014606 \n",
            " test loss:0.11330538988113403 \n",
            " ------------------\n",
            "loss: 0.014458253979682922 \n",
            " test loss:0.11329126358032227 \n",
            " ------------------\n",
            "loss: 0.014409849420189857 \n",
            " test loss:0.11327715218067169 \n",
            " ------------------\n",
            "loss: 0.014361436478793621 \n",
            " test loss:0.11326299607753754 \n",
            " ------------------\n",
            "loss: 0.014313030056655407 \n",
            " test loss:0.11324886977672577 \n",
            " ------------------\n",
            "loss: 0.014264616183936596 \n",
            " test loss:0.1132347360253334 \n",
            " ------------------\n",
            "loss: 0.014216205105185509 \n",
            " test loss:0.11322060972452164 \n",
            " ------------------\n",
            "loss: 0.014167791232466698 \n",
            " test loss:0.11320646852254868 \n",
            " ------------------\n",
            "loss: 0.014119381085038185 \n",
            " test loss:0.11319233477115631 \n",
            " ------------------\n",
            "loss: 0.014070972800254822 \n",
            " test loss:0.11317820847034454 \n",
            " ------------------\n",
            "loss: 0.01402255892753601 \n",
            " test loss:0.11316406726837158 \n",
            " ------------------\n",
            "loss: 0.01397415716201067 \n",
            " test loss:0.11314992606639862 \n",
            " ------------------\n",
            "loss: 0.01392574142664671 \n",
            " test loss:0.11313579976558685 \n",
            " ------------------\n",
            "loss: 0.013877326622605324 \n",
            " test loss:0.11312166601419449 \n",
            " ------------------\n",
            "loss: 0.013828916475176811 \n",
            " test loss:0.11310753971338272 \n",
            " ------------------\n",
            "loss: 0.013780507259070873 \n",
            " test loss:0.11309341341257095 \n",
            " ------------------\n",
            "loss: 0.013732098042964935 \n",
            " test loss:0.11307927221059799 \n",
            " ------------------\n",
            "loss: 0.013683686032891273 \n",
            " test loss:0.11306514590978622 \n",
            " ------------------\n",
            "loss: 0.013635274954140186 \n",
            " test loss:0.11305101215839386 \n",
            " ------------------\n",
            "loss: 0.013586866669356823 \n",
            " test loss:0.11305246502161026 \n",
            " ------------------\n",
            "loss: 0.013538452796638012 \n",
            " test loss:0.11305440962314606 \n",
            " ------------------\n",
            "loss: 0.01349004078656435 \n",
            " test loss:0.11305632442235947 \n",
            " ------------------\n",
            "loss: 0.013441622257232666 \n",
            " test loss:0.11305825412273407 \n",
            " ------------------\n",
            "loss: 0.013393217697739601 \n",
            " test loss:0.11306017637252808 \n",
            " ------------------\n",
            "loss: 0.013344809412956238 \n",
            " test loss:0.11306212097406387 \n",
            " ------------------\n",
            "loss: 0.013296401128172874 \n",
            " test loss:0.11306405067443848 \n",
            " ------------------\n",
            "loss: 0.013247993774712086 \n",
            " test loss:0.11306597292423248 \n",
            " ------------------\n",
            "loss: 0.013199579901993275 \n",
            " test loss:0.11306791007518768 \n",
            " ------------------\n",
            "loss: 0.013151166029274464 \n",
            " test loss:0.11306984722614288 \n",
            " ------------------\n",
            "loss: 0.013102756813168526 \n",
            " test loss:0.11307176202535629 \n",
            " ------------------\n",
            "loss: 0.013054346665740013 \n",
            " test loss:0.11307370662689209 \n",
            " ------------------\n",
            "loss: 0.013005937449634075 \n",
            " test loss:0.1130756288766861 \n",
            " ------------------\n",
            "loss: 0.012957528233528137 \n",
            " test loss:0.1130775660276413 \n",
            " ------------------\n",
            "loss: 0.012909114360809326 \n",
            " test loss:0.1130795031785965 \n",
            " ------------------\n",
            "loss: 0.012860700488090515 \n",
            " test loss:0.1130814179778099 \n",
            " ------------------\n",
            "loss: 0.012812294065952301 \n",
            " test loss:0.11308334767818451 \n",
            " ------------------\n",
            "loss: 0.012763877399265766 \n",
            " test loss:0.11308528482913971 \n",
            " ------------------\n",
            "loss: 0.012715470977127552 \n",
            " test loss:0.11308721452951431 \n",
            " ------------------\n",
            "loss: 0.012667062692344189 \n",
            " test loss:0.11308915913105011 \n",
            " ------------------\n",
            "loss: 0.01261864323168993 \n",
            " test loss:0.11309106647968292 \n",
            " ------------------\n",
            "loss: 0.01257023960351944 \n",
            " test loss:0.11309299618005753 \n",
            " ------------------\n",
            "loss: 0.012521828524768353 \n",
            " test loss:0.11309492588043213 \n",
            " ------------------\n",
            "loss: 0.012473421171307564 \n",
            " test loss:0.11309685558080673 \n",
            " ------------------\n",
            "loss: 0.012425007298588753 \n",
            " test loss:0.11309881508350372 \n",
            " ------------------\n",
            "loss: 0.012376596219837666 \n",
            " test loss:0.11310072988271713 \n",
            " ------------------\n",
            "loss: 0.012328186072409153 \n",
            " test loss:0.11310265213251114 \n",
            " ------------------\n",
            "loss: 0.012279780581593513 \n",
            " test loss:0.11310458183288574 \n",
            " ------------------\n",
            "loss: 0.012231366708874702 \n",
            " test loss:0.11310651153326035 \n",
            " ------------------\n",
            "loss: 0.012182957492768764 \n",
            " test loss:0.11310844123363495 \n",
            " ------------------\n",
            "loss: 0.012134539894759655 \n",
            " test loss:0.11311037838459015 \n",
            " ------------------\n",
            "loss: 0.012086128816008568 \n",
            " test loss:0.11311230808496475 \n",
            " ------------------\n",
            "loss: 0.012037716805934906 \n",
            " test loss:0.11311423778533936 \n",
            " ------------------\n",
            "loss: 0.011989309452474117 \n",
            " test loss:0.11311616748571396 \n",
            " ------------------\n",
            "loss: 0.011940894648432732 \n",
            " test loss:0.11311809718608856 \n",
            " ------------------\n",
            "loss: 0.011892488226294518 \n",
            " test loss:0.11312001943588257 \n",
            " ------------------\n",
            "loss: 0.011844079941511154 \n",
            " test loss:0.11312196403741837 \n",
            " ------------------\n",
            "loss: 0.011795669794082642 \n",
            " test loss:0.11312389373779297 \n",
            " ------------------\n",
            "loss: 0.011747258715331554 \n",
            " test loss:0.11312582343816757 \n",
            " ------------------\n",
            "loss: 0.011698843911290169 \n",
            " test loss:0.11312774568796158 \n",
            " ------------------\n",
            "loss: 0.011650435626506805 \n",
            " test loss:0.11312969028949738 \n",
            " ------------------\n",
            "loss: 0.011602025479078293 \n",
            " test loss:0.11313161998987198 \n",
            " ------------------\n",
            "loss: 0.011553609743714333 \n",
            " test loss:0.11313353478908539 \n",
            " ------------------\n",
            "loss: 0.01150520145893097 \n",
            " test loss:0.11313547194004059 \n",
            " ------------------\n",
            "loss: 0.011456793174147606 \n",
            " test loss:0.11313740909099579 \n",
            " ------------------\n",
            "loss: 0.011408383026719093 \n",
            " test loss:0.1131393313407898 \n",
            " ------------------\n",
            "loss: 0.011359969154000282 \n",
            " test loss:0.1131412535905838 \n",
            " ------------------\n",
            "loss: 0.011311562731862068 \n",
            " test loss:0.113143190741539 \n",
            " ------------------\n",
            "loss: 0.011263148859143257 \n",
            " test loss:0.11314511299133301 \n",
            " ------------------\n",
            "loss: 0.011214738711714745 \n",
            " test loss:0.11314704269170761 \n",
            " ------------------\n",
            "loss: 0.011166324838995934 \n",
            " test loss:0.11314897239208221 \n",
            " ------------------\n",
            "loss: 0.011117914691567421 \n",
            " test loss:0.11315091699361801 \n",
            " ------------------\n",
            "loss: 0.01106951106339693 \n",
            " test loss:0.11315283924341202 \n",
            " ------------------\n",
            "loss: 0.011021105572581291 \n",
            " test loss:0.11315476894378662 \n",
            " ------------------\n",
            "loss: 0.010972685180604458 \n",
            " test loss:0.11315672099590302 \n",
            " ------------------\n",
            "loss: 0.010924275033175945 \n",
            " test loss:0.11315862089395523 \n",
            " ------------------\n",
            "loss: 0.010875864885747433 \n",
            " test loss:0.11316055059432983 \n",
            " ------------------\n",
            "loss: 0.010827451013028622 \n",
            " test loss:0.11316248029470444 \n",
            " ------------------\n",
            "loss: 0.010779041796922684 \n",
            " test loss:0.11316442489624023 \n",
            " ------------------\n",
            "loss: 0.010730627924203873 \n",
            " test loss:0.11316635459661484 \n",
            " ------------------\n",
            "loss: 0.010682220570743084 \n",
            " test loss:0.11316828429698944 \n",
            " ------------------\n",
            "loss: 0.010633802972733974 \n",
            " test loss:0.11317020654678345 \n",
            " ------------------\n",
            "loss: 0.010585395619273186 \n",
            " test loss:0.11317215859889984 \n",
            " ------------------\n",
            "loss: 0.010536985471844673 \n",
            " test loss:0.11317406594753265 \n",
            " ------------------\n",
            "loss: 0.01048857532441616 \n",
            " test loss:0.11317600309848785 \n",
            " ------------------\n",
            "loss: 0.010440164245665073 \n",
            " test loss:0.11317793279886246 \n",
            " ------------------\n",
            "loss: 0.010391755029559135 \n",
            " test loss:0.11317986249923706 \n",
            " ------------------\n",
            "loss: 0.010343344882130623 \n",
            " test loss:0.11318180710077286 \n",
            " ------------------\n",
            "loss: 0.01029493473470211 \n",
            " test loss:0.11318371444940567 \n",
            " ------------------\n",
            "loss: 0.010246517136693 \n",
            " test loss:0.11318565905094147 \n",
            " ------------------\n",
            "loss: 0.010198110714554787 \n",
            " test loss:0.11318758875131607 \n",
            " ------------------\n",
            "loss: 0.0101496996358037 \n",
            " test loss:0.11318951845169067 \n",
            " ------------------\n",
            "loss: 0.010101472027599812 \n",
            " test loss:0.11324147135019302 \n",
            " ------------------\n",
            "loss: 0.01005316898226738 \n",
            " test loss:0.11324338614940643 \n",
            " ------------------\n",
            "loss: 0.010004756040871143 \n",
            " test loss:0.11324533075094223 \n",
            " ------------------\n",
            "loss: 0.009956348687410355 \n",
            " test loss:0.11324727535247803 \n",
            " ------------------\n",
            "loss: 0.009907936677336693 \n",
            " test loss:0.11324918270111084 \n",
            " ------------------\n",
            "loss: 0.00985952652990818 \n",
            " test loss:0.11325112730264664 \n",
            " ------------------\n",
            "loss: 0.009811115451157093 \n",
            " test loss:0.11325304210186005 \n",
            " ------------------\n",
            "loss: 0.009762701578438282 \n",
            " test loss:0.11325498670339584 \n",
            " ------------------\n",
            "loss: 0.009714296087622643 \n",
            " test loss:0.11325690895318985 \n",
            " ------------------\n",
            "loss: 0.009665882214903831 \n",
            " test loss:0.11325883865356445 \n",
            " ------------------\n",
            "loss: 0.009617535397410393 \n",
            " test loss:0.11331560462713242 \n",
            " ------------------\n",
            "loss: 0.009569351561367512 \n",
            " test loss:0.11333437263965607 \n",
            " ------------------\n",
            "loss: 0.00952093955129385 \n",
            " test loss:0.11335314810276031 \n",
            " ------------------\n",
            "loss: 0.009472528472542763 \n",
            " test loss:0.11337192356586456 \n",
            " ------------------\n",
            "loss: 0.00942411832511425 \n",
            " test loss:0.11339070647954941 \n",
            " ------------------\n",
            "loss: 0.009375701658427715 \n",
            " test loss:0.11340947449207306 \n",
            " ------------------\n",
            "loss: 0.009327293373644352 \n",
            " test loss:0.1134282574057579 \n",
            " ------------------\n",
            "loss: 0.009278886951506138 \n",
            " test loss:0.11344703286886215 \n",
            " ------------------\n",
            "loss: 0.009230474941432476 \n",
            " test loss:0.1134658083319664 \n",
            " ------------------\n",
            "loss: 0.00918206200003624 \n",
            " test loss:0.11348458379507065 \n",
            " ------------------\n",
            "loss: 0.009133652783930302 \n",
            " test loss:0.11350338160991669 \n",
            " ------------------\n",
            "loss: 0.009085488505661488 \n",
            " test loss:0.1135866791009903 \n",
            " ------------------\n",
            "loss: 0.009037120267748833 \n",
            " test loss:0.11360546201467514 \n",
            " ------------------\n",
            "loss: 0.008988717570900917 \n",
            " test loss:0.11362423747777939 \n",
            " ------------------\n",
            "loss: 0.008940303698182106 \n",
            " test loss:0.11364300549030304 \n",
            " ------------------\n",
            "loss: 0.008891892619431019 \n",
            " test loss:0.11366180330514908 \n",
            " ------------------\n",
            "loss: 0.008843475952744484 \n",
            " test loss:0.11368056386709213 \n",
            " ------------------\n",
            "loss: 0.00879506766796112 \n",
            " test loss:0.11369933933019638 \n",
            " ------------------\n",
            "loss: 0.008746661245822906 \n",
            " test loss:0.11371812969446182 \n",
            " ------------------\n",
            "loss: 0.008698238991200924 \n",
            " test loss:0.11373688280582428 \n",
            " ------------------\n",
            "loss: 0.00864983256906271 \n",
            " test loss:0.11375568062067032 \n",
            " ------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        " pred = model1(testX)\n",
        "\n",
        "plot_predictions(trainX , trainY , testX , testY , predictions=pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "CrTm8Hm7idim",
        "outputId": "7285b239-5ca8-44df-db71-0bb41e375542"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGR-_4FK2IDM",
        "outputId": "c87d8fd0-9f75-4386-920b-19dbd413cddf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear_layer.weight', tensor([[0.6647]])),\n",
              "             ('linear_layer.bias', tensor([0.3169]))])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model1.state_dict() , \"models/secondModel.pth\")"
      ],
      "metadata": {
        "id": "_rOihi7j2L0O"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}